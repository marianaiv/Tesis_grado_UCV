{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(alglhco)=\n",
    "# Algoritmos LHCO 2020\n",
    "El rendimiento de los distintos métodos que utilizaron los participantes de las LHCO 2020 sólo se puede comparar mediante los resultados que obtuvieron. Sin embargo, como se puede observar en la {numref}`lhco-resultados`, la mayoría de los modelos no reportaron el error asociado al resultado obtenido. Más aún, cada participante reportó los resultados y rendimiento del modelo de acuerdo al método utilizado, dificultando la comparación directa de los análisis. \n",
    "\n",
    "Uno de los objetivos principales de este trabajo es comparar directamente algunos modelos participantes de las LHCO 2020. Para ello, es necesario poder reproducir el resultado de dichos modelos. Sin embargo, estos deben cumplir múltiples de los requisitos para hacer investigación reproducible explicados en la {numref}`rpd-investigacion`, a manera de asegurar que se pueda obtener el resultado de los modelos en un rango de tiempo adecuado para el desarrollo de este trabajo. \n",
    "\n",
    "A continuación, se hablará del análisis de los algoritmos participantes a nivel de la reproducibilidad de sus resultados y se explicarán los modelos a comparar en este trabajo.\n",
    "\n",
    "(alglhco-rep)=\n",
    "## Reproducibilidad\n",
    "Para escoger los algoritmos a comparar, se hizo una revisión extensiva de la información proporcionada por los participantes mencionados en la {numref}`lhc-participantes`. Como se explica en la {numref}`rpd-investigacion`, para poder reproducir resultados en este contexto es necesario principalmente que se encuentre pública la información sobre el pre-procesamiento de los datos, el código del modelo, instrucciones para utilizarlo, información acerca del entorno computacional y licencia. De cada participante se encontró lo siguiente,\n",
    "\n",
    "```{table} Información disponible de cada participante de las LHCO 2020\n",
    ":name: alglhc-participantesrep\n",
    "\n",
    "| Nombre | Pre-procesamiento | Código| Instrucciones | Entorno| Licencia|\n",
    "|:------:|:-----------------:|:-----:|:-------------:|:------:|:-------:|\n",
    "| VRNN   |$\\checkmark$ |$\\checkmark$ |$\\checkmark$ |- |- |\n",
    "| ANODE  |- |- |- |- |- |\n",
    "| BuHuLaSpa|- |$\\checkmark$ |- |- |- |\n",
    "| GAN-AE |$\\checkmark$ |$\\checkmark$ |$\\checkmark$ |$\\checkmark$ |- |\n",
    "| GIS    |- |- |- |- |- |\n",
    "|  LDA   |$\\checkmark$ |$\\checkmark$ |$\\checkmark$ |$\\checkmark$ |- |\n",
    "|  PGAE  |$\\checkmark$ |$\\checkmark$ |$\\checkmark$ |$\\checkmark$ |- |\n",
    "| Reg. Likelihoods|- |- |- |- |- |\n",
    "|UCluster|$\\checkmark$ |$\\checkmark$ |$\\checkmark$ |$\\checkmark$ |$\\checkmark$ |\n",
    "| CWoLa  |- |$\\checkmark$ |- |- |- |\n",
    "|CWoLa AE Compare|- |- | |- |- |\n",
    "|Tag N' Train|$\\checkmark$ |$\\checkmark$ |- |$\\checkmark$ |- |\n",
    "| SALAD  |- |$\\checkmark$ |- |$\\checkmark$ |- |\n",
    "|SA-CWoLa|- |$\\checkmark$ |- |$\\checkmark$ |- |\n",
    "|Deep Ensemble|- |$\\checkmark$ |$\\checkmark$ |$\\checkmark$ |- |\n",
    "|Factorized Topics|- |$\\checkmark$ |- |- |$\\checkmark$ |\n",
    "| QUAK   |- |- |- |- |- |\n",
    "| LSTM   | - |- |- |- |- |- |\n",
    "```\n",
    "Un resumen de general de la información proporcionada por todos los algoritmos se encuentra en la {numref}`alglhco-repfigure`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from myst_nb import glue\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "plt.rc('axes', labelsize=12)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=10)    # fontsize of the tick labels\n",
    "\n",
    "datos = {'Pre-procesamiento':6, 'Código':12, 'Instrucciones':6, 'Entorno':8, 'Licencia':2}\n",
    "items = list(datos.keys())\n",
    "valores = list(datos.values())\n",
    "colores = ['darkorange', 'crimson', 'green', 'blue', 'purple']\n",
    "\n",
    "fig = plt.figure(figsize = (9, 6), facecolor='white')\n",
    " \n",
    "# creating the bar plot\n",
    "plt.bar(items, valores, color = colores,width = 0.6)\n",
    "plt.xlabel(\"Información disponible\")\n",
    "plt.ylabel(\"Nro. de participantes\")\n",
    "plt.title(\"Reproducibilidad de los algoritmos de las LHCO 2020\")\n",
    "plt.ylim([0,18])\n",
    "\n",
    "plt.savefig('./../../figuras/alglhco-repfig.png', bbox_inches='tight', facecolor=fig.get_facecolor(),edgecolor='none')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ./../../figuras/alglhco-repfig.png\n",
    "---\n",
    "name: alglhco-repfigure\n",
    "width: 60%\n",
    "---\n",
    "Resumen de la reproducibilidad de los participantes de las LHCO 2020.\n",
    "```\n",
    "En la {numref}`alglhco-repfigure` se observa que la mayoría de los algoritmos participantes no poseen suficiente información pública para poder reproducir sus resultados, lo que inmmediatamente límita las opciones de modelos a utilizar en este trabajo. De los 18 participantes, sólo *UCluster* cumple con todas las características. *GAN-AE*, *LDA*  y *PGAE* cumplen con cuatro de las características. Para este trabajo se escogieron ***UCluster*** y ***GAN-AE*** debido a la simplicidad de la información proporcionada. Los algoritmos se explicarán siguiendo {cite}`Kasieczka_2021`.\n",
    "\n",
    "(alglhco-ucluster)=\n",
    "## UCluster\n",
    "UCluster (Unsupervised Clustering){cite}`Mikuni_2021` es un método que agrupa puntos con características similares, inmersos en una representación reducida de los datos de eventos. Esta representación se crea utilizando una red neuronal para reducir la dimensionalidad de los datos. El método es independiente de la arquitectura de la red neuronal. \n",
    "\n",
    "Para crear el encjae se hace una *clasificación de masa de jet por partícula*, es decir, a cada partícula perteneciente a un jet se le asigna una etiqueta proporcional a la masa del jet al que pertenece. Estas etiquetas se crean definiendo 20 intervalos equidistantes entre 10 y 1000 GeV. \n",
    "\n",
    "En particular, el modelo debe aprender la masa del jet al que la partícula pertenece y cuáles partículas pertenecen al musmo jet. El modelo utiliza las primeras 100 partículas asociadas a los dos jets más masivos de cada evento y consta de dos tareas, una de clasificación y otra de agrupamiento. La función de pérdida conjunta es:\n",
    "\n",
    "$$\n",
    "    L = L_{focal} + \\beta L_{cluster}.\n",
    "$$ (loss-ucluster)\n",
    "\n",
    "Un valor de $\\beta=10$ se utiliza para darle a las componentes el mismo orden de magnitud relativo.\n",
    "\n",
    "$L_{focal}$ hace referencia a la función de pérdida focal, que es comúnmente utilizada para clasificar datos con etiquetas altamente desbalanceadas{cite}`Kasieczka_2021`.\n",
    "\n",
    "$$\n",
    "    L_{focal}=-\\frac{1}{N}\\sum_j^N\\sum_m^M y_{j,m}(1-p_{\\theta,m}(x_j))^\\gamma \\log(p_{\\theta,m}(x_j))\n",
    "$$ (focal-loss)\n",
    "\n",
    "donde $p_{\\theta,m}(x_j)$ es la confianza de la red para el evento $x_j$, con parámetros entrenables $\\theta$, de ser clasificado como clase $m$. $y_{j,m}$ es 1 si la clase $m$ es correctamente asignada al evento $x_j$ y 0 de otra forma. Utilizaron $\\gamma=2$.\n",
    "\n",
    "La función de pérdida del agrupamiento fue definida como,\n",
    "\n",
    "$$\n",
    "    L_{cluster}=\\frac{1}{N}\\sum_k^K\\sum_j^N\\|f_\\theta(x_j)-\\mu_k\\|^2\\pi_{jk}.\n",
    "$$ (cluster-loss)\n",
    "\n",
    "La distancia entre el evento $x_j$ y el centroide $\\mu_k$ se calcula en el espacio de encaje $f_\\theta$, creado por la clasificación. $\\pi_{jk}$ pesa la importancia de cada evento para el objetivo de agrupación y está definido como:\n",
    "\n",
    "$$\n",
    "    \\pi_{jk}=\\frac{e^{-\\alpha\\|f_\\theta(x_j)-\\mu_k\\|}}{\\sum_{k'}e^{-\\alpha\\|f_\\theta(x_j)-\\mu_k\\|}}\n",
    "$$ (peso-clusterloss)\n",
    "\n",
    "con el hiperparámetro $\\alpha$.\n",
    "\n",
    "El valor inicial de los centroides se hace utilizando el algoritmo K-means, para luego realizar el entrenamiento utilizando la función de pérdida conjunta en la ecuación {eq}`loss-ucluster`. \n",
    "\n",
    "Como se mencionó anteriormente, el modelo es libre de la arquitectura de la red neuronal. Sin embargo, la implementación utilizada en las LHCO 2020 utiliza ABCNet, una red basada en grafos donde cada partícula reconstruida es un nodo del grafo. La arquitectura y la capa del encaje se encuentran en el siguiente diagrama,\n",
    "\n",
    "```{figure} ../../figuras/bench-ABCNet.png\n",
    "---\n",
    "width: 500px\n",
    "name: bench-ABCNet\n",
    "---\n",
    "Diagrama de la red ABCNet utilizada por UCluster.\n",
    "```\n",
    "Para crear las GAPLayers se utiliza la distancia entre los 10 vecinos mas cercanos de cada partícula. La primera GAPLayer utiliza la distancia $\\Delta R=\\sqrt{\\Delta\\eta^2+\\Delta\\phi^2}$ y la segunda utiliza las distancias euclídeas.\n",
    "\n",
    "(alglhco-ganae)=\n",
    "## GAN-AE\n",
    "El método GAN-AE intenta asociar un discriminante y un codificador automatico como una red generativa antagónica siguiendo los pasos a continuación:\n",
    "\n",
    "La **red discriminativa** es un perceptrón multicapas (MLP). Inicialmente es entrenado utilizando la entropia cruzada binaria (ec.{eq}`binary-crossentropy`) en una muestra mezclada de eventos reconstruidos y originales, con el objetivo de exponer las débilidades del codificador automatico.\n",
    "\n",
    "El **codificador automatico** es entrenado utilizando una función de pérdida que combina la reconstrucción del error (en este caso, la distancia euclídea media (MED) entre la entrada y la salida) y la entropía binaria cruzada.\n",
    "\n",
    "$$  \n",
    "    \\mathcal{L}_{AE} = \\mathcal{L}_{BC}+\\epsilon\\times\\text{MED}+\\alpha\\times\\text{DisCo}\n",
    "$$ (loss-ae)\n",
    "\n",
    "donde $\\epsilon$ y $\\alpha$ son hiperparámetros para balancear los pesos de cada término y $\\text{DisCo}$ es la correlación de distancia, para descorrelacionar el error de reconstrucción de la masa invariante. En este caso, $\\mathcal{L}_{BC}$ se evalúa dando eventos reconstruidos a la red discriminativa, pero esta vez con la etiqueta \"equivocada\", con el objetivo de engañarla.\n",
    "\n",
    "Por último, se evalúa el codificador automatico utilizando una *figura de mérito*:\n",
    "\n",
    "$$\n",
    "    \\text{FoM}=\\text{MED}+(1-\\text{Media salida MLP}).\n",
    "$$ (fom)\n",
    "\n",
    "El término $1-\\text{Media salida MLP}$ es cercano a cero a medida que el codificador automático engaña mejor al MLP.\n",
    "\n",
    "Estos tres pasos se repiten en un búcle y una vez que el codificador automatico ha sido entrenado se descarta la red discriminativa. Una vez descartada, el codificador automatico utiliza las distancias euclídeas como característica discriminativa.\n",
    "\n",
    "Los participantes utilizaron el método anteriormente descrito en conjunto con *BumpHunter*, un algoritmo que compara la distribución de los datos con datos de referencia y evalúa el valor p y la significancia de cualquier desviación. Sin embargo, la implementación en este trabajo se limita al uso del algoritmo GAN-AE, que es la parte publicada de este método.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "be244558907c567e73a32fad5ffef5514602d6da01bb2b6b51508d7e46fcc84d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
