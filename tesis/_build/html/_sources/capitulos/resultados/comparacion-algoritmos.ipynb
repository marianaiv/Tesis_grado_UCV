{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(comp)=\n",
    "# Comparación de algoritmos\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "El rendimiento de los algoritmos utilizados anteriormente no puede ser comparado directamente mediante las ditribuciones de las predicciones para cada variable. Como vimos en la sección anterior, un algoritmo puede parecer ajustar muy bien a los datos en una variable pero no en otra. Por esto, utilizamos las métricas explicadas en la {numref}`met` para comparar directamente el rendimiento de los algoritmos.\n",
    "\n",
    "En esta sección se compararán los algoritmos utilizados en la sección anterior, agregando UCluster y GAN-AE. \n",
    "\n",
    "Usualmente, del conjunto R&D se obtendrían las métricas de rendimiento con el fin de configurar los modelos para que realicen la mejor clasificación posible. Sin embargo, en el caso de este trabajo, comparamos el rendimiento de los algoritmos al clasificar un subconjunto del conjunto utilizado para entrenamiento y al clasificar un conjunto ligeramente distinto, en este caso el conjunto R&D y BB1, respectivamente.\n",
    "\n",
    "Los resultados presentados en esta sección se obtuvieron utilizando el pipeline de `benchtools`.\n",
=======
    "El rendimiento de los algoritmos utilizados anteriormente no puede ser comparado directamente mediante las ditribuciones de las predicciones para cada variable. Como se evidencia en la sección anterior, un algoritmo puede parecer ajustar muy bien a los datos en una variable, pero no en otra. Por ello, utilizamos las métricas explicadas en la {numref}`met` para comparar directamente el rendimiento de los algoritmos. En esta sección se compararán los algoritmos utilizados en la sección anterior, agregando UCluster y GAN-AE. \n",
    "\n",
    "Usualmente, del conjunto R&D se obtendrían las métricas de rendimiento con el fin de configurar los modelos para que realicen la mejor clasificación posible. Sin embargo, en este trabajo, comparamos el rendimiento de los algoritmos al clasificar un subconjunto del conjunto R&D, el conjutno utilizado para entrenamiento, y luego al clasificar un conjunto ligeramente distinto, el conjunto BB1.\n",
=======
    "El rendimiento de los algoritmos utilizados anteriormente no puede ser comparado directamente por las ditribuciones de las variables de acuerdo a la clasificación realizada por los modelos. Como se evidencia en la sección anterior, un algoritmo puede parecer ajustar muy bien a los datos en una variable, pero no en otra. Por ello, utilizamos las métricas explicadas en la {numref}`met` para comparar directamente el rendimiento de los algoritmos. En esta sección se compararán los algoritmos utilizados en la sección anterior, agregando UCluster y GAN-AE. \n",
>>>>>>> correcciones
    "\n",
    "Los resultados presentados en esta sección se obtuvieron utilizando el pipeline de `benchtools`, descrito en la {numref}`bench-pipeline-cap`.\n",
>>>>>>> correcciones-1
    "\n",
    "(comp-RnD)=\n",
    "## Conjunto R&D\n",
    "En la sección anterior, se mostraron las distribuciones de algunas variables de acuerdo a la clasificación realizada por cada modelo. Esto se hizo con el objetivo de tener una idea general de cómo están clasificando los algoritmos y para poder asociar esta clasificación a las métricas que se mostrarán a continuación.\n",
    "\n",
    "La clasificación realizada por los algoritmos de las LHCO 2020 para el conjunto R&D no se lleva a cabo utilizando el mismo subconjunto de datos en todos los modelos, contrario a los empleados en este trabajo, que se entrenan y clasifican utilizando los mismos subconjuntos de entrenamiento y prueba. Estos subconjuntos varían para los algoritmos de las olimpiadas de acuerdo al código publicado por los participantes. Sin embargo, el subconjunto usado para calcular las métricas a continuación no fue utilizado para entrenamiento en ninguno de los modelos.\n",
    "\n",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
    "Es necesario acotar que la clasificación realizada por los algoritmos de las LHCO 2020 para el conjunto R&D no se realiza utilizando exactamente el mismo subconjunto de datos utilizado con los demás algoritmos. Aunque es el mismo conjunto de datos, la distribución de los subconjuntos de entrenamiento y prueba varían para los algoritmos de las olimpiadas de acuerdo al código publicado. Sin embargo, la proporción de datos es la misma para todos los métodos.\n",
=======
    "Es necesario acotar, que la clasificación realizada por los algoritmos de las LHCO 2020 para el conjunto R&D no se lleva a cabo utilizando exactamente el mismo subconjunto de datos con todos los modelos. Aunque es el mismo conjunto, los subconjuntos de entrenamiento y prueba varían para los algoritmos de las olimpiadas de acuerdo al código publicado por los participantes. Sin embargo, la proporción de datos de los conjuntos es la misma para todos los métodos; las metricas a continuación representan la clasificación de 30% del conjunto y no fue utilizado para entrenamiento.\n",
>>>>>>> correcciones-1
=======
    "La clasificación realizada por los algoritmos de las LHCO 2020 para el conjunto R&D no se lleva a cabo utilizando el mismo subconjunto de datos en todos los modelos. Los subconjuntos de entrenamiento y prueba varían para los algoritmos de las olimpiadas de acuerdo al código publicado por los participantes. Sin embargo,  este subconjunto no fue utilizado para entrenamiento en ninguno de los modelos.\n",
>>>>>>> correcciones
=======
>>>>>>> correcciones
    "### Métricas numéricas\n",
    "En la {numref}`comp-metricas-num-RnD` se observa una comparación visual de las métricas numéricas: *exactitud balanceada*, *precisión*, *recuperación* y *puntaje f1*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from benchtools.src.plotools import image_grid\n",
    "\n",
    "# Definimos el path de las imagenes\n",
    "PATH_FIGURES = '../../figuras/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "lista_imagenes = ['../../figuras/comp-balancedacc-RnD.png','../../figuras/comp-precision-RnD.png',\n",
    "                '../../figuras/comp-recall-RnD.png','../../figuras/comp-f1score-RnD.png']\n",
    "image_grid(rows=2, columns=2, images=lista_imagenes, name='comp-metricas-num-RnD', path=PATH_FIGURES, remove=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../figuras/comp-metricas-num-RnD.png\n",
    "---\n",
    "width: 80%\n",
    "name: comp-metricas-num-RnD\n",
    "---\n",
    "Diagramas de barras de las métricas numéricas obtenidas utilizando el pipeline de `benchtools` para el conjunto R&D. De izquierda a derecha, en la fila superior: exactitud balanceada y precisión. En la fila inferior: recuperación y puntaje f1.\n",
    "```\n",
    "Todos los modelos obtuvieron un puntaje de exactitud balanceada mayor a 70%, lo que se considera alto. Sin embargo, observamos que la precisión y la recuperación varía entre los clasificadores. \n",
    "\n",
<<<<<<< HEAD
    "Los modelos supervisados QDA, GB y RFC obtuvieron altos puntajes de precisión pero bajos puntajes de recuperación. Esto significa que los algoritmos clasifican pocos eventos como señal, pero estas clasificaciones son mayormente correctas. En contraste, el modelo MLP obtuvo un alto puntaje de precisión y recuperación. \n",
    "\n",
    "Por otra parte el modelos de TensorFlow, obtuvo un menor puntaje de precisión pero el mayor puntaje de recuperación. Es decir, clasifica mas eventos como señal, clasificando eventos de fondo incorrectamente, pero logra clasificar la mayoría de los eventos de señal con la etiqueta correcta.\n",
    "\n",
    "Los modelos no supervisados KMeans, UCluster y GAN-AE son los modelos con menor precisión pero mayor recuperación. Es decir, están etiquetando una mayor cantidad de eventos de fondo incorrectamente como señal, pero entre los eventos etiquetados como señal se encuentra la mayor parte de los eventos de señal real. \n",
    "\n",
    "La media armónica de estas dos métricas se observa en el gráfico del puntaje f1, donde en general los modelos supervisados obtuvieron puntajes más altos que los modelos no supervisados.\n",
    "\n",
    "El mayor puntaje f1 fue obtenido por MLP. Sin embargo, el mayor puntaje de precisión lo obtuvo RFC y el mayor puntaje de recuperación lo obtuvo el modelo de TensorFlow, que también obtuvo el mayor puntaje de exactitud balanceada.\n",
    "\n",
    "Los menores puntajes fueron obtenidos por UCluster, con la menor precisión y menor puntaje f1, y por GBC, con el menor puntaje de exactitud balanceada y recuperación. \n",
=======
    "Los modelos supervisados QDA, GB y RFC obtuvieron altos puntajes de precisión pero bajos puntajes de recuperación. Esto significa que los algoritmos clasifican pocos eventos como señal, pero estas clasificaciones son mayormente correctas. En contraste, el modelo MLP obtuvo un alto puntaje de precisión y recuperación. A su vez, el modelo de TensorFlow obtuvo un menor puntaje de precisión pero el mayor puntaje de recuperación. Es decir, clasifica más eventos como señal, clasificando eventos de fondo incorrectamente, pero logra clasificar la mayoría de los eventos de señal con la etiqueta correcta.\n",
    "\n",
    "Los modelos no supervisados KMeans, UCluster y GAN-AE son los modelos con menor precisión pero mayor recuperación. Es decir, están etiquetando una mayor cantidad de eventos de fondo incorrectamente como señal, pero entre los eventos etiquetados como señal, se encuentra la mayor parte de los eventos de señal real. \n",
    "\n",
    "La media armónica de estas dos métricas se observa en el gráfico del puntaje f1, donde, en general, los modelos supervisados obtuvieron puntajes más altos que los modelos no supervisados. \n",
    "\n",
    "El mayor puntaje f1 fue alcanzado por MLP. Por otra parte, el mayor puntaje de precisión lo logró RFC y el mayor puntaje de recuperación lo consiguió el modelo de TensorFlow, que también obtuvo el mayor puntaje de exactitud balanceada. Los menores puntajes fueron obtenidos por UCluster, con la menor precisión y menor puntaje f1, y por GBC, con el menor puntaje de exactitud balanceada y recuperación. \n",
    "\n",
    "Los modelos supervisados obtienen una mejor clasificación porque poseen más información en el momento del aprendizaje, puesto que utilizan las etiquetas de los eventos, mientras que los modelos no supervisados requieren estructura en los datos de entrenamiento. Si el conjunto de datos original no posee esta estructura naturalmente, se debería lograr mediante el pre-procesamiento. Sin embargo, el pre-procesamiento de los tres modelos no-supervisados no logró una estructura suficientemente significativa como para obtener una mejor clasificación que las de los modelos supervisados.\n",
    "\n",
<<<<<<< HEAD
    "El mayor puntaje f1 fue alcanzado por MLP. Por otra parte, el mayor puntaje de precisión lo logró RFC y el mayor puntaje de recuperación lo consiguió el modelo de TensorFlow, que también obtuvo el mayor puntaje de exactitud balanceada. Los menores puntajes fueron obtenidos por UCluster, con la menor precisión y menor puntaje f1, y por GBC, con el menor puntaje de exactitud balanceada y recuperación. \n",
>>>>>>> correcciones-1
=======
    "Se consideró una clasificación aleatoria en la que un modelo predice que el 50% de los eventos son señal y el 50% son fondo, aleatoriamente. La clasificación aleatoria obtuvo los menores puntajes excepto en la precisión y el puntaje f1, donde UCluster obtuvo menor puntaje. En general, los modelos clasifican mejor que un clasificador aleatorio, salvo UCluster.\n",
>>>>>>> correcciones
    "\n",
    "Un resumen de los valores numéricos obtenidos se encuentran en la {numref}`comp-metricasnumericas-RnD`.\n",
    "\n",
    "```{table} Métricas numéricas obtenidas con el pipeline de *benchtools* para el conjunto R&D.\n",
    ":name: comp-metricasnumericas-RnD\n",
    "\n",
    "| Clasificador                  | Exactitud balanceada |   Precisión | Recuperación | Puntaje f1  |\n",
    "|:-----------------------------:|:--------------------:|:-----------:|:------------:|:-----------:|\n",
    "| Random classification         |            0.50      |   0.09      |  0.15        |  0.50       |\n",
    "| TensorflowClassifier          |            0.91      |   0.50      |  0.90        |  0.65       |\n",
    "| RandomForestClassifier        |            0.81      |   0.86      |  0.64        |  0.73       |\n",
    "| GradientBoostingClassifier    |            0.79      |   0.81      |  0.60        |  0.69       |\n",
    "| QuadraticDiscriminantAnalysis |            0.81      |   0.70      |  0.66        |  0.68       |\n",
    "| MLPClassifier                 |            0.86      |   0.84      |  0.74        |  0.79       |\n",
    "| KMeans                        |            0.79      |   0.26      |  0.83        |  0.39       |\n",
    "| GAN-AE                        |            0.80      |   0.40      |  0.84        |  0.54       |\n",
    "| UCluster                      |            0.81      |   0.03      |  0.89        |  0.06       |\n",
    "\n",
    "```\n",
<<<<<<< HEAD
    "### Métricas gráficas\n",
    "Las métricas gráficas obtenidas con el pipeline de `benchtools` son la eficiencia de señal vs. rechazo de fondo, la ROC inversa, la curva precisión recuperación y la mejora significativa."
=======
    "### Métricas bidimensionales\n",
    "En la sección anterior se consideró la clasificación después de cortar en un umbral específico para cada clasificador. A continuación, observaremos las métricas bidimensionales, que proporcionan información más general del rendimiento de los algoritmos, porque se obtiene el rendimiento para múltiples umbrales.\n",
    "\n",
<<<<<<< HEAD
    "Las métricas bidimensionales obtenidas con el pipeline de `benchtools` son la eficiencia de señal vs. rechazo de fondo, la ROC inversa, la curva precisión recuperación y la mejora significativa."
>>>>>>> correcciones-1
=======
    "Las métricas bidimensionales obtenidas con el pipeline de `benchtools` son la *eficiencia de señal vs. rechazo de fondo*, la *ROC inversa*, la *curva precisión recuperación* y la *mejora de la significancia*."
>>>>>>> correcciones
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "lista_imagenes = ['../../figuras/comp-rejection-RnD.png','../../figuras/comp-inverseroc-RnD.png',\n",
    "                '../../figuras/comp-precisionrecall-RnD.png','../../figuras/comp-significance-RnD.png']\n",
    "image_grid(rows=2, columns=2, images=lista_imagenes, name='comp-metricas-graf-RnD', path=PATH_FIGURES, remove=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
    "De acuerdo a todas las métricas gráficas, el modelo MLP es el que tiene un mejor rendimiento, con un AUC de 0.975 y una precisión promedio de 0.862 ({numref}`comp-metricas-graf-RnD`).\n",
=======
    "De acuerdo a todas las métricas bidimensionales, el modelo MLP es el que tiene un mejor rendimiento, con un AUC de 0.975 y una precisión promedio de 0.862 ({numref}`comp-metricas-graf-RnD`).\n",
>>>>>>> correcciones-1
=======
    "De acuerdo a todas las métricas bidimensionales, el modelo MLP es el que tiene un mejor rendimiento al clasificar el conjunto R&D, con un AUC de 0.975 y una precisión promedio de 0.862, como se observa en la {numref}`comp-metricas-graf-RnD`.\n",
>>>>>>> correcciones
    "```{figure} ../../figuras/comp-metricas-graf-RnD.png\n",
    "---\n",
    "width: 90%\n",
    "name: comp-metricas-graf-RnD\n",
    "---\n",
<<<<<<< HEAD
    "Metricas gráficas para la clasificación del conjunto R&D. De izquierda derecha, en la fila superior: eficiencia de señal vs. rechazo de fondo, ROC inversa, y en la fila inferior: precisión-recuperación y mejora significativa.\n",
    "```\n",
    "Del gráfico de eficiencia de señal vs. rechazo de fondo y del gráfico de precisión-recuperación, observamos que el clasificador de TensorFlow se encuentra en segundo lugar con un AUC de 0.969 y una precisión media de 0.832. Sin embargo, en los gráficos de la curva ROC inversa y de mejora significativa, no es evidente cuál da un mejor resultado entre el clasificador de TensorFlow y RFC. \n",
    "\n",
    "Seguido de estos modelos, se encuentran GBC y QDA en todas las métricas.\n",
    "\n",
    "Por último, las curvas de los modelos UCluster y GAN-AE se cruzan múltiples veces en los gráficos de eficiencia de señal vs. rechazo de fondo, la ROC inversa y la mejora significativa, por lo que es difícil determinar cuál de los algoritmos da un mejor resultado. De acuerdo al AUC, GAN-AE da un mejor resultado que UCluster. Sin embargo, según la curva de precisión-recuperación, GAN-AE tiene un mejor resultado, con una precisión media de 0.521, mientras que la clasificación de UCluster da una precisión promedio de 0.130.\n",
    "\n",
    "Por último, KMeans obtuvo el menor AUC y la curva de este modelo se encuentra por debajo de todos los demás modelos en todos los gráficos excepto el de precisión-recuperación. De acuerdo a este gráfico, KMeans obtiene un mejor resultado, con una precisión promedio de 0.130.\n",
=======
    "Metricas bidimensionales para la clasificación del conjunto R&D. De izquierda derecha, en la fila superior: eficiencia de señal vs. rechazo de fondo, ROC inversa, y en la fila inferior: precisión-recuperación y mejora significativa.\n",
    "```\n",
    "Del gráfico de eficiencia de señal vs. rechazo de fondo y del gráfico de precisión-recuperación, observamos que el clasificador de TensorFlow se encuentra en segundo lugar con un AUC de 0.969 y una precisión media de 0.832. Seguido de estos modelos, se encuentran RFC, GBC y QDA en todas las métricas.\n",
    "\n",
    "Los modelos no supervisados obtuvieron menores puntajes que los clasificados. De acuerdo al AUC, GAN-AE obtiene un mejor resultado que UCluster, con 0.866 y 0.865, respectivamente. GAN-AE también obtiene un mejor resultado según la curva de precisión-recuperación, con una precisión media de 0.521, mientras que la clasificación de UCluster da una precisión media de 0.060.\n",
    "\n",
<<<<<<< HEAD
    "Por último, KMeans obtuvo el menor AUC y la curva de este modelo se encuentra por debajo de todos los demás modelos en todos los gráficos, excepto el de precisión-recuperación. De acuerdo a este gráfico, KMeans obtiene un mejor resultado que UCluster, con una precisión promedio de 0.130.\n",
>>>>>>> correcciones-1
=======
    "Por último, KMeans obtuvo el menor AUC y la curva de este modelo se encuentra por debajo de todos los demás modelos en todos los gráficos, excepto el de precisión-recuperación. De acuerdo a este gráfico, KMeans obtiene un mejor resultado que UCluster, con una precisión promedio de 0.130 y 0.060, respectivamente.\n",
>>>>>>> correcciones
    "\n",
    "Las razones por las que algunos modelos tienen mejor rendimiento que otros al clasificar este conjunto de datos, se discutirán en el [siguiente capítulo](discusion).\n",
    "\n",
    "En este trabajo, es de interés conocer qué tan bien funcionan los algoritmos al clasificar señal y fondo en conjuntos de datos desconocidos para los modelos, con variaciones en las distribuciones de masa, como el conjunto BB1.\n",
    "\n",
    "(comp-BB1)=\n",
    "## Conjunto BB1\n",
    "A diferencia de las predicciones realizadas con el cojunto R&D, las predicciones para el conjunto BB1 se hacen sobre todos los eventos, utilizando los modelos entrenados previamente con el 70% del conjunto R&D. Las métricas obtenidas en este apartado proveen una medida más cercana al verdadero rendimiento de los algoritmos para detectar anomalías.\n",
    "\n",
    "### Métricas numéricas\n",
    "En la {numref}`comp-metricas-num-BB1` vemos que las métricas varían con respecto a lo obtenido para el conjunto R&D. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "lista_imagenes = ['../../figuras/comp-balancedacc-BB1.png','../../figuras/comp-precision-BB1.png',\n",
    "                '../../figuras/comp-recall-BB1.png','../../figuras/comp-f1score-BB1.png']\n",
    "image_grid(rows=2, columns=2, images=lista_imagenes, name='comp-metricas-num-BB1', path=PATH_FIGURES, remove=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../figuras/comp-metricas-num-BB1.png\n",
    "---\n",
    "width: 100%\n",
    "name: comp-metricas-num-BB1\n",
    "---\n",
    "Diagramas de barras de las métricas numéricas obtenidas utilizando el pipeline de `benchtools` para el conjunto BB1. De izquierda a derecha, en la fila superior: exactitud balanceada y precisión. En la fila inferior: recuperación y puntaje f1.\n",
    "```\n",
    "La exactitud balanceada sigue siendo alta para todos los algoritmos pero la precisión disminuyó notablemente para todos los modelos. En este conjunto, la mayor precisión fue lograda por RFC con un 3% de precisión, contrario a la precisión obtenida para el conjunto R&D, de 86%. Esto implica que los algoritmos clasifican más eventos de fondo incorrectamente, o menos eventos de señal correctamente, en el conjunto BB1.\n",
    "\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "La recuperación aumentó para los modelos GBC, GAN-AE y UClueste. Para UCluster, GAN-AE, KMeas y el clasificador de TensorFlow, la recuperación se mantuvo cercana o sobre el 80%. La baja precisión y alta recuperación indica que los modelos están clasificando una gran cantidad de eventos de fondo como señal. Sin embargo, en estos eventos clasificados como señal logran clasificar la señal real.\n",
=======
    "La recuperación aumentó para los modelos GBC, GAN-AE y UClueste. Para UCluster, GAN-AE, KMeas y el clasificador de TensorFlow, la recuperación se mantuvo cercana o sobre el 80%. La baja precisión y alta recuperación indica que los modelos están clasificando una gran cantidad de eventos de fondo como señal. Sin embargo, en estos eventos clasificados como señal, logran clasificar la señal real.\n",
>>>>>>> correcciones-1
=======
    "La recuperación aumentó para los modelos GBC, GAN-AE y UCluster. Para UCluster, GAN-AE, KMeas y el clasificador de TensorFlow, la recuperación se mantuvo cercana o sobre el 80%. La baja precisión y alta recuperación indica que los modelos están clasificando una gran cantidad de eventos de fondo como señal. Sin embargo, en estos eventos clasificados como señal, logran clasificar la señal real.\n",
>>>>>>> correcciones
    "\n",
    "Debido a la baja precisión, el puntaje f1 se vio afectado y disminuyó notablemente para todos los clasificadores. El clasificador con mayor puntaje es RFC, con un puntaje f1 de 0.05, mientras que en el conjunto R&D obtuvo 0.73. \n",
    "\n",
    "Nuevamente, el clasificador aleatorio obtuvo métricas más bajas que todos los modelos, excepto en la precisión y el puntaje f1, donde UCluster obtuvo las métricas más bajas. \n",
    "\n",
    "```{table} Métricas numéricas obtenidas con el pipeline de *benchtools* para el conjunto BB1.\n",
    ":name: comp-metricasnumericas-BB1\n",
    "\n",
    "| Clasificador                  | Exactitud balanceada |   Precisión | Recuperación | Puntaje f1  |\n",
    "|:-----------------------------:|:--------------------:|:-----------:|:------------:|:-----------:|\n",
    "| Random classification         |            0.50      |    0.0008   |   0.50       |    0.0017   |\n",
    "| TensorflowClassifier          |            0.84      |    0.0049   |   0.82       |    0.0096   |\n",
    "| RandomForestClassifier        |            0.70      |    0.0362   |   0.41       |    0.0666   |\n",
    "| GradientBoostingClassifier    |            0.72      |    0.0367   |   0.44       |    0.0678   |\n",
    "| QuadraticDiscriminantAnalysis |            0.69      |    0.0264   |   0.39       |    0.0495   |\n",
    "| MLPClassifier                 |            0.81      |    0.0028   |   0.91       |    0.0055   |\n",
    "| KMeans                        |            0.81      |    0.0039   |   0.79       |    0.0078   |\n",
    "| GAN-AE                        |            0.91      |    0.0201   |   0.88       |    0.0393   |\n",
    "| UCluster                      |            0.81      |    0.0002   |   0.98       |    0.0003   |\n",
    "```\n",
<<<<<<< HEAD
    "### Métricas gráficas\n",
    "Las métricas gráficas también varían con respecto a las obtenidas utilizando el conjunto R&D. Los resultados se observan en la {numref}`comp-metricas-graf-BB1`."
=======
    "### Métricas bidimensionales\n",
    "Las métricas bidimensionales también varían con respecto a las obtenidas utilizando el conjunto R&D. Los resultados se observan en la {numref}`comp-metricas-graf-BB1`."
>>>>>>> correcciones-1
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "lista_imagenes = ['../../figuras/comp-rejection-BB1.png','../../figuras/comp-inverseroc-BB1.png',\n",
    "                '../../figuras/comp-precisionrecall-BB1.png','../../figuras/comp-significance-BB1.png']\n",
    "image_grid(rows=2, columns=2, images=lista_imagenes, name='comp-metricas-graf-BB1', path=PATH_FIGURES, remove=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../figuras/comp-metricas-graf-BB1.png\n",
    "---\n",
    "width: 90%\n",
    "name: comp-metricas-graf-BB1\n",
    "---\n",
<<<<<<< HEAD
    "Metricas gráficas obtenidas para el conjunto BB1. De izquierda derecha, en la fila superior: eficiencia de señal vs. rechazo de fondo, ROC inversa, y en la fila inferior: precisión-recuperación y mejora significativa.\n",
=======
    "Metricas bidimensionales obtenidas para el conjunto BB1. De izquierda derecha, en la fila superior: eficiencia de señal vs. rechazo de fondo, ROC inversa, y en la fila inferior: precisión-recuperación y mejora significativa.\n",
>>>>>>> correcciones-1
    "```\n",
    "De acuerdo al AUC, el rendimiento mejoró en este conjunto para KMeans y GAN-AE, y disminuyó para los demás modelos. Sin embargo, de acuerdo a la precisión promedio, todos los modelos tuvieron menor rendimiento al clasificar este conjunto.\n",
    "\n",
    "En todos los gráficos, es evidente que GAN-AE tiene un mejor rendimiento en este conjunto de datos que los demás modelos. Que un modelo no-supervisado haya logrado una mejor clasificación en este conjunto indica que tiene una mejor capacidad de generalización. Esta capacidad de generalización no se observó en KMeans y UCluster, siendo estos los clasificadores con menor rendimiento.\n",
    "\n",
    "De acuerdo al gráfico de eficiencia de señal vs. rechazo de fondo, el algoritmo con mejores resultados después de GAN-AE es MLP, con un AUC de 0.953. Seguido a estos modelos, no es evidente cuál da un mejor resultado entre GBC y RFC y entre QDA y el clasificador de TensorFlow. De acuerdo al AUC, a MLP le sigue GBC, RFC, el clasificador de TensorFlow, QDA y RFC. Los algoritmos no supervisados KMeans y UCluster son los que tuvieron menor rendimiento, donde UCluster obtuvo el rendimiento más bajo.\n",
    "\n",
    "De la métrica de precisión-recuperación también se obtuvo un mejor resultado para GAN-AE en comparación a los demás modelos, seguido de GBC, el clasificador de TensorFlow y QDA, que obtuviero una mayor precisión promedio que el modelo MLP. A este le sigue RFC y por último los modelos no-supervisados KMeans y UCluster, donde UCluster obtuvo la menor presición promedio, de 0.000. Este mismo orden de redimiento de los modelos se obtiene en el gráfico de mejora significativa.\n",
    "\n",
    "En general, las curvas de UCluster y KMeans se mantienen cercanas a las de un clasificador aleatorio, excepto en el gráfico de eficiencia de señal vs. rechazo de fondo, donde se obtienen curvas que indican una clasificación mejor a la del clasificador aleatorio. \n",
    "\n",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
    "Por último, es evidente que GAN-AE tiene un mejor rendimiento en este conjunto de datos de acuerdo a todas las métricas gráficas.\n",
    "\n",
    "```{bibliography}\n",
    ":style: unsrt\n",
    "```"
=======
    "De la métrica de precisión-recuperación, GBC, el clasificador de TensorFlow y QDA obtuviero una mayor precisión promedio que el modelo MLP. Este mismo orden de los modelos se obtiene en el gráfico de mejora significativa."
>>>>>>> resultados
=======
    "De la métrica de precisión-recuperación, GBC, el clasificador de TensorFlow y QDA obtuviero una mayor precisión promedio que el modelo MLP. Este mismo orden de los modelos se obtiene en el gráfico de mejora significativa."
>>>>>>> correcciones-1
=======
    "En el siguiente capítulo, se discutirán las razones por las que los algoritmos disminuyen su rendimiento en este conjunto de datos y por qué GAN-AE tiene mejor rendimiento que los demás modelos."
>>>>>>> correcciones
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "be244558907c567e73a32fad5ffef5514602d6da01bb2b6b51508d7e46fcc84d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
