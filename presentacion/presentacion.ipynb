{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T17:24:30.796417Z",
     "start_time": "2022-04-19T17:24:30.789287Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<style>\n",
    "header {\n",
    "    padding: 0.5px;\n",
    "}\n",
    "footer {\n",
    "  padding: 0.5px;\n",
    "  background-color: #70c6ca;\n",
    "  color: white;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div class=\"header\">\n",
    "    <div style=\"display:inline-block;vertical-align:top;\">\n",
    "    <img src=\"../tesis/figuras/logo_ucv.png\" width=\"100\" align=\"left\"/>\n",
    "    </div>\n",
    "    <div style=\"display:inline-block;\">\n",
    "    <p>\n",
    "    Universidad Central de Venezuela<br>\n",
    "            Facultad de Ciencia<br>\n",
    "            Escuela de Física\n",
    "    </p>\n",
    "</div>\n",
    "    <p>\n",
    "\n",
    "<h2 align=\"center\"> Búsqueda de nueva física utilizando técnicas de aprendizaje automático en eventos de múltiples jets </h2>\n",
    "<h3 align=\"center\"> Análisis comparativo de algoritmos de clasificación en términos de reproducibilidad y rendimiento </h3>\n",
    "\n",
    "\n",
    "\n",
    "<footer>\n",
    "   <p style='text-align: left;'> \n",
    "       &emsp;<strong>Autor</strong>: Mariana Vivas (<a href=\"mailto:marianaivivas@gmail.com\">marianaivivas@gmail.com</a>)<br>\n",
    "       &emsp;<strong>Tutores</strong>: Dra. Reina Camacho Toro (LPNHE/CNRS), Dr. José López (UCV) <br>\n",
    "       &emsp;<strong>Co-tutor</strong>: Dra. Camila Rangel Smith (The Alan Turing Institute)\n",
    "    </p>\n",
    "    \n",
    "</footer>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <font color='#307a71'>Introducción</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Este trabajo está enfocado en la búsqueda de nueva física utilizando aprendizaje automático, pero ¿por qué buscar nueva física? ¿por qué utilizar aprendizaje automático?\n",
    "\n",
    "Desde que finalizó el planteamento teórico principal del modelo estándar, los programas en centros de investigación de física de partículas, como el Gran Colisionador de Hadrones (LHC), se enfocaron en comprobar las predicciones hechas con este modelo. El modelo estándar intenta clasificar y describir las partículas fundamentales y sus interacciones, y las partículas y la mayoría de los fenómenos que predice se han corroborado experimentalmente de manera exitosa, convirtiendo al modelo estándar en una de las teorías más exitosas de la física moderna. Sin embargo, sabemos que no es una teoría completa ya que presenta múltiples inconsistencias teóricas y experimentales, como por ejemplo, que no incluye todas las fuerzas fundamentales conocidas: la fuerza gravitatoria no es parte del modelo. Por esto, desde el hallazgo del bosón de Higgs en 2012, que era una de las últimas predicciones a comprobar del modelo, los centros de investigación se han enfocado en buscar partículas y fenómenos que pudieran explicar algunas de las limitaciones del modelo, es decir, buscar nueva física o física más allá del modelo estándar.\n",
    "\n",
    "Sin embargo, han pasado 10 años desde el hallazgo del bosón de Higgs, y no se han encontrado nuevas partículas ni nuevos fenómenos. En paralelo a esta búsqueda, el gran colisionador de hadrones aumenta cada vez más su energía de colisión, lo que implica una mayor cantidad de datos y eventos de colisión más complejos, por lo que el análisis de estos datos se puede ver limitado por las herramientas disponibles. Múltiples esfuerzos se han hecho para desarrollar y estudiar herramientas que ayuden a resolver alguna de estas problematicas. Una de estas herramientas es el aprendizaje automático, que trabaja con grandes cantidades de datos y es apta para hacer búsquedas más generales: no es necesario buscar una partícula en específico. Sin embargo, su aplicación en la búsqueda de nueva física no es tan sencilla porque los eventos de colisiones no proveen datos parecidos a los datos utilizados en el desarrollo de estos modelos. Por lo que es necesario estudiar su uso con este proposito.\n",
    "\n",
    "Con este objetivo se plantean las olimpiadas LHC 2020, donde de forma abierta se proporcionaron datos para que cualquier grupo interesado en estudiar el uso de aprendizaje automático como herramienta para hallar nuevas partículas pudiera participar. Sin embargo los resultados de cada modelo no son directamente comparables: no utilizaron una métrica general para analizar el rendimiento de los modelos, más allá del resultado de la búsqueda. A partir de esto se plantea este proyecto con el objetivo de comparar algoritmos de aprendizaje automático en la tarea de clasificación de eventos. Además, siguiendo parte de la idea principal de las olimpiadas LHC 2020, que es un evento para buscar nueva física pero para hacerlo de forma abierta y en comunidad, el proyecto tiene un enfoque en la reproducibilidad científica de estos nuevos métodos, es decir, estudiamos diferentes herramientas y lineamientos para aumentar la reproducibilidad científica al utilizar estas herramientas. \n",
    "\n",
    "De hecho este trabajo está documentado en distintos formatos. La monografía se encuetra publicamente disponible en un jupyter book en https://marianaiv.github.io/tesis_grado_UCV/intro.html y las herramientas utilizadas se encuentran en un paquete de software basado en python que se discutirá más adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Contenido\n",
    "- Marco teórico.\n",
    "- Aprendizaje automático para la búsqueda de nueva física.\n",
    "- Datos y métodos.\n",
    "- Exploración de datos.\n",
    "- Resultados.\n",
    "- Conclusiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click para mostrar/ocultar las celdas de código.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import os.path\n",
    "\n",
    "# De scikit-learn importamos herramientas\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Lo necesario para construir el modelo de tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Funciones de benchtools\n",
    "from benchtools.src.plotools import pred_test_hist, image_grid, bkg_sig_scatter\n",
    "from benchtools.src.clustering import build_features\n",
    "from benchtools.src.datatools import separate_data\n",
    "from benchtools.scripts.run import evaluate\n",
    "\n",
    "# Definimos semillas para la reproducibilidad\n",
    "tf.random.set_seed(125)\n",
    "np.random.seed(6)\n",
    "\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "#HTML(\"\"\"\n",
    "#<style>\n",
    "#.output_png {\n",
    "#    display: block;\n",
    "#    text-align: center;\n",
    "#    align-items: middle;\n",
    "#}\n",
    "#</style>\n",
    "#\"\"\")\n",
    "\n",
    "CSS = \"\"\"\n",
    ".output {\n",
    "    align-items: center;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "HTML('<style>{}</style>'.format(CSS))\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click para mostrar/ocultar las celdas de código.\"></form>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <font color='#307a71'>Marco teórico</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## El modelo estándar\n",
    "Es la teoría cuántica relativista de campos que intenta describir las partículas elementales y sus interacciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "----\n",
    "El modelo estándar de física de partículas es una de las teorías más exitosas de la física moderna. Intenta clasificar los componentes más fundamentales de la materia y sus interacciones mediante tres de las cuatro fuerzas fundamentales: la interacción electromagnética, la interacción débil y la interacción fuerte. Es una teoría cuántica relativista de campos en la cual las partículas corresponden a excitaciones de campos. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<figure>\n",
    "<center><img src=\"../tesis/figuras/sm-particulas.png\" style=\"width:75%\"></center>\n",
    "<figcaption align = \"center\"> <small>Fig.1: Contenido de partículas del modelo estándar. Los leptones están organizados de acuerdo a sus generaciones <a href=\"https://es.wikipedia.org/wiki/Archivo:Standard_Model_of_Elementary_Particles.svg\">[ref]</a>.</small></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "En el modelo estándar se contemplan dos tipos de partículas: fermiones y bosones, y dos teorías principales: la teoría electrodébil, que incluye la interacción electromagnética y la interacción débil, y la cromodinámica cuántica, que intenta explicar la interacción fuerte.\n",
    "\n",
    "Los fermiones son los componentes de la materia visible. Poseen espín 1/2 y se rigen por la estadística de Fermi-Dirac. Existen dos tipos: leptones, que interactuan mediante la fuerza electrodébil, y quarks, que interactuán mediante la fuerza fuerte. Los fermiones están organizados en generaciones, como se observa en la Fig.1. Cada generación consta de dos quarks, con carga 2/3 y -1/3, y dos leptones, uno cargado y uno neutro.\n",
    "\n",
    "Los bosones son las partículas intermediarias de las interacciones, por lo que cada bosón está asociado a una interacción fundamental. Poseen espín entero y se rigen por la estadística de Einstein-Bose. El gluon es la partícula intermediaria de la interacción fuerte, el fotón de la interacción electromagnética y los bosones Z y W de la interacción débil. Por último, el boson de Higgs es la manifestación experimental del campo escalar de Higgs, responsable de dar masa a las partículas del modelo (exceptuando los neutrinos).\n",
    "\n",
    "En este trabajo nos interesan los fenómenos relacionados a la interacción fuerte, es decir, nos enfocamos en la cromodinámica cuántica. Como se mencionó anteriormente, la cromodinámica cuántica es la teoría de la interacción fuerte, donde se describe la dinámica de los quarks y gluones. El gluón es el bosón de la teoría, y al igual que los quarks, posee carga de color.\n",
    "\n",
    "La carga de color es el análogo a la carga eléctrica y es el número cuántico conservado en la teoría. Tanto los quarks como los gluones poseen carga de color. Que los gluones poseean carga de color hace que la teoría sea más complicada, puesto que los gluones presentan interacción propia. \n",
    "\n",
    "Los quarks son partículas que se encuentran confinadas en grupos de dos o más quarks y con carga de color neutro, conocidos como hadrones.\n",
    "\n",
    "Nos interesan particularmente dos características fundamentales de esta teoría: la libertad asintótica y el confinamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cromodinámica cuántica\n",
    "Es la teoría de la interacción fuerte y describe la dinámica de los quarks y gluones.\n",
    "\n",
    "<img style=\"float: right; width:60%\" src=\"../tesis/figuras/qcd-alphas.png\">\n",
    "\n",
    "**Libertad asintótica** \n",
    "\n",
    "   La constante de acople $\\alpha_s$ disminuye al aumentar la transferencia de momento $Q$ y desaparece asintóticamente.\n",
    "\n",
    "**Confinamiento**\n",
    "    \n",
    "   La fuerza de la interacción aumenta a pequeñas transferencias de momento.\n",
    "  \n",
    "<figcaption align = \"right\"> <small>Fig.2: Resumen de medidas experimentales de $\\alpha_s$ en función de la escala de energía $Q$ <a href=\"https://doi.org/10.1051/epjconf/201612007005\">[ref]</a></small></figcaption> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "En teoría de campos, el acople efectivo hace referencia a la fuerza de la interacción, y esta se ve modificada por la interacción.\n",
    "\n",
    "La libertad asintótica hace explica una característica observada en las colisiones de hadrones: los quarks en hadrones a altas energías se comportan como partículas libres, es decir, cuando la tranferencia de momento en una colisión es alta, la fuerza de la interacción fuerte disminuye. Esto se observa en la Fig.2, vemos que en valores más a la derecha en el eje x, donde se tiene la transferencia de momento, la constante de acople es menor.\n",
    "\n",
    "Contrario a la libertad asintótica se tiene el concepto de confinamiento, que justifica por que los quarks se encuentran en grupos. El confinamiento explica que a bajas energias, o pequeña tranferencia de momento, la fuerza de la interacción fuerte aumenta, por lo que los quarks se encuentran estrechamente unidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Formación de jets\n",
    "<figure>\n",
    "<center><img src=\"../tesis/figuras/presentacion-formacionjet.png\" style=\"width:60%\"></center>\n",
    "<figcaption align = \"center\"> <small>Fig.3: Esquema de la formación de jets [1].</small></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "La libertad asintótica y el confinamiento son los procesos que explican la formación de jets en colisiones de protones a altas energías. Los jets son lluvias colimadas de partículas de color neutro. Debido a la alta tasa de producción de jets en eventos de colisión, se han convertido en objeto de estudio para mejorar la comprensión del modelo estándar, así como para buscar nueva física.\n",
    "\n",
    "Los procesos involucrados en la formación de jets se pueden observar en la fig. 3:\n",
    "\n",
    "El proceso de colisión se conoce como **dispersión fuerte**, y es el proceso principal en la formación de jets. Los protones colisionan a altas energías, produciendo una interacción con alta transferencia de momento, donde los quarks y gluones en los protones (llamados también partones) se comportan como partículas libres debido a la libertad asintótica. En este proceso se puede crear una partícula resonante de corta vida o puede suceder un proceso de QCD estándar. A partir del resultado de la colisión se pueden producir decaimientos y/o procesos de QCD, generando otras partículas.\n",
    "\n",
    "También se considera la **radiación de estado inicial y de estado final**, donde las partículas entrantes y salientes pueden radiar otras partículas.\n",
    "\n",
    "Por último se encuentran los **eventos subyacentes**, que son las interacciones entre partones que no participan en la dispersión fuerte y que pueden generar otras partículas.\n",
    "\n",
    "Mediate todos estos procesos se generan partículas que forman una lluvia que se dice colimada porque las partículas se generan a ángulos pequeños del partón original. Una vez que se alcanzan bajas energías el confinamiento domina el proceso, por lo que las partículas creadas se unen para formar partículas de color neutro. Esto se conoce como **hadronización**. A la colección de todos los hadrones resultantes cerca de la dirección del partón original se le llama jet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T18:14:34.553747Z",
     "start_time": "2022-04-19T18:14:34.543052Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Más allá del modelo estándar\n",
    "Limitaciones **fenomenológicas**:\n",
    "\n",
    "- Asimetría materia-antemateria\n",
    "- Masa de los neutrinos\n",
    "- Materia oscura y energía oscura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "A pesar de que se han verificado exitosamente múltiples predicciones hechas a partir del modelo estándar, es evidente que no es una teoría final debido a que no explica varios fenómenos observados experimentalmente. Las limitaciones del modelo estándar se pueden separar en fenomenológicas y teóricas.\n",
    "\n",
    "Las limitaciones fenomenológicas hacen referencia a observaciones experimentales para las que el modelo estándar no tiene explicación, entre ellas: \n",
    "- La asimetría materia-antimateria: de acuerdo al modelo estándar, se esperaría igual cantidad de materia que anti-materia. Sin embargo, se observa mucha más materia que anti-materia.\n",
    "- La masa de los neutrinos: los neutrinos se suponía que eran partículas no-masivas. Aunque se puede corregir el modelo estándar para que posean masa, no se conoce el mecanismo mediante el cual la obtienen.\n",
    "- Materia oscura y energía oscura: la existencia de la materia oscura se ha inferido por los efectos gravitacionales que tiene en la materia visible y la energía oscura se utiliza para explicar la tasa de expansión del universo y su aceleración. Sin embargo, la naturaleza de ambas no está incluida en el modelo estándar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T18:14:34.587384Z",
     "start_time": "2022-04-19T18:14:34.556811Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Limitaciones **teóricas**:\n",
    "- Descripción cuántica de la gravedad\n",
    "- Origen de las masas/mezclas\n",
    "- Problema CP fuerte\n",
    "- Problema de jerarquía"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Las limitaciones teóricas hacen referencia a predicciones no observadas o valores y parámetros para los que la teoría no tiene explicación. Entre estas se encuentran:\n",
    "- Descripción cuántica de la gravedad: la gravedad no se encuentra incluida en el modelo.\n",
    "- Origen de las masas/mezclas: el modelo estándar no explica el origen del mecanismo de Brout–Englert–Higgs, solo se sabe que es necesario para poder coincidir con las observaciones experimentales. Además, las masas de los fermiones y los ángulos de mezcla de los quarks parecen presentar un patrón para el cual no se tiene explicación.\n",
    "- Problema CP fuerte: En la QCD se predice violación carga-paridad (CP), pero esto no se ha observado experimentalmente.\n",
    "- Problema de jerarquía: teóricamente la masa del bosón de Higgs debería ser del orden de la escala de Planck. Sin embargo, el valor experimentar hallado es $10^{-19}$ órdenes de magnitud menor.\n",
    "\n",
    "A partir de estas limitaciones, se han planteado múltiples teorías que intentan responder las preguntas para las que el modelo estándar no tiene explicación. Estás teorías se conocen como teorías más allá del modelo estándar (BSM) y su verificación/refutación es uno de los objetivos del programa del LHC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Eventos dijet\n",
    "Múltiples modelos de física más allá del modelo estándar (BSM) plantean la existencia de nuevas pactículas. Son de especial interés los eventos que decaen a dos jets porque:\n",
    "- Una alta fracción de los eventos de colisión resultan en jets.\n",
    "- Varios modelos predicen la existencia de nuevas partículas masivas que decaen a dos jets.\n",
    "- Los jets son la firma experimental de los quarks y gluones, productos en el decaimiento de los bosones *W/Z* y del bosón de Higgs.\n",
    "\n",
    "<figure>\n",
    "<center><img src=\"http://cds.cern.ch/record/2669151/files/ATLAS_Experiment_dijet_event.png\" style=\"width:60%\"></center>\n",
    "<figcaption align = \"center\"> <small>Fig.4: Evento de dos jets. <a href=\"http://cds.cern.ch/record/2669151/files/ATLAS_Experiment_dijet_event.png\">[ref]</a></small></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "La búsqueda de nuevas partículas es parte fundamental en la búsqueda de física BSM. La topología dijet es de particular interés y es la utilizada en este proyecto.\n",
    "\n",
    "Varios modelos de física BSM predicen la existencia de nuevas partículas masivas que decaen a dos jets. Además, los jets son la firma experimental de los quarks y gluones, y son productos en el decaimiento hadrónico de los bosones W/Z, y el estado final del decaimiento a quarks del bosón de Higgs. Así, su estudio no está motivado únicamente por la búsqueda de nueva física, sino también para ganar una mejor comprensión de la QCD y del modelo estándar.\n",
    "\n",
    "(Históricamente, uno de los métodos utilizados para descubrir nuevas partículas es la búsqueda de estructuras de resonancia en espectros de masas invariantes de los productos de desintegración de la partícula. A diferencia de las búsquedas dirigidas a estados finales más complejos, para una topología específica, las búsquedas de resonancia de dos cuerpos solo son sensibles a dos parámetros: la masa de la nueva partícula y la sección transversal de producción.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Agrupamiento de jets\n",
    "La existencia de un jet es dependiente de la regla matemática que lo define. Esta regla matemática agrupa los constituyentes del jet de acuerdo a propiedades cinemáticas.\n",
    "\n",
    "### Recombinación secuencial \n",
    "\n",
    "<figure>\n",
    "<center><img src=\"../tesis/figuras/presentacion-algrecombinacion.png\" style=\"width:100%\"></center>\n",
    "<figcaption align = \"center\"> <small>Fig.5: Algoritmos de recombinación secuencial. </small></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Para poder estudiar los eventos de jets, hace falta reconstruirlos, puesto que lo que se mide de los eventos de jets son características de las partículas de color neutro resultado de la hadronización. \n",
    "\n",
    "La definición de un jet no es única. Su existencia es dependiente de la regla matemática que lo define. Esta regla matemática se conoce como algoritmo de agrupamiento de jets. En este trabajo partícularmete, vamos a utilizar un algoritmo de recombinación secuencial.\n",
    "\n",
    "Los algoritmos de recombinación secuencial asumen la diferencia en el momento transverso de los los constituyentes de un jet es pequeña, por lo que las partículas son agrupadas en el espacio de momento (en vez de en el espacio eta-phi). Utilizan las medidas de distancia entre dos constituyentes, y la distancia entre el eje del haz y el constituyente detectado. El algoritmo a utilizar en el trabajo se conoce como **anti-kt** que resulta en jets circulares en el espacio de ángulo azimutal(phi) y pseudorapidez(eta).\n",
    "\n",
    "(El algoritmo halla el minímo entre la distancia entre constituyentes y la distancia entre el eje del haz y el constituyente. Si el mínimo se halla entre dos constituyentes, se unen en un solo constituyente, se eliminan individualemnte de la lista de constituyentes y se repite le evaluación. Si el mínimo es la distancia entre el eje del haz y el constituyente, se considera que se reconstruyó un jet y se elimina de la lista de constituyentes.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Variables de subestructura\n",
    "La subestructura de un jet puede analizarse para diferenciar si el jet proviene de un gluon, un quark, un bosón que decae hadrónicamente o partículas masivas aún no descubiertas.\n",
    "\n",
    "<figure>\n",
    "<center><img src=\"../tesis/figuras/jets-diagramajets.png\" style=\"width:75%\"></center>\n",
    "<figcaption align = \"center\"> <small>Fig.6: Representación de jets producto de: quarks y gluones (arriba a la izquierda), quark bottom (arriba en el centro), y topologías de jets impulsados: bosones de alto $pT$ W y Z (arriba a la derecha), bosón de Higgs (abajo a la izquierda) y quark top (abajo a la derecha) decayendo a un estado final de quarks. <a href=\"https://link.aps.org/doi/10.1103/PhysRevD.102.012010\">[ref]</a></small></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "La suestructura de un jet puede analizarse para diferencias de qué partícula proviene un jet. Como se observa en la fig. 5. A alto momento transverso un bosón, por ejemplo, puede reconstruirse como un jet único debido a que los productos del decaimiento se encuentran muy cercanos. Las variables de subestructura nos ayudan a diferenciar entre jets provinientes de distintos tipos de partículas. En este trabajo utilizamos dos variables de subestructura: masa y subjettiness. Estás se mostrarán más adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <font color='#307a71'>Aprendizaje automático para la búsqueda de nueva física</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Aprendizaje automático\n",
    "El aprendizaje automático es un subcampo de la inteligencia artificial.\n",
    "\n",
    "Hay dos tipos principales de tareas: clasificación y regresión. Estas pueden ser supervisadas y no supervisadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<figure>\n",
    "<center><img src=\"../tesis/figuras/presentacion-sup-unsup.png\" style=\"width:80%\"></center>\n",
    "<figcaption align = \"center\"> <small>Fig.7: Ejemplo de clasificación por algoritmo supervisado y no supervisado <a href=\"https://lawtomated.com/supervised-vs-unsupervised-learning-which-is-better/\">[ref]</a>.</small></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Tiene como objetivo el desarrollo de algoritmos que mejoran su desempeño de manera cuantificable en una tarea determinada.\n",
    "\n",
    "En partícular, nos interesa la tarea de clasificación binaria, donde los algoritmos intentan distinguir entre dos clases.\n",
    "\n",
    "Como se ve en la fig. 8, el aprendizaje no supervisado trata de clasificar elementos con características iguales, sin ningún conocimiento sobre la clase a la que pertencen, es decir, sin etiquetas. Por otra parte, el aprendizaje supervisado utiliza las etiquetas para aprender a clasificar elementos similares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Aprendizaje automático en HEP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "El uso de aprendizaje automático en HEP es amplio. Sin embargo, este trabajo se enfoca en las técnicas de detección de anomalías y búsquedas libres de modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Imagen detección anomalias\n",
    "# Creando las etiquetas con el ratio de BB1\n",
    "label = np.random.choice([0, 1], size=1000, p=[.992, .008])\n",
    "# Creamos los datos \n",
    "xaxis=np.random.rand(1000)\n",
    "yaxis=np.random.rand(1000)\n",
    "# Creamos un df con los datos\n",
    "df = pd.DataFrame(np.stack((xaxis, yaxis, label), axis=1), columns=['x-axis','y-axis','label'])\n",
    "\n",
    "# Graficamos\n",
    "fig, ax = plt.subplots(facecolor='white', figsize=[10,3])\n",
    "# Separamos en señal y fondo\n",
    "sig = df.loc[df['label']==1]\n",
    "bkg = df.loc[df['label']==0]\n",
    "# Plots\n",
    "ax.scatter(sig['x-axis'], sig['y-axis'], c='r', alpha=1, label='señal')\n",
    "ax.scatter(bkg['x-axis'], bkg['y-axis'], c='b', alpha=0.3, label='fondo')\n",
    "# Información\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig('../tesis/figuras/presentacion-conjuntodesb.png', bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Detección de anomalías\n",
    "La tarea de detección de anomalías tiene como objetivo predecir la categoría a la que pertenece una muestra: \"normal\" o \"anómala\".\n",
    "<figure>\n",
    "<center><img src=\"../tesis/figuras/presentacion-conjuntodesb.png\" style=\"width:65%\"></center><figcaption align = \"center\"> <small>Fig 8: conjunto de datos con clases altamente desbalanceadas.</small></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "En HEP se habla de detección de anomalías porque las clases se encuentran altamente desbalanceadas: hay un porcentaje muy bajo de una clase con respecto a la otra. En la fig. 9 se muestra visualmente un ejemplo donde hay 0.8% de señal en una muestra, uno de los casos que se trata en este trabajo. Hasta ahora, parte de la dificultad en la búsqueda recae en que hay muy pocos eventos de interés para la gran cantidad de fondo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Búsquedas independientes de modelo\n",
    "El objetivo de estas búsquedas es que sean lo más agnósticas posibles al proceso físico subyacente que puede ser responsable de la señal de nueva física."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Con la introducción del aprendizaje automático se han propuesto métodos para búsquedas de nueva física que puedan buscar de manera general, es decir, que no busque específicamente una partícula, si no que busquen un tipo de partícula con algunas características generales. El objetivo de estas búsquedas es que sean lo más agnósticas posibles al proceso físico subyacente que puede ser responsable de la señal de nueva física.\n",
    "\n",
    "Idealmente se quiere que las búsquedas sean libres de modelo, no paramétricas y no clasificadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Olimpiadas LHC 2020\n",
    "En las olimpiadas se simulan datos que podrían generarse en el acelerador. Los participantes interpretan los datos, buscando partículas y evidencia de teorías que no se han presenciado o confirmados. \n",
    "\n",
    "Las olimpiadas de 2020 estuvieron enfocadas en el uso de aprendizaje automático para búsqueda de anomalías en eventos con un estado final de múltiples jets.\n",
    "\n",
    "<figure>\n",
    "<center><img src=\"../tesis/figuras/lhco-topologia.png\" style=\"width:55%\"></center>\n",
    "<figcaption align = \"center\"> <small>Fig.9: Un tipo de topología que se puede encontrar en los datos de las LHCO 2020. Evento dijet por el decaimiento de una partícula de nueva física en dos partículas de nueva física que decaen a jets <a href=\"https://lhco2020.github.io/homepage/\">[ref]</a>.</small></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "El uso de estas herramientas para este tipo de búsquedas se ha estudiado en los últimos diez años. Un ejemplo de esto son las olimpiadas LHC 2020, un evento dedicado al estudio de aprendizaje automático para buscar nueva física utilizando técnicas de detección de anomalías. En este evento se utilizaron datos simulados de eventos que podrían generarse en los aceleradores y el objetivo es que los participantes interpreten los datos buscando nuevas partículas.\n",
    "\n",
    "Estas olimpiadas estuvieron enfocadas en eventos con un estado final de múltiples jets y proporcionaron tres conjuntos de datos, o cajas negras, disponibles para participar, donde cada caja negra podía contener o no eventos de nueva física."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Resultados LHCO 2020\n",
    "Los participantes debían reportar el número de eventos de señal, una descripción de la nueva física y un valor p asociado a la hipótesis nula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Cada equipo participante reportó el número de eventos de señal en los datos, una descripción de la nueva física y/o la probabilidad de que en el conjunto no hubieran nuevas partículas. En el artículo común se explicó el método utilizado y los resultados obtenidos. Sin embargo, no hubo una métrica común a reportar para comparar el rendimiento de los modelos en la tarea de detecció de anomalías."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Reproducibilidad\n",
    "<center><img src=\"../tesis/figuras/alglhco-repfig.png\" style=\"width:60%\"></center>\n",
    "<figcaption align = \"center\"> <small>Fig.10: Resumen de la reproducibilidad de los participantes de las LHCO 2020.</small></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Además, el uso de estas herramientas requiere nuevas consideraciones sobre la reproducibilidad científica, es decir, la información necesaria para reproducir resultados. En este caso, hubieron 18 equipos participantes, de los cuales solo uno proporcionó información \"suficiente\" para reproducir lo obtenido. En la fig. 11 se muestra la cantidad de equipos que cumplieron con algunas características a tomar en cuenta a la hora de hacer una investigación utilizando aprendizaje automático.\n",
    "\n",
    "El preprocesamiento hace referencia a la información acerca de como se realizó el preprocesamiento de los datos utilizados por los modelos. El código se refiere a si se encontraba públicamente disponible el código utilizado por los participantes. Las instrucciones indican si se publicó información de como usar el código. El entorno se apunta a si se encontraba disponible información sobre el entorno computacional utilizado: versiones de librerias, hardware. Y la licencia indica cuántos grupos participantes le colocaron una licencia de uso a su trabajo, para saber como utilizarlo y compartirlo correctamente.\n",
    "\n",
    "Estas consideraciones son importantes para este trabajo porque el objetivo fue reproducir el resultado de dos algoritmos de las olimpiadas para hacer una comparación directa entre ellos y con modelos más simples que no están enfocados en la búsqueda de nuevas partículas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Barreras para la investigación reproducible\n",
    "<img style=\"float: right; width:60%\" src=\"../tesis/figuras/presentacion-turing.png\">\n",
    "\n",
    "- Estructura actual de la <br> investigación académica\n",
    "\n",
    "- Desafios técnicos\n",
    "\n",
    "- Percepciones de los <br> científicos de datos\n",
    "\n",
    "#### <font color='mediumTurquoise'>The Turing Way</font>\n",
    "\n",
    "Es una guía con la información que los investigadores y científicos de datos necesitan para garantizar que los proyectos en los que trabajan sean fáciles de reproducir y reutilizar.\n",
    "\n",
    "<figcaption align = \"right\"> <small>Fig.11: Fig: Proyecto The Turing Way. Zenodo. <a href=\"http://doi.org/10.5281/zenodo.3332807¶\">[ref]</a><br> </small></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Sin embargo, entendemos que existen múltiples barreras que dificultan hacer investigación reproducible. Entre estas encontramos la estructura actual de la investigación científica, donde publicar todos los pasos de una investigación es desalentado porque hay mayor posibilidad de hallar errores o son sujetas a mayores estándares. Los desafios técnicos, relacionados a la gran cantidad de datos que se utilizan y la complejidad de los análisis, dificultan hacer investigaciones reproducibles utilizando estas herramientas. Además, consideramos las percepciones de los científicos de datos sobre que hacer un trabajo reproducible puede ser más complicado o que hay que aprender nuevas herramientas.\n",
    "\n",
    "Sin embargo, todos estos puntos incluyen los beneficios de hacer ciencia de datos: la facilidad para encontrar errores también es beneficiosa porque se encuentran errores más facilmente en etapas iniciales, se permite una mayor complejidad del analisis porque se comprende más facilmente, además la investigación reproducible es requerida hoy en día por múltiples organizaciones debido a que resulta en investigaciones de mayor calidad, que pueden ser utilizadas más fácilmente por la comunidad científica.\n",
    "\n",
    "Todos estos puntos están discutidos en The Turing Way, una guía sobre investigación abierta y reproducible, que incluye información sobre las herramientas necesarias y consideraciones a tomar a la hora de hacer proyectos de investigación utilizando herramientas como el aprendizaje automático."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <font color='#307a71'>Datos y métodos</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Conjuntos de datos\n",
    "Cada evento está compuesto por una lista de todos los hadrones y su cinemática<br> ($p_T,\\eta,\\phi$). La señal de los conjuntos es $Z'\\rightarrow XY$, donde $Z'$, $X$ y $Y$ son partículas BSM.\n",
    "\n",
    "<img style=\"float: right; width:60%\" src=\"../tesis/figuras/lhco-RnD.png\">\n",
    "\n",
    "### R&D\n",
    "Este conjunto contiene 9.09% de eventos de señal con masas:\n",
    "\n",
    "$Z'=3.5$ TeV<br> $X=500$ GeV<br> $Y=100$ GeV.\n",
    "\n",
    "### BB1\n",
    "Este conjunto contiene 0.083% de señal con masas:\n",
    "\n",
    "$Z'=3.823$ TeV<br> $X=732$ GeV<br> $Y=378$ GeV.\n",
    "\n",
    "<figcaption align = \"right\"> <small>Fig.12: Diagrama de Feynmann para la señal del conjunto R&D y la BB1.</small></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "En este trabajo se utilizaron dos conjuntos de datos proporcionados por las LHCO 2020. Cada conjunto contiene una lista de la cinemática de los hadrones resultado de un evento de dos jets. Cada evento con hasta 700 hadrones.\n",
    "\n",
    "En ambos conjuntos se tiene una señal de la forma $Z'\\rightarrow XY$, como se observa en la figura 13, donde el producto de la colisión es un bosón Z' de nueva física que decae a dos partículas de nueva física X y Y. Sin embargo, los conjuntos difieren en las masas de las partículas y la proporción de señal.\n",
    "\n",
    "Particularmente, el conjunto R&D fue el utilizado por los algoritmos para entrenar los modelos y el conjunto BB1 fue el primer conjunto publicado para participar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Algoritmos para detección de anomalías"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "En este trabajo se implementarion varios algoritmos simples para detección de anomalías con el fin de compararlos con dos algoritmos de las LHCO 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Los algoritmos utilizados en este trabajo se escogieron a partir de su rendimiento, estudiado durante el desarrollo de las herramientas de análisis de datos.\n",
    "\n",
    "| Modelos                             | Tipo de aprendizaje |\n",
    "|:------------------------------------|:-------------------:|\n",
    "| Bosque aleatorio                    | Supervisado         |\n",
    "| Potenciación del gradiente          | Supervisado         |\n",
    "|Análisis de discriminante cuadrático | Supervisado         |\n",
    "| Redes neuronales                    | Supervisado         |\n",
    "| KMeans                              | No supervisado      |\n",
    "| Codificador automático              | No supervisado      |\n",
    "| Red Generativa Antagónica           | No supervisado      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Se escogieron algoritmos implementados en la librería de python `scikit-learn` y se construyó un modelo sencillo utilizando `tensorflow`. A continuación se explicarán brevemente estos modelos, y algunos principios necesarios para entender más adelante los modelos escogidos de las LHCO 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Algoritmos de ensamble\n",
    "<table><tr>\n",
    "<td> <br><h3>Bosque aleatorio</h3>La decisión final se obtiene promediando las predicciones probabilísticas.\n",
    "    <img src=\"../tesis/figuras/ml-bosquealeatorio.png\" alt=\"Drawing\" style=\"width:100%;\"/> <figcaption align = \"center\"> <small>Fig.13: Representación visual del funcionamiento de un bosque aleatorio <a href=\"https://www.tibco.com/reference-center/what-is-a-random-forest\">[ref]</a>.</small></figcaption> </td>\n",
    "<td> <h3>Potenciación del gradiente</h3> Cada árbol se ajusta al error asociado al árbol anterior, con el objetivo de minimizarlo.\n",
    "    <img src=\"../tesis/figuras/ml-gbc.png\" alt=\"Drawing\" style=\"width:100%;\"/> <figcaption align = \"center\"> <small>Fig.14: Ilustración del funcionamiento de GBC <a href=\"https://medium.com/swlh/gradient-boosting-trees-for-classification-a-beginners-guide-596b594a14ea\">[ref]</a>.</small></figcaption></td>    \n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Los métodos de ensamble utilizan conjuntos de algoritmos de aprendizaje automático cuyas decisiones se combinan para mejorar el rendimiento del sistema en general.\n",
    "\n",
    "Los bosques aleatorios y la potenciación del gradiente son algoritmos son ensambles de árboles de decisión. Los árboles de decisión utilizan una serie de preguntas para realizar la partición jerárquica de los datos con el objetivo de hallar un conjunto de reglas que separen el espacio de características.\n",
    "\n",
    "Para crear un bosque aleatorio se forman múltiples árboles simples con subconjuntos de los datos de entrenamiento y cada árbol emite un voto unitario para la clase más popular dada una entrada. Finalmente, la clase con más votos es asignada a esta entrada, como se observa en la figura 14. Sin embargo, la implementación utilizada en este trabajo combina los árboles individuales promediando su predicción probabilística.\n",
    "\n",
    "El algoritmo de potenciación de gradiente (GBC, por sus siglas en inglés) utilizando un método de ensamble conocido como impulso. En este caso, en vez de entrenar múltiples árboles de decisión paralelamente, se entrena cada árbol para minimizar el error del árbol anterior, partiendo de la idea de que hallar varias reglas generales aproximadas puede ser más sencillo que hallar una regla general altamente precisa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Redes neuronales\n",
    "\n",
    "<table width=\"100%\"><tr>\n",
    " <td width=\"(100/2)%\"><h3>Perceptrón multicapas</h3> Es el tipo de red neuronal supervisada más simple.\n",
    "    <img src=\"../tesis/figuras/presentacion-nn.png\" style=\"width:100%;\"/> <br> <br> <figcaption align = \"center\"> <small>Fig.15: Diagrama de una red neuronal. Las transformaciones se ordenan por capas, donde la salida de una capa es la entrada de la siguiente <a href=\"https://www.javatpoint.com/multi-layer-perceptron-in-tensorflow\">[ref]</a>.</small></figcaption> </td>\n",
    " <td width=\"(100/2)%\"><br> <h3>Codificador automático</h3> Redes no supervisadas que mapean una entrada a una representación comprimida en un encaje y luego vuelve a sí misma.\n",
    "     <center><img style=\"width:80% height=15% \" src=\"../tesis/figuras/alg-ae.png\"></center>\n",
    "     <figcaption align = \"center\"> <small>Fig.16: Diagrama del funcionamiento de un codificador automático <a href=\"https://www.compthree.com/blog/autoencoder/\">[ref]</a>.</small></figcaption></td>    \n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Las redes neuronales se definen mediante una serie de transformaciones que mapean la entrada a uno o más estados ocultos hasta una última transformación que mapea los datos a la función de salida.\n",
    "\n",
    "(Las transformaciones se pueden escribir matemáticamente como: $ \\mathbf{h}_i = g_i(W_i\\mathbf{h}_i+\\mathbf{b}_i)$, donde $g_i$ es una función conocida como *función de activación* y $\\mathbf{h}_i$ representa la transformación iésima de $\\mathbf{x}$, llamada *encaje*. $W$ es la matriz de los *pesos* y $\\mathbf{b}$ el vector de los *sesgos*. El objetivo es hallar los pesos y sesgos que optimizan la función de pérdida.)\n",
    "\n",
    "La tarea de la red depende de su arquitectura y aunque su uso es extenso, nos enfocaremos en su aplicación para clasificación binaria. Particularmente, utilizaremos un perceptrón multicapas que es un tipo de red neuronal prealimentada que utiliza la retropropagación para entrenar. Este es el tipo más básico de red neuronal utilizada en aprendizaje supervisado.\n",
    "\n",
    "Los codificadores automáticos (AE, por sus siglas en inglés) son redes neuronales de aprendizaje no supervisado que mapean una entrada a una representación comprimida en un encaje, o espacio latente, y luego vuelve a sí misma. La red se puede dividir en dos, el codificador que comprime los datos a un espacio latente, o de menores dimensiones, y el decodificador que los reconstruye.  Una medida de qué tan bien funciona el codificador es la diferencia entre la entrada y la salida de acuerdo a alguna métrica de distancia conocida como «error de reconstrucción».\n",
    "\n",
    "Este algoritmo se ha empezado a utilizar en HEP como detector de anomalías puesto que, se enfoca en aprender las características de de los datos mayoritarios, y se espera que un evento de señal no sea reconstruido correctamente. Así, se puede utilizar un corte en el error de reconstrucción como un umbral de anomalía."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Algoritmos que requieren estructura en los datos\n",
    "\n",
    "<table width=\"100%\"><tr>\n",
    " <td width=\"(100/2)%\"><br><h3>Análisis de discriminante cuadrático</h3> Es un clasificador con un límite de decisión cuadrático.<br><br>\n",
    "    <img src=\"../tesis/figuras/ml-qda.png\" style=\"height:225px;\"/> <figcaption align = \"center\"> <small>Fig.17: Clasificación con QDA. a) Los puntos a ser clasificados, b) los límites o fronteras de decisión. La barra de color indica la probabilidad de pertenecer a la clase 1 <a href=\"https://towardsdatascience.com/quadratic-discriminant-analysis-ae55d8a8148a\">[ref]</a>.</small></figcaption> </td>\n",
    " <td width=\"(100/2)%\"><h3>KMeans</h3> Es un algoritmo que separa los datos en grupos con igual varianza.\n",
    "     <center>\n",
    "         <img style=\"width:60%\" src=\"../tesis/figuras/ml-kmeans.webp\"></center>\n",
    "     <figcaption align = \"center\"> <small>Fig.18: Distintas inicialización de KMeans en los mismos datos <a href=\"https://realpython.com/k-means-clustering-python/\">[ref]</a>.</small></figcaption></td>    \n",
    "</tr></table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Aqui agrupamos algoritmo que clasifican de acuerdo a alguna estructura en los datos.\n",
    "\n",
    "El análisis de discriminante cuadrático (QDA, por sus siglas en inglés) es un clasificador supervisado con un límite de decisión cuadrático. El modelo asume que las densidades condicionales de clase para cada clase están distribuidas normalmente. Las predicciones para cada muestra de entrenamiento se obtienen utilizando el teorema de Bayes.\n",
    "\n",
    "KMeans es un algoritmo no supervisado de agrupamiento que separa los datos en K grupos con igual varianza. Los grupos están caracterizados por la media de los datos pertenecientes al grupo y estos se conocen como «centroides».\n",
    "\n",
    "(El algoritmo funciona mediante los siguientes pasos:\n",
    "1. Escoger los centroides. En la primera inicialización se escogen puntos aleatorios de los datos.\n",
    "2. Asignar cada muestra al centroide más cercano, minimizando la inercia.\n",
    "3. Crear nuevos centroides tomando el valor medio de todas las muestras asignadas a cada centroide anterior.\n",
    "4. Calcular la diferencia entre los centroides anteriores y los nuevos.\n",
    "\n",
    "Los últimos tres pasos se repiten hasta que la diferencia entre los centroides esté debajo de un umbral, es decir, hasta que los centroides no se muevan significativamente.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Red generativa antagónica\n",
    "Está basada en modelado generativo y en conceptos de teoría de juegos. Una GAN se construye a partir de dos redes neuronales conocidas como *generador* y *discriminador*. El discriminador se entrena para diferenciar entre las muestras sintéticas y los datos reales y el generador para engañar al discriminador.\n",
    "<center><img src=\"../tesis/figuras/presentacion-gan.png\" style=\"width:70%\"></center>\n",
    "<figcaption align = \"center\"> <small>Fig.19: Diagrama del funcionamiento de una red generativa antagónica. El generador aprende a producir datos para engañar al discriminador <a href=\"https://wiki.pathmind.com/generative-adversarial-network-gan\">[ref]</a>.</small></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Una red generativa antagónica (GAN, por sus siglas en inglés) está basada en modelado generativo y en conceptos de teoría de juegos.\n",
    "\n",
    "El modelado generativo es una tarea de aprendizaje no supervisado en la que los modelos aprenden automáticamente regularidades o patrones en los datos de entrada, con el objetivo de generar nuevos ejemplos que podrían haberse extraído del conjunto de datos original.\n",
    "\n",
    "Una GAN se construye a partir de dos redes neuronales conocidas como generador y discriminador. El discriminador se entrena para diferenciar entre las muestras sintéticas y los datos reales y el generador para engañar al discriminador. Los modelos se entrenan juntos hasta que el discriminador es engañado una cantidad de veces sobre algún umbral, lo que significa que el generador está generando ejemplos plausibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Algoritmos LHCO 2020\n",
    "De los algoritmos participantes, se escogieron **UCluster** y **GAN-AE** porque cumplen con las características de reproducibilidad necesarias y proporcionaron la información de manera simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "| Nombre | Preprocesamiento | Código| Instrucciones | Entorno| Licencia|\n",
    "|:------:|:-----------------:|:-----:|:-------------:|:------:|:-------:|\n",
    "| GAN-AE |$\\checkmark$ |$\\checkmark$ |$\\checkmark$ |$\\checkmark$ |- |\n",
    "|UCluster|$\\checkmark$ |$\\checkmark$ |$\\checkmark$ |$\\checkmark$ |$\\checkmark$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### UCluster\n",
    "El objetivo principal de UCluster es **reducir la dimensionalidad de los datos** para retener las características principales del evento de colisión **para luego agrupar puntos con características similares en este espacio reducido** o latente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Para la representación reducida se utiliza una red neuronal, sin embargo el método es independiente de la arquitectura de la red neuronal. El método consiste en hacer una *clasificación de masa de jet por partícula*, donde a cada partícula perteneciente a un jet se le asigna una etiqueta proporcional a la masa del jet al que pertenece. El modelo debe aprender la masa del jet al que la partícula pertenece y cuáles partículas pertenecen al mismo jet. El algoritmo utiliza las primeras 100 partículas asociadas a los dos jets más masivos de cada evento.\n",
    "\n",
    "El algoritmo consta de dos tareas: una de clasificación y una de agrupamiento. Una parte de la función de pérdida consta de la pérdida focal, que es similar a la entropia binaria cruzada utilizada por las redes neuronales en problemas de clasificación binaria, y la pérdida con respecto al agrupamiento, similar a la inercia utilzada por KMeans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### GAN-AE\n",
    "GAN-AE es un método que intenta **asociar una red neuronal con un codificador automático como una red generativa antagónica**. La red discriminate es un perceptrón multicapas y la red generadora un codificador automático."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "La red discriminate es un perceptrón multicapas, inicialmente entrenado en una muestra mezclada de eventos reconstruidos por el codificador automático y originales con el objetivo de exponer las debilidades del codificador automático. \n",
    "\n",
    "El codificador automático es entrenado utilizando una función de pérdida que combina la reconstrucción del error, en este caso, la distancia euclídea media (MED, por sus siglas en inglés) entre la entrada y la salida y la entropía binaria cruzada. La función de pérdida del codificador incluye un término que se evalúa dando eventos reconstruidos a la red discriminativa, pero esta vez con la etiqueta «equivocada», con el objetivo de engañarla.\n",
    "\n",
    "Por último, se evalúa el codificador automático utilizando una figura de mérito que cuando es cercana a cero indica que el codificador automático engaña mejor al MLP. Cuando esto sucede, se descarta el discriminante y el codificador automático utiliza la distancia euclidea media entre la entrada y la salida como característica discriminativa.\n",
    "\n",
    "Los participantes utilizaron este método en conjunto con BumpHunter, un algoritmo que compara la distribución de los datos con datos de referencia y evalúa el valor p y la significancia de cualquier desviación. Sin embargo, la implementación en este trabajo se limita al uso del algoritmo GAN-AE, que es la parte de clasificación binaria de este método."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Métricas de rendimiento\n",
    "La métrica de evaluación primaria es la matriz de confusión.\n",
    "\n",
    "<center><img src=\"https://2.bp.blogspot.com/-EvSXDotTOwc/XMfeOGZ-CVI/AAAAAAAAEiE/oePFfvhfOQM11dgRn9FkPxlegCXbgOF4QCLcBGAs/s1600/confusionMatrxiUpdated.jpg\" style=\"width:80%\"></center>\n",
    "<figcaption align = \"center\"> <small>Fig.20: Matriz de confusión y métricas numéricas <a href=\"https://manisha-sirsat.blogspot.com/2019/04/confusion-matrix.html\">[ref]</a>.</small></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Para medir el rendimiento de los algoritmos vamos a utilizar métricas relacionadas a la clasificación binaria.\n",
    "\n",
    "La métrica principal es la matriz de confusión. En esta matriz se resumen el número de etiquetas predichas correctamente e incorrectamente. La diagonal en la matriz de confusión representa las etiquetas predichas correctamente, mientras que los elementos fuera de la diagonal son las predicciones incorrectas. A partir de los valores de esta matriz se definen el resto de las métricas.\n",
    "\n",
    "Algunas de las métricas que vamos a utilizar en este proyecto son:\n",
    "- Exactitud balanceada: Promedio de la proporción de predicciones correctas de cada clase\n",
    "- Precisión: Proporción de eventos clasificado correctamente\n",
    "- Recuperación: Efectividad del clasificador para identificar etiquetas positivas\n",
    "- Promedio ponderado de precisión y recuperación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Métricas bidimensionales\n",
    "\n",
    "- Los clasificadores asignan un puntaje $\\mathcal{D}$ $\\longrightarrow$ puntuación más alta significa mayor probabilidad de ser señal.\n",
    "- La clasificación discreta se logra escogiendo un *punto de operación* $\\mathcal{D}_{thr}$ $\\longrightarrow$ si $\\mathcal{D}\\geq\\mathcal{D}_{thr}$ se clasifica el evento como señal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Los clasificadores binarios no asignan directamente la etiqueta a los eventos, más bien calculan un puntaje relacionado a la probabilidad de ser señal. La clasificación discreta se logra escogiendo un umbral de decisión: valores mayores que ese umbral pertenecen a la señal.\n",
    "\n",
    "Las métricas bidimensionales miden el rendimiento de los modelos en distintos umbrales de decisión, permitiendo un mayor rango en la evaluación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<table width=\"100%\"><tr>\n",
    " <td width=\"(100/3)%\"><h4>Curva ROC</h4> \n",
    "    <center><img src=\"../tesis/figuras/ml-roc.png\" style=\"width:100%\"></center>\n",
    "<figcaption align = \"center\"> <small>Fig.21: Ejemplos de curvas ROC <a href=\"https://commons.wikimedia.org/w/index.php?title=File:Roc-draft-xkcd-style.svg&oldid=588966985.\">[ref]</a>.</small></figcaption> </td>\n",
    " <td width=\"(100/3)%\"><h4>Curva PR</h4>\n",
    "<center><img src=\"../tesis/figuras/ml-curvapr.png\" style=\"width:100%\"></center>\n",
    "<figcaption align = \"center\"> <small>Fig.22: Ejemplos de curvas precisión-recuperación <a href=\"doi:10.5281/zenodo.1405727\">[ref]</a>.</small></figcaption></td>   \n",
    " <td width=\"(100/3)%\"><h4>Mejora de la significancia</h4> \n",
    "<center><img src=\"../tesis/figuras/ml-significancia.PNG\" style=\"width:100%\"></center>\n",
    "<figcaption align = \"center\"> <small>Fig.23: Ejemplo de curva de mejora de la significancia <a href=\"https://doi.org/10.1088%2F1361-6633%2Fac36b9\">[ref]</a>.</small></figcaption>\n",
    "    \n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Las métricas bidimensionales que utilizamos en este trabajo son:\n",
    "- La curva característica de funcionamiento del receptor (curva ROC, por sus siglas en inglés) de la cual se calcula el área bajo la curva o AUC para cuantificar que tan bueno es el clasificador distinguiendo entre clases.\n",
    "- La curva precisión-recuperación (curva PR) en conjunto con la precisión promedio, que cuantifica que tan bueno es el clasificador clasificando señal.\n",
    "- La mejora de la significancia que indica cuánto se amplifica la significancia para cada umbral de decisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ✨ Benchtools ✨\n",
    "Es un paquete de software desarrollado en python con herramientas para el manejo de datos de eventos de jets. Incluye funciones para:\n",
    "- Manejo de grandes cantidades de datos.\n",
    "- Calcular variables cinemática y de subestructura de los jets.\n",
    "- Agrupar jets y calcular variables cinemáticas.\n",
    "- Graficar datos con dos clases.\n",
    "- Calcular métricas de rendimiento de algoritmos de clasificación binaria.\n",
    "\n",
    "Y un *pipeline* para comparar modelos de clasificación binaria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Finalmente, para realizar el análisis, el entrenamiento de los modelos y la comparación mediante el uso de las métricas discutidas anteriormente, se desarrollo un paquete de software basado en python para facilitar el agrupamiento de los jets, el manejo de grandes cantidades de datos, análisis de datos, entre otras. Incluye múltiples funciones y un pipeline para entrenar y comparar los modelos escogidos e implementados en este trabajo con con modelos externos como los de las LHCO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <font color='#307a71'>Exploración de datos</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Datos preprocesados\n",
    "Los datos se preprocesan de acuerdo al método a utilizar y los intereses del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "El preprocesamiento de datos es fundamental en el entrenamiento de modelos de aprendizaje automático. Además, en el contexto de este trabajo es necesario porque es necesario agrupar los jets. Cada participante de las LHCO 2020 realizó el preprocesamiento que consideró conveniente para su modelo y en este proyecto también se realizó un preprocesamiento particular. A continuación les presentaré el preprocesamiento realizado para los modelos de este trabajo, hecho con benchtools, y el preprocesamiento de UCluster y GAN-AE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Benchtools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "En el preprocesamiento realizado con benchtools se utilizó el algoritmo anti-kt para agrupar los jets y se utilizaron los dos jets con mayor pT de cada eventos para calcular variables cinemáticas y de subestructura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "| Variable          | Descripción                                                         |\n",
    "|:------------------|:--------------------------------------------------------------------|\n",
    "| $pT_{ji}$         | Momento transversal del jet *i*                                     |\n",
    "| $m_{ji}$          | Masa invariante del jet *i*                                         |\n",
    "| $\\eta_{ji}$       | Pseudorapidez del jet *i*                                           |\n",
    "| $\\phi_{ji}$       | Ángulo azimutal en el plano transverso del jet *i*                  |\n",
    "| $E_{ji}$          | Energía del jet *i*                                                 |\n",
    "| $\\tau_{21,ji}$    | Subjetiness del jet *i*                                             |\n",
    "| nro. hadrones $ji$| Número de hadrones constituyentes del jet *i*                       |\n",
    "| $\\Delta R$        | Distancia angular entre los dos jets principales                    |\n",
    "| $m_{jj}$          | Masa invariante de los dos jets principales                         |\n",
    "| nro. hadrones     | Número de hadrones del evento                                       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Las variables con el subíndice $ji$ fueron calculadas para el jet principal y secundario. Las demás variables están relacioadas a la posición espacial y la masa de los dos jets principales y el numero de hadrones es del evento en general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Número de hadrones\n",
    "Los eventos de fondo, que provienen principalmente de gluones y quark, tienden a tener más divisiones en su evolución y producen mayor radiación.\n",
    "<center><img src=\"../tesis/figuras/rawdata-nhadrones.png\" style=\"width:80%\"></center>\n",
    "<figcaption align = \"center\"> <small>Fig.24: Distribución del número de hadrones para eventos de señal y fondo. A la izquierda el conjunto R&D y a la derecha el conjunto BB1.</small></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Podemos ver las distribuciones de los datos para la señal y el fondo. Por ejemplo el número de hadrones vemos que es menor para los eventos de señal (rosa) que para los eventos de fondo (violeta) debido a que los jets iniciados por gluones tienden a tener más divisiones en su evolución y mayor radiación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Jet principal: mayor separación entre señal y fondo\n",
    "- El $pT$ y $E$ es mayor para los jets de fondo debido a que provienen de partículas más masivas.\n",
    "- Los picos en la distribución de masa del corresponden a las masas de las partículas $X$ y $Y$ del conjunto.\n",
    "- $\\tau_{21}$ es menor para los jets de señal porque están compuestos de dos o más subjets.\n",
    "<center><img src=\"../tesis/figuras/datospp-vardiff-RnD.png\" style=\"width:65%\"/></center>\n",
    "<figcaption align = \"center\"> <small>Fig.25: Distribución de $pT$, $E$, $m$ y $\\tau_{21}$ del jet principal en el conjunto R&D.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Algunas de las variables con mayor separación entre señal y fondo son el momento transverso, la energía, la masa y el subjettiness. \n",
    "\n",
    "El momento transverso y la energía vemos que es mayos para los jets de señal que para los de fondo, debido a que estas cantidades son proporcionales a la masa de la partícula que genera el jet.\n",
    "\n",
    "En la distribución de masa del jet se observan dos picos alrededor de 100 y 500 GeV, correspondientes a las partículas X y Y de nueva física que se presentan en la señal y decaen en jets.\n",
    "\n",
    "Por último el subjetines es menor para la señal que para el fondo, debido a que la subestructura del jet puede estar compuesta de dos o más subjets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Relaciones entre jets\n",
    "- $\\Delta R$ es más angosta para los eventos de señal. Los jets de señal, al ser producto de colisiones más energéticas, se generan en direcciones opuestas, y son más centrales en los detectores.\n",
    "- La masa invariante de la señal presenta un máximo que coincide con la masa de la partícula $Z'$ de la señal de nueva física.\n",
    "<center><img src=\"../tesis/figuras/datospp-jetrelations-RnD.png\" alt=\"Drawing\" style=\"width:80%;\"/></center>\n",
    "<figcaption align = \"center\"> <small>Fig.26: Distribución de las variables $\\Delta R$ y $m_{jj}$ para el conjunto R&D.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "En las relaciones entre jets, vemos que la distribución de distancia angular entre los jets de señal es más angosta que la de los jets de fondo, porque los jets de señal son producto de colisiones más energéticas y se generan en direcciones opuestas más consistentemente. \n",
    "\n",
    "La distribución de masa invariante de la señal y el fondo difieren considerablemente, ya que el jet de señal es más masivo. El máximo que se observa coincide con la masa de la partícula Z' de nueva física."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Datos de UCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "|Variable &emsp;&emsp;&emsp;&emsp;|Descripción &emsp;&emsp;&emsp;&emsp;|\n",
    "|:--------------------------:|:-----------------------------------------------------------------------------|\n",
    "|$\\Delta\\eta$                | Diferencia entre la pseudo-rapidez del constituyente y del jet               |\n",
    "|$\\Delta\\phi$                | Diferencia entre el ángulo azimutal del constituyente y del jet              |\n",
    "|$\\log{pT}$                  | Logaritmo del $pT$ del constituyente                                         |\n",
    "|$\\log{E}$                   | Logaritmo de la $E$ del constituyente                                        |\n",
    "|$\\log\\frac{pT}{pT_{jet}}$   | Logaritmo de la relación entre el $pT$ del constituyente y el $pT$ del jet   |\n",
    "|$\\log\\frac{E}{E_{jet}}$     | Logaritmo de la relación entre la $E$ del constituyente y la $E$ del jet     |\n",
    "|$\\Delta R$                  | Distancia entre el constituyente y el jet en el espacio $\\eta-\\phi$          |\n",
    "|label                       | Etiquetas de masa asignadas a los constituyentes de los jet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Las variables cinemáticas son similares a algunas calculadas con benchtools. Sin embargo, como se hizo una clasificación de masa por partícula para crear la representación reducida, las variables cinemáticas se tomaron de los primeros 100 constituyentes de los dos jets principales. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Distribuciones\n",
    "A diferencia de <code>benchtools</code>, las variables utilizadas por UCluster están relacionadas a las partículas constituyentes de los jets.\n",
    "<center><img src=\"../tesis/figuras/dUCluster-dist-data.png\" alt=\"Drawing\" style=\"width:100%;\"/></center>\n",
    "<figcaption align = \"center\"> <small>Fig.27: Distribución de algunas de las variables preprocesadas para UCluster.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Las distribuciones se pueden observar en la  figura 35. Notamos que al utilizar el logaritmo y la cinemáticas de las partículas en vez de los jets, las distribuciones presentan menor separación de clases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Datos de GAN-AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Las variables obtenidas en el preprocesamiento de GAN-AE para los dos jets principales son:\n",
    "- $pT$, $\\eta$, $\\phi$, $m$ y $E$.\n",
    "- El número de subjets inclusivos y exclusivos del evento y el número de constituyentes.\n",
    "- Subjettiness: $\\tau_1$, $\\tau_2$, $\\tau_3$, $\\tau_{32}$ y $\\tau_{21}$.\n",
    "- 10 anillos de energía.\n",
    "\n",
    "Y del evento:\n",
    "- $m_{jj}$ y número de jets $n_{jets}$ con $pT\\geq20$ GeV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Para el modelo GAN-AE se calculan una gran cantidad de variables. Las variables de momento tranverso, pseudorapidez, angulo azimutal, masa y energía se calculan para los dos jets principales como en benchtools. Sin embargo, incluyeron el número de subjets inclusivos y exclusivos de los eventos, varios valores de subjettiness y anillos de energía, para estudiar la distribución de energía dentro del jet.\n",
    "\n",
    "Del evento, se calcula la masa ivariante y el número de jets con momento transverso mayor de 20 GeV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Distribuciones\n",
    "Las variables más importantes para GAN-AE son las que presentan mayor separación entre señal y fondo, y son algunas de las mismas variables preprocesadas para <code>benchtools</code>.\n",
    "<center><img src=\"../tesis/figuras/dGANAE-dist-importance.png\" alt=\"Drawing\" style=\"width:100%;\"/></center>\n",
    "<figcaption align = \"center\"> <small>Fig.28: Distribución de algunas de las variables preprocesadas para GAN-AE.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Algunas de las variables más importantes para la clasificación se pueden observan en la figura 36 y estas coinciden con las variables calculadas para este trabajo usando benchtools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <font color='#307a71'>Resultados</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pipeline\n",
    "Para obtener la comparación de los modelos, se utilizó el pipeline de <code>benchtools</code>.\n",
    "<center><img src=\"../tesis/figuras/bench-pipeline.png\" style=\"width:100%\"></center>\n",
    "<figcaption align = \"center\"> <small>Fig.29: Diagrama del pipeline de <code>benchtools</code>.</small></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Para este trabajo se desarrollo un pipeline que permite simplificar el proceso de importar los datos, preprocesarlos, entrenar algoritmos y compararlos con algoritmos entrenados externamente. Los resultados que se mostrarán a continuación son el resultado de este pipeline, parte del paquete benchtools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Modelos a comparar\n",
    "| Modelos                       | Implementación |\n",
    "|:------------------------------|:--------------:|\n",
    "| TensorflowClassifier          | TensorFlow     |\n",
    "| RandomForestClassifier        | scikit-learn   |\n",
    "| GradientBoostingClassifier    | scikit-learn   |\n",
    "| QuadraticDiscriminantAnalysis | scikit-learn   |\n",
    "| MLPClassifier                 | scikit-learn   |\n",
    "| KMeans                        | scikit-learn   |\n",
    "| GAN-AE                        | LHCO 2020      |\n",
    "| UCluster                      | LHCO 2020      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Los modelos a comparar son:\n",
    "- Un clasificador construido con TensorFlow (TensorFlowClassifier) incluido en el paquete benchtools (4 capas internas, 1 de entrada y 1 de salida. 512 neuronas por capa, no todas activas porque se utilizo dropout para evitar overfitting, excepto la ultima capa que posee una sola neurona con la función sigmoide).\n",
    "\n",
    "Los demás modelos que se utilizaron se encuentran implementados en scikit-learn:\n",
    "- Un clasificador de bosque aleatorio (RandomForestClassifier).\n",
    "- Potenciador de gradiente (GradientBoostingClassifier).\n",
    "- Discriminante cuadratico (QuadraticDiscriminantAnalysis).\n",
    "- Perceptrón múlticapa (MLPClassifier)\n",
    "- K-Means\n",
    "\n",
    "Y los algoritmos de las LHCO 2020:\n",
    "- GAN-AE\n",
    "- UCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Datos a utilizar\n",
    "Las unicas variables que no se utilizaron en el entrenamiento y la clasificación fueron $m_{ji}$ y $m_{jj}$.\n",
    "<br>\n",
    "\n",
    "| Variable          | Descripción                                                         |\n",
    "|:------------------|:--------------------------------------------------------------------|\n",
    "| $pT_{ji}$         | Momento transversal del jet *i*                                     |\n",
    "| $\\eta_{ji}$       | Pseudorapidez del jet *i*                                           |\n",
    "| $\\phi_{ji}$       | Ángulo azimutal en el plano transverso del jet *i*                  |\n",
    "| $E_{ji}$          | Energía del jet *i*                                                 |\n",
    "| $\\tau_{21,ji}$    | Subjetiness del jet *i*                                             |\n",
    "| nro. hadrones $ji$| Número de hadrones constituyentes del jet *i*                       |\n",
    "| $\\Delta R$        | Distancia angular entre los dos jets principales                    |\n",
    "| nro. hadrones     | Número de hadrones del evento                                       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Las unicas variables que no se utilizaron en el entrenamiento y la clasificación fueron las variables de masa, para procurar una clasificación lo más general posible, evitando que los modelos aprendan las masas de las partículas de nueva física."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Conjunto de datos R&D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Primero observaremos la clasificación del conjunto R&D. Este es el conjunto que los participantes utilizaron para investigación y desarrollo, y fue el conjunto utilizado para entrenar los modelos.\n",
    "\n",
    "Un recordatorio: tanto el conjunto R&D como el BB1 contienen la misma forma de evento pero las masas de las partículas difieren y la proporción de eventos de señal también (9% y 0.08%, respectivamente)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Importancia de las características\n",
    "Las variables con mayor separación entre señal y fondo son más importantes para el entrenamiendo de los algoritmos.\n",
    "<center><img src=\"../tesis/figuras/clas-feature-imp.png\" style=\"width:100%\"></center>\n",
    "<figcaption align = \"center\"> <small>Fig.30: Importancia de las variables utilizadas en el entrenamiento realizado con el conjunto de datos R&D según RFC (izquierda) y GBC (derecha).</small></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "De los modelos de bosque aleatorio y potencidor de gradiente se puede obtener la importancia de la variable en la forma final del modelo. En este caso, ambos modelos coinciden en que las variables de subjettiness de ambos jets y el momento transverso del jet secundario son las más relevantes para separar clases. Luego de estás se encuentran el momento transverso del jet principal, el número de hadrones del evento y la distancia angular entre jets.\n",
    "\n",
    "En las distribuciones mostradas anteriormente observamos que estas son las variables que presentan mayor separación entre señal y fondo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Métricas numéricas\n",
    "<center><img src=\"../tesis/figuras/comp-metricas-num-RnD.png\" style=\"width:100%\"></center>\n",
    "<figcaption align = \"center\"> <small>Fig.31: Diagramas de las métricas numéricas obtenidas utilizando el pipeline de benchtools para el conjunto R&D.</small></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Las métricas númericas que se calcularon con el pipeline fueron: la exactitud balanceada, la precisión, la recuperación y el puntaje f1. Notamos que las métricas no coinciden con respecto a qué modelo tiene mejor rendimiento. Particularmente vemos que todos los modelos logran una exactitud balanceada sobre el 70% en el conjunto R&D. La precisión y recuperación vemos que es contraria: los modelos con mayor precisión tienen menor recuperación y viseversa. El puntaje f1 resume estas dos métricas. El único modelo con menor rendimiento que un clasificador aleatorio de acuerdo a esta métrica fue UCluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Métricas bidimensionales\n",
    "<center><img src=\"../tesis/figuras/comp-metricas-graf-RnD.png\" style=\"width:65%\"></center>\n",
    "<figcaption align = \"center\"> <small>Fig.32: Métricas bidimensionales para la clasificación del conjunto R&D. De izquierda derecha, en la fila superior: eficiencia de señal vs. rechazo de fondo, ROC inversa, y en la fila inferior: precisión-recuperación y mejora significativa.</small></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "En las métricas bidimensionales observamos que en general, el modelo MLP (morado) tiene un mejor rendimiento de acuerdo a todas las métricas. En general, el orden de rendimiento de los clasificadores se mantiene en estas métricas, salvo para UCluster y KMeans: la curva PR difiere de las demás curva.\n",
    "\n",
    "De acuerdo a las métricas anteriores y estos gráficos vemos que la mayoría de los modelos tienen buen rendimiento para la dificultad de clasificar este conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Métricas de interés\n",
    "No existe un único modelo que sea mejor de acuerdo a todas las métricas, por lo que se debe definir cuál métrica es más relevante para esta tarea de clasificación. \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Debido a que no todas las métricas coinciden en cuál modelo tiene un mejor rendimiento, es necesario decidir qué métrica es más relevante para esta tarea de clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>Existe un <b>desbalance cualitativo</b>: la clasificación correcta de eventos de fondo es irrelevante.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Particularmente es importante tomar en cuenta que el desbalance de los datos no es solo cuantitativo si no también cualitativo: la clasificación correcta de fondo es irrelevante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>$\\downarrow$</center>\n",
    "<center>Los verdaderos negativos son irrelevantes.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "En lenguaje de clasificación binaria, no es de interés medir o mejorar la clasificación de eventos de fondo: los verdaderos negativos son irrelevantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>$\\downarrow$</center>\n",
    "<center> Las métricas para la clasificación de eventos en HEP deben enfocarse en la correcta clasificación de eventos de señal.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "En otras palabras, las métricas utilizadas para medir el rendimiento de modelos en la clasificación de eventos en HEP deben enfocarse en la correcta clasificación de eventos de señal, es decir, en los verdaderos positivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Métricas de interés\n",
    "<br>\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "La <b>exactitud balanceada</b>, <b>la curva ROC</b> y el <b>AUC</b> son métricas que incluyen en sus cálculos la capacidad del modelo para etiquetar correctamente eventos de fondo.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Esto incide directamente en las métricas que son más relevantes para este problema. La exactitud balanceada, las curvas ROC y el AUC son métricas que incluyen los verdaderos negativos en sus cálculos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>$\\downarrow$</center>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>La precisión</b>, <b>recuperación</b>, <b>el puntaje f1</b>, la <b>curva precisión-recuperación</b> y la <b>precisión promedio</b> se enfocan en medir la correcta clasificación de eventos de señal.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "La precisión, recuperación, el puntaje f1 y la curva PR con la precisión promedio están enfocadas en medir la correcta clasificación de los eventos de señal, por lo que son más relevantes en el resultado de este trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "lista_imagenes = ['../tesis/figuras/comp-precisionrecall-RnD.png', '../tesis/figuras/comp-f1score-RnD.png']\n",
    "image_grid(rows=2, columns=1, images=lista_imagenes, name='presentacion-metricas-RnD', path='../tesis/figuras/', remove=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Clasificación del conjunto de datos R&D\n",
    "<img style=\"float: left; padding: 0px 20px 0px 0px; width:45%\" src=\"../tesis/figuras/presentacion-metricasRnD.png\">\n",
    "<br>\n",
    "<br>\n",
    "• **Los modelos supervisados tienen mejor rendimiento en este conjunto** porque poseen más información en el entrenamiento.<br>\n",
    "<br>\n",
    "<br>\n",
    "• *De los modelos supervisados las redes neuronales presentan un mejor rendimiento* debido a que son los modelos más complejos.<br>\n",
    "<br>\n",
    "<br>\n",
    "• *De los modelos no supervisados GAN-AE obtuvo un mejor resultado* debido a que utiliza un codificador automático, que prioriza el aprendizaje de una función que reconstruya la mayor cantidad de datos.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<figcaption align = \"left\"> <small>Fig.33: Puntaje f1 y curva PR de los distintos clasificadores en el conjunto de datos R&D.</small></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Tomando esto en cuenta, nos enfocaremos en el puntaje f1 (que incluye la precisión y recuperación) y la curva PR. En el conjunto R&D notamos que los modelos supervisados tienen un mayor rendimiento clasificando eventos. Los modelos supervisados utilizan etiquetas en el entrenamiento, por lo que poseen más información en el proceso de aprendizaje, resultando en una mejor clasificación. También observamos que el clasificador de TensorFlow y el MLP son los mejores de los modelos supervisados: ambos son redes neuronales, es decir, son los modelos más complejos, que ajustan una mayor cantidad de parámetros y son partícularmente utilizados en problemas complejos.\n",
    "\n",
    "De los modelos no supervisados, GAN-AE es el que tuvo mejor rendimiento. Al utilizar un codificador automático, el modelo prioriza aprender la función de fondo, esto es muy utilizado en detección de anomalías porque se ha observado que da buenos resultados en datos desbalanceados. UCluster y KMeans, que obtuvieron el menor rendimiento, son algoritmos de agrupamiento y necesitan estructura en los datos, estructura que no fue lograda en el preprocesamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## R&D: modelos de las LHCO 2020\n",
    "De acuerdo al puntaje f1 podemos comparar los modelos de las LHCO 2020 con las distribuciones obtenidas de la clasificación realizada por los modelos implementados en este proyecto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "De acuerdo al puntaje f1 podemos acotar las distribuciones de masa que podrían obtener los modelos de las LHCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# En esta celda preparamos los datos para utilizar los algoritmos\n",
    "# Separamos los datos en conjuntos de entrenamiento y prueba\n",
    "df_RnD = pd.read_csv('../datos/RnD-1100000.csv')\n",
    "X, y = separate_data(df_RnD, standarize=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y) # 70% training y 30% test\n",
    "\n",
    "# Guardamos las columnas de masa\n",
    "X_train_m = X_train.loc[:,['m_j1', 'm_j2', 'm_jj']].copy()\n",
    "X_test_m = X_test.loc[:,['m_j1', 'm_j2', 'm_jj']].copy()\n",
    "\n",
    "# Eliminamos las masas de los conjuntos de entrenamiento y prueba\n",
    "X_train.drop(['m_j1', 'm_j2', 'm_jj'], axis=1, inplace=True)\n",
    "X_test.drop(['m_j1', 'm_j2', 'm_jj'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# En esta celda cargamos los modelos entrenados\n",
    "models = []\n",
    "\n",
    "# Cargamos el algoritmo entrenado de tensorflow\n",
    "tf_model = load_model(os.path.join('../datos','tf_model_{}.h5'.format('log')))\n",
    "models.append(('TensorflowClassifier', tf_model))\n",
    "\n",
    "# Cargamos los algoritmos entrenados de scikit-learn\n",
    "with open(os.path.join('../datos',\"sklearn_models_{}.pckl\".format('log')), \"rb\") as f:\n",
    "    while True:\n",
    "        try:\n",
    "            models.append(pickle.load(f))\n",
    "        except EOFError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [03:08<00:00, 31.44s/it]\n"
     ]
    }
   ],
   "source": [
    "# Evaluamos\n",
    "clfs = evaluate(X_test, y_test, models ,train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# En esta celda graficamos algunas variables importantes\n",
    "variables=['m_jj']#,'pT_j2', 'tau_21_j1','n_hadrons']\n",
    "for model in [clfs[0],clfs[5]]: #TensorFlow y KMeans\n",
    "    list_images =[]\n",
    "    for variable in variables:\n",
    "        # Obtenemos predicciones como pandas Series\n",
    "        pred = pd.Series(model.pred.flatten(), name='ypred')\n",
    "        # Obtenemos las etiquetas como pandas Serie\n",
    "        label = pd.Series(model.label, name='ytest').reset_index(drop=True)\n",
    "        # Juntamos las masas, las predicciones y las etiquetas\n",
    "        X_plot = pd.concat([X_test.reset_index(drop=True), X_test_m.reset_index(drop=True), pred, label], axis=1)\n",
    "        # Graficamos\n",
    "        fig = plt.figure(facecolor='white')\n",
    "        pred_test_hist(X_plot, variable, ypred='ypred', ytest='ytest', n_bins=50, log=False)\n",
    "        plt.title('{}: distribución de {}'.format(model.name, variable))\n",
    "        # Guardando el path de cada imagen\n",
    "        filename = os.path.join('../tesis/figuras/','{}-{}.png'.format(model.name,variable))\n",
    "        list_images.append(filename)\n",
    "        # Salvamos la imagen como png\n",
    "        plt.savefig(filename, bbox_inches='tight', facecolor=fig.get_facecolor(),edgecolor='none')\n",
    "        plt.close('all')\n",
    "        del X_plot, pred, label\n",
    "        \n",
    "    image_grid(rows=1, columns=1, images=list_images, name='presentacion-comparacionRnD-{}'.format(model.name), path='../tesis/figuras/', remove=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<table><tr>\n",
    "    <td> <center><medium> UCluster $\\longrightarrow$ </medium></center> </td>\n",
    "    <td> <img src=\"../tesis/figuras/presentacion-comparacionRnD-KMeans.png\" alt=\"Drawing\" style=\"width:150%;\"/><figcaption align = \"center\"> <small>Fig.34: Distribución de $m_{jj}$ en el conjunto R&D de acuerdo a KMeans.</small></figcaption> </td>\n",
    "    <td> <center><medium> GAN-AE $\\longrightarrow$</medium></center> </td>\n",
    "    <td> <img src=\"../tesis/figuras/presentacion-comparacionRnD-TensorflowClassifier.png\" alt=\"Drawing\" style=\"width:150%;\"/> <figcaption align = \"center\"> <small>Fig.35: Distribución de $m_{jj}$ en el conjunto R&D de acuerdo al clasificador de TensorFlow.</small></figcaption></td>    \n",
    "</tr></table>\n",
    "\n",
    "<center><img src=\"../tesis/figuras/presentacion-aumentorendimiento.png\" style=\"width:50%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "A la izquierda se encuentra UCluster, que fue el modelo con menor rendimiento según el puntaje f1. El siguiente modelo con mayor rendimiento es KMeans, y observamos que logra hallar el pico de señal. Sin embargo esta sobreestimando la cantidad de fondo y subestimando la señal. Además, notamos que, a pesar de que las variables de masa no se utilizaron en el entrenamiento, la clasificación agrupo a los eventos de fondo en masas menores de 5000 GeV y a los de señal en masas mayores de 3000 GeV.\n",
    "\n",
    "Luego de KMeans se encuentra GAN-AE, con mayor rendimiento, y le sigue el clasificador de TensorFlow. El clasificador de TensorFlow logró hallar el pico de señal, aunque subestimó la cantidad de señal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Conjunto de datos BB1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Ahora veremos el resultado de las métricas al clasificar el conjunto BB1 utilizando los modelos entrenados con el conjunto R&D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Métricas numéricas\n",
    "<center><img src=\"../tesis/figuras/comp-metricas-num-BB1.png\" style=\"width:100%\"></center>\n",
    "<figcaption align = \"center\"> <small>Fig.36: Diagramas de las métricas numéricas obtenidas utilizando el pipeline de benchtools para el conjunto BB1.</small></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "La exactitud balanceada se mantuvo alta para este conjunto de datos. Sin embargo, notamos que las demás métricas disminuyeron. La precisión en este conjunto de datos es muy baja, niguno de los modelos llega al 1%. La recuperación no disminuyó tan dramáticamente. Por la disminución de la precisión, el puntaje f1 disminuyó notablemente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Métricas bidimensionales\n",
    "<center><img src=\"../tesis/figuras/comp-metricas-graf-BB1.png\" style=\"width:65%\"></center>\n",
    "<figcaption align = \"center\"> <small>Fig.37: Métricas bidimensionales para la clasificación del conjunto BB1. De izquierda derecha, en la fila superior: eficiencia de señal vs. rechazo de fondo, ROC inversa, y en la fila inferior: precisión-recuperación y mejora significativa.</small></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "En las métricas bidimensionales también se observa la disminusión en el rendimiento de los modelos al clasificar este conjunto. Todas las métricas coinciden en que GAN-AE es el modelo que logra el mejor rendimiento. Las curvas en la fila superior son similares a las curvas al clasificar el conjunto R&D, sin embargo en la curva PR sí se observa la disminución drástica por la disminución en la precisión promedio de todos los modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Clasificación del conjunto de datos BB1\n",
    "<img style=\"float: left; padding: 0px 20px 0px 0px; width:45%\" src=\"../tesis/figuras/presentacion-metricasBB1.png\">\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "• La clasificación fue menos exitosa porque **se utilizaron variables dependientes de la masa.**<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<figcaption align = \"left\"> <small>Fig.38: Puntaje f1 y curva PR de los distintos clasificadores en el conjunto de datos BB1.</small></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Comparando las métricas relevates vemos que en el conjunto BB1 la clasificación resultó menos exitosa porque, a pesar de que no se utilizaron las variables de masa en el entrenamiento y la clasificación, se utilizaron variables que dependen de la masa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Variables dependientes de la masa\n",
    "<center><img src=\"../tesis/figuras/clas-senal-RnD-BB1.png\" style=\"width:100%\"></center>\n",
    "<figcaption align = \"center\"> <small>Fig.39: Comparación de las distribuciones de señal de las variables $pT$ del jet principal, $\\tau_{21}$ del jet principal y $\\Delta R$ de los conjuntos de datos R&D y BB1.</small></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Como se ve en la figura, a pesar de que las distribuciones de momento tranverso del jet principal de la señal son similares en ambos conjuntos, el momento es mayor para BB1 porque la partícula es más masiva. Igualmente, el subjettiness es menor y la distribución de la distancia de los jets es más angosta por esta razón. Es decir, las variables cambian con las masas de las nuevas partículas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Clasificación del conjunto de datos BB1\n",
    "<img style=\"float: left; padding: 0px 20px 0px 0px; width:45%\" src=\"../tesis/figuras/presentacion-metricasBB1.png\">\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "• La clasificación fue menos exitosa porque **se utilizaron variables dependientes de la masa**.<br>\n",
    "<br>\n",
    "<br>\n",
    "• De acuerdo a las métricas bidimensionales, el modelo no supervisado GAN-AE tiene mejor rendimiento, debido a que tiene una mayor capacidad de generalización por el uso del codificador automático.\n",
    "<br>\n",
    "<br>\n",
    "• Sin embargo, los modelos no supervisados UCluster y KMeans son los que resultan en un menor rendimiento.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<figcaption align = \"left\"> <small>Fig.40: Puntaje f1 y curva PR de los distintos clasificadores en el conjunto de datos BB1.</small></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "En este conjunto, las métricas bidimensionales coinciden en que GAN-AE tiene un mejor rendimiento. Como se dijo anteriormente, los codificadores automáticos se han optimizado para búsqueda de anomalías porque aprenden a reconstruir los datos mayoritarios, por lo que aunque la masa varie, sigue siendo anómalo para este modelo.\n",
    "\n",
    "Los modelos no supervisados UCluster y KMeans fueron nuevamente los que tuvieron menor rendimiento debido a que las variables no se encuentran en grupos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [08:06<00:00, 81.09s/it]\n"
     ]
    }
   ],
   "source": [
    "# En esta celda preparamos los datos a utilizar por los algoritmos\n",
    "# Separamos los datos variables y label\n",
    "df_BB1 = pd.read_csv('../datos/BB1-1000000.csv')\n",
    "X, y = separate_data(df_BB1, standarize=False)\n",
    "\n",
    "# Guardamos las columnas de masa\n",
    "X_m = X.loc[:,['m_j1', 'm_j2', 'm_jj']].copy()\n",
    "\n",
    "# Eliminamos las masas\n",
    "X.drop(['m_j1', 'm_j2', 'm_jj'], axis=1, inplace=True)\n",
    "\n",
    "# Realizamos las predicciones\n",
    "clfs = evaluate(X, y, models ,train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TensorflowClassifier',\n",
       " 'RandomForestClassifier',\n",
       " 'GradientBoostingClassifier',\n",
       " 'QuadraticDiscriminantAnalysis',\n",
       " 'MLPClassifier',\n",
       " 'KMeans']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[clf.name for clf in clfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# En esta celda graficamos algunas variables importantes\n",
    "variables=['m_jj']\n",
    "for model in [clfs[4],clfs[2],clfs[5]]: #MLP, GBC y KMeans\n",
    "    list_images =[]\n",
    "    for variable in variables:\n",
    "        # Obtenemos predicciones como pandas Series\n",
    "        pred = pd.Series(model.pred.flatten(), name='ypred')\n",
    "        # Obtenemos las etiquetas como pandas Serie\n",
    "        label = pd.Series(model.label, name='ytest').reset_index(drop=True)\n",
    "        # Juntamos las masas, las predicciones y las etiquetas\n",
    "        X_plot = pd.concat([X.reset_index(drop=True), X_m.reset_index(drop=True), pred, label], axis=1)\n",
    "        # Graficamos\n",
    "        fig = plt.figure(facecolor='white')\n",
    "        pred_test_hist(X_plot, variable, ypred='ypred', ytest='ytest', n_bins=50, log=False)\n",
    "        plt.title('{}: distribución de {}'.format(model.name, variable))\n",
    "        # Guardando el path de cada imagen\n",
    "        filename = os.path.join('../tesis/figuras/','{}-{}.png'.format(model.name,variable))\n",
    "        list_images.append(filename)\n",
    "        # Salvamos la imagen como png\n",
    "        plt.savefig(filename, bbox_inches='tight', facecolor=fig.get_facecolor(),edgecolor='none')\n",
    "        plt.close('all')\n",
    "        del X_plot, pred, label\n",
    "\n",
    "    image_grid(rows=1, columns=1, images=list_images, name='presentacion-comparacionBB1-{}'.format(model.name), path='../tesis/figuras/', remove=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## BB1: modelos de las LHCO 2020\n",
    "\n",
    "<table><tr>\n",
    "<td> <center><medium> UCluster $\\longrightarrow$ </medium></center> </td>\n",
    "<td> <img src=\"../tesis/figuras/presentacion-comparacionBB1-KMeans.png\" alt=\"Drawing\" style=\"width:100%;\"/> <figcaption align = \"center\"> <small>Fig.41: Distribución de $m_{jj}$ en el conjunto BB1 de acuerdo a KMeans.</small></figcaption></td>    \n",
    "<td> $\\dots$ </td>\n",
    "</tr></table>\n",
    "\n",
    "<center><img src=\"../tesis/figuras/presentacion-aumentorendimiento.png\" style=\"width:50%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Comparando nuevamente la distribución de masa pero para el conjunto BB1, empezamos con UCluster que fue el modelo con menor rendimiento. Se le puede colocar una cota superior a la distribución de masa que se podría obtener de este modelo con el modelo de KMeans, que fue el siguiente con mejor rendimiento. Notamos nuevamoente que sobreestima la cantidad de fondo, y que está clasificando como señal en masas menores a las del conjunto BB1, más cercanas a las del conjunto R&D con el que fue entrenado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## BB1: modelos de las LHCO 2020\n",
    "<table><tr>\n",
    "<td> <img src=\"../tesis/figuras/presentacion-comparacionBB1-MLPClassifier.png\" alt=\"Drawing\" style=\"width:100%;\"/> <figcaption align = \"center\"> <small>Fig.42: Distribución de $m_{jj}$ en el conjunto BB1 de acuerdo a MLP.</small></figcaption> </td>\n",
    "    <td> <center><medium> GAN-AE $\\longrightarrow$ </medium></center> </td>\n",
    "<td> <img src=\"../tesis/figuras/presentacion-comparacionBB1-GradientBoostingClassifier.png\" alt=\"Drawing\" style=\"width:100%;\"/> <figcaption align = \"center\"> <small>Fig.43: Distribución de $m_{jj}$ en el conjunto BB1 de acuerdo a GBC.</small></figcaption></td>    \n",
    "</tr></table>\n",
    "\n",
    "<center><img src=\"../tesis/figuras/presentacion-aumentorendimiento.png\" style=\"width:50%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Seguido de KMeans, en orden creciente de rendimiento, se encuentra el clasificador de TensorFlow, análisis discriminante cuadrático y perceptron multicapas, que es la cota inferior para GAN-AE. MLP logra hallar un pico de señal pero con menor masa que la del conjunto BB1, más cercana a la del conjunto R&D. Como cota superior de GAN-AE se encuentra el clasificador de potenciación del gradiente. Este clasificador abarca más de la señal real, pero el pico de masa de la señal sigue movido a menores valores de masa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reproducibilidad\n",
    "\n",
    "<img style=\"float: left; padding: 0px 20px 0px 0px; width:50%\" src=https://the-turing-way.netlify.app/_images/reproducibility.jpg>\n",
    "\n",
    "Los resultados de los modelos de las LHCO 2020, GAN-AE y UCluster, fueron fácilmente reproducidos. Sin embargo:\n",
    "<br>\n",
    "<br>\n",
    "   • Se hicieron algunos ajustes para poder correr GAN-AE de la manera en que requería este proyecto.\n",
    "    \n",
    "   • No se encontró disponible la configuración exacta de UCluster, por lo que se usó la configuración por defecto de este modelo.\n",
    "<br>\n",
    "<br>\n",
    "En general, los modelos de las olimpiadas LHCO 2020 inspiraron la construcción de este trabajo.\n",
    "\n",
    "<figcaption align = \"left\"> <small>Fig.??: Pasos para hacer una investigación reproducible.</small></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Es importante discutir lo hallado respecto a la reproducibilidad durante este proyecto. A nivel general, los dos modelos utilizados de las olimpiadas fueron fáciles de utilizar, sin embargo:\n",
    "- Se hicieron algunos ajustes en el código de GAN-AE para poder utilizarlo en la manera que requería este proyecto. Recordemos que los participantes analizaron el conjunto BB1 sin utilizar las etiquetas, pero para este trabajo son necesarias para el cálculo de las métricas. Además, originalmete utilizaron el algoritmo BumpHunter luego de utilizar GAN-AE para clasificación.\n",
    "- La configuración exacta de UCluster para los resultados obtenidos en el artículo común no se encontraban publicados y se utilizó la configuración por defecto.\n",
    "\n",
    "Todos los modelos de las olimpiadas inspiraron la construcción de este trabajo: desde el preprocesamiento de los datos, el tipo de modelos a utilizar y estilos de codigo, hasta las consideraciones de reproducibilidad y cómo aplicarlas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <font color='#307a71'>Conclusiones</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sobre los modelos\n",
    "De acuerdo a los resultados presentados anteriormente:\n",
    "- **Los modelos sencillos implementados en este trabajo se muestran competentes en la tarea de clasificación**. Especialmente, se evidencia que los modelos no supervisados requieren mayores esfuerzos para lograr la clasificación que los modelos supervisados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **De los modelos de las LHCO 2020, GAN-AE obtuvo mejores resultados** que UCluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Se demostró que **un modelo supervisado clasifica mejor eventos con características similares** a los utilizados en el entrenamiento, pero **un modelo no supervisado más complejo puede ser mejor a la hora de generalizar** y clasificar eventos en los que algunas características difieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A mejorar\n",
    "\n",
    "A lo largo del proyecto se demuestra la necesidad de estudiar con mayor profundidad:\n",
    "- Variables independientes de la masa para analizar estos eventos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Métricas para estudiar el rendimiento de las herramientas de aprendizaje automático en la tarea de clasificación de eventos en HEP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Técnicas referentes a la reproducibilidad científica a la hora de hacer investigaciones con estas herramientas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Recomendaciones\n",
    "En general, se recomienda:\n",
    "- Unificar y fijar las métricas más relevantes para poder comparar directamente el rendimiento de distintos modelos de aprendizaje automático en la tarea de clasificación de eventos en HEP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Adoptar las practicas para hacer ciencia abierta de forma reproducible discutidas en este trabajo y planteadas en The Turing Way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <font color='#70c6ca'>Gracias!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='#70c6ca'>Referencias</font>\n",
    "\n",
    "[1] Beresford, Lydia Audrey. *Searches for Dijet Resonances: Using √s = 13 TeV Proton–Proton Collision Data Recorded by the ATLAS Detector at the Large Hadron Collider*. Springer International Publishing, 2018. DOI.org (Crossref), doi:10.1007/978-3-319-97520-7.\n",
    "\n",
    "[2] Kasieczka, Gregor, et al. *The LHC Olympics 2020: A Community Challenge for Anomaly Detection in High Energy Physics.* ArXiv:2101.08320 [Hep-Ex, Physics:Hep-Ph, Physics:Physics], Jan. 2021. arXiv.org, http://arxiv.org/abs/2101.08320.\n",
    "\n",
    "[3] The Turing Way Community, Becky Arnold, Louise Bowler, Sarah Gibson, Patricia Herterich, Rosie Higman, … Kirstie Whitaker. (2019, Marzo 25). *The Turing Way: A Handbook for Reproducible Data Science (Version v0.0.4)*. Zenodo. http://doi.org/10.5281/zenodo.3233986  \n",
    "\n",
    "[4] Andrea Valassi. *Fisher information metrics for binary classifier evaluation and training*. August 2018. URL: https://doi.org/10.5281/zenodo.1405727, doi:10.5281/zenodo.1405727.\n",
    "\n",
    "[5] *A Living Review of Machine Learning for Particle Physics*. (n.d.). Consultado en julio 15, 2021, de https://iml-wg.github.io/HEPML-LivingReview/ \n",
    "\n",
    "[6] D’Agnolo, Raffaele Tito. *Machine Learning for New Physics Searches.* ArXiv:1809.11150 [Hep-Ph], Sept. 2018. arXiv.org, http://arxiv.org/abs/1809.11150.\n",
    "\n",
    "##### Las referencias de todo el proyecto se encuentran en [este link](https://marianaiv.github.io/tesis_grado_UCV/capitulos/referencias/referencias.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <font color='#307a71'>Extra </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Agrupamiento de jets\n",
    "La existencia de un jet es dependiente de la regla matemática que lo define. Esta regla matemática agrupa los constituyentes del jet de acuerdo a propiedades cinemáticas.\n",
    "### Recombinación secuencial\n",
    "Utilizan las siguientes medidas de distancia entre dos constituyentes:\n",
    "$$\n",
    "    d_{ij} = min(p_{Ti}^{2p},p_{Tj}^{2p})\\times \\frac{\\Delta R_{ij}^2}{R}\n",
    "$$\n",
    "y la distancia entre el eje del haz y el constituyente detectado:\n",
    "$$\n",
    "    d_{iB}=p_{Ti}^{2p}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Para poder estudiar los eventos de jets, hace falta reconstruirlos, puesto que lo que se mide de los eventos de jets son características de las partículas de color neutro resultado de la hadronización. \n",
    "\n",
    "La definición de un jet no es única. Su existencia es dependiente de la regla matemática que lo define. Esta regla matemática se conoce como algoritmo de agrupamiento de jets. En este trabajo partícularmete, vamos a utilizar un algoritmo de recombinación secuencial.\n",
    "\n",
    "Los algoritmos de recombinación secuencial asumen que los constituyentes de un jet poseen una pequeña diferencia en el momento transverso. Por esto, las partículas son agrupadas en el espacio de momento.\n",
    "\n",
    "En general, los algoritmos de recombinación secuencial utilizan las medidas de distancia presentadas en la lámina: la distancia entre dos constituyentes, y la distancia entre el eje del haz y el constituyente detectado. $p_T$ es el momento transverso de las partículas, $\\Delta R_{ij} = \\sqrt{(\\eta_i-\\eta_j)^2+(\\phi_i-\\phi_j)^2}$ es la distancia entre dos constituyentes en el espacio $(\\eta-\\phi)$, $R$ es el radio final del jet, usualmente entre 0.4-0.7 y $p$ es un parámetro referente al tipo de algoritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Algoritmos de recombinación secuencial**\n",
    "<hr> \n",
    "\n",
    "**Input:** Constituyentes del evento<br>\n",
    "1. Hallar el mínimo en el conjunto $\\{d_{ij},d_{iB}\\}$:<br>\n",
    "  1. *Si el mínimo es $d_{ij}$*: los constituyentes *i* y *j* se unen en un solo constituyente *ij*, sumando el cuadri-momento y eliminando *i* y *j*  de la lista de constituyentes.<br>\n",
    "  2. *Si el mínimo es $d_{iB}$*: *i* se considera jet y eliminado de la lista de constituyentes.  \n",
    "\n",
    "<hr> \n",
    "\n",
    "<figure>\n",
    "<center><img src=\"../tesis/figuras/presentacion-algrecombinacion.png\" style=\"width:100%\"></center>\n",
    "<figcaption align = \"center\"> <small>Fig: Algoritmos de recombinación secuencial.</small></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "El algoritmo halla el minímo entre la distancia entre constituyentes y la distancia entre el eje del haz y el constituyente. Si el mínimo se halla entre dos constituyentes, se unen en un solo constituyente, se eliminan individualemnte de la lista de constituyentes y se repite le evaluación. Si el mínimo es la distancia entre el eje del haz y el constituyente, se considera que se reconstruyó un jet y se elimina de la lista de constituyentes. Esto se repite hasta que todas las partículas forman parte de un jet o hasta que se tengan una catidad de jets específicos, lo que se conoce como agrupamiento inclusivo y exclusivo, respectivamente.\n",
    "\n",
    "El algoritmo a utilizar en el trabajo se conoce como **anti-kt**, para el cual p = -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Masa\n",
    "La masa de un jet es cercana a la masa de la partícula de la cual se origina, asumiendo que los productos del decaimiento están contenidos en el jet. \n",
    "\n",
    "Está definida como la suma de la masa invariante de todos los constituyentes del jet, calculada a partir del cuadri-momento de cada constituyente. \n",
    "\n",
    "<figure>\n",
    "<center><img src=\"../tesis/figuras/jets-masa.png\" style=\"width:50%\"></center>\n",
    "<figcaption align = \"center\"> <small>Fig: Distribuciones de la masa del jet principal para jets de quarks y gluones, o fondo, y de una señal de nueva física con partículas de 500 y 100 GeV.</small></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "La masa es la variable más evidente para diferenciar entre jets provinientes de distintas partículas puesto que esta va a ser cercana a la masa de la partícula de la cual se origina el jet. \n",
    "\n",
    "En la Figura 6 se presenta un ejemplo de la distribución de la masa del jet principal de eventos provenientes de quarks y gluones, o fondo, y para una señal de nueva física con partículas de 500 y 100 GeV que decaen a jets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### N-subjettiness\n",
    "Esta variable intenta diferenciar jets de acuerdo al número N de subjets que lo conforman utilizando $\\tau_N$.\n",
    "\n",
    "Usualmente se utilizan variables adimensionales variables adimensionales $\\longrightarrow$ $\\tau_{N,N-1}=\\frac{\\tau_{N}}{\\tau_{N-1}}$\n",
    "\n",
    "<figure>\n",
    "<center><img src=\"../tesis/figuras/jets-tau.png\" style=\"width:45%\"></center>\n",
    "<figcaption align = \"center\"> <small>Fig: Distribuciones de $\\tau_{21}$ para jets de quarks y gluones, o fondo, y de una señal de nueva física con jets que tienen subestructura de 2 o más jets.</small></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "N-subjettiness diferencia entre jets por el número N de subjets que conforman a un jet. Para esto se hace un agrupamiento exclusivo de N jets y se calcula $\\tau_N$. Un valor menor de $\\tau_N$ corresponde a una cantidad de subjets igual o menor, mientras que un mayor valor indica más de N subjets. \n",
    "\n",
    "Se suelen utilizar variables adimensionales como $\\tau_{N,N-1}$. Un jet con N-1 subjets tendrá un valor de $\\tau_{N,N-1}$ mayor a un jet conformado por N o más subjets. \n",
    "\n",
    "En la fig. 7, se presenta un ejemplo de la distribución de  para jets provenientes de quarks y gluones, o fondo, y para una señal de nueva física que decae a jets con una subestructura de 2 o más subjets.\n",
    "\n",
    "($\\tau_N$: la suma corre sobre todos los constituyentes de cada jet de momento $pT_i$ y distancia angular $\\Delta R_{min,i}$ entre el constituyente y los ejes de los jets. $d_0$ es la suma de momento de todos los constituyentes multiplicada por el radio del jet.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### UCluster\n",
    "La objetivo principal de UCluster es **reducir la dimensionalidad de los datos** para retener las características principales del evento de colisión **para luego agrupar puntos con características similares en este espacio reducido** o latente.\n",
    "\n",
    "Para la reducción de dimensiones:\n",
    "- Se utiliza una red neuronal.\n",
    "- Se hace una *clasificación de masa de jet por partícula*.\n",
    "\n",
    "El algoritmo consta de dos tareas: una de clasificación y una de agrupamiento, por lo que la función de pérdida consta de dos partes:\n",
    "- La pérdida focal.\n",
    "- La pérdida respecto al agrupamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### GAN-AE\n",
    "GAN-AE es un método que intenta asociar una red neuronal con un codificador automático como una red generativa antagónica. La red discriminate es un perceptrón multicapas y la red generadora un codificador automático.\n",
    "\n",
    "1. La red discriminante se entrena con una mezcla de eventos originales y reconstruidos por el codificador.\n",
    "2. El codificador automático se entrena en función de qué tan bien engaña a la red discriminante.\n",
    "3. Se evalúa el codificador con una figura de mérito que indica qué tan bien engaña a la red discriminante.\n",
    "4. Una vez que la figura de mérito es cercana a cero se descarta la red discriminante y se utiliza la distancia euclídea media como característica discriminativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Métricas de rendimiento\n",
    "La métrica de evaluación primaria es la matriz de confusión.\n",
    "<center><img src=\"../tesis/figuras/met-matrizconfusion.png\" style=\"width:100%\"></center>\n",
    "<figcaption align = \"center\"> <small>Fig: Matriz de confusión.</small></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Métricas numéricas\n",
    "| Métrica      | Ecuación &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;| Enfoque de evaluación   |\n",
    "|:------------|:---------------------------:|:-------------------------|\n",
    "| Exactitud    | $\\frac{TP+TN}{TP+FP+FN+TN}$ | Número correcto de predicciones sobre todas las predicciones hechas |\n",
    "| Precisión    | $\\frac{TP}{TP+FP}$ | Proporción de tasa de verdaderos positivos |\n",
    "| Recuperación | $\\frac{TP}{TP+FN}$ | Efectividad del clasificador para identificar etiquetas positivas |\n",
    "| Especificidad| $\\frac{TN}{TN+FP}$ | Efectividad del clasificador para identificar etiquetas negativas |\n",
    "| Puntaje f1 | $\\frac{2\\text{precisión}\\times\\text{recuperación}}{\\text{precisión}+\\text{recuperación}}$ | Promedio ponderado de precisión y sensibilidad |\n",
    "| Exactitud balanceada| $\\frac{\\text{recuperación}+\\text{especificidad}}{2}$| Promedio de la proporción de predicciones correctas de cada clase |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  <strong>Nota:</strong> los clasificadores de agrupamiento no asignan puntuaciones, asignan distancias entre los datos y los centroides. En este trabajo, se tomó la distancia hacia el centroide de señal como el puntaje relacionado a la probabilidad de ser señal.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "Sin embargo, los clasificadores de agrupamiento como KMeans y UCluster, al agrupar los datos, no proporcionan un puntaje o probabilidad. En este trabajo se tomó la distancia al centroide de señal como puntaje relacionado a la probabilidad de ser señal: mientras los datos estén más cerca del centroide, más probable que sean señal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Curva de funcionamiento del receptor (ROC)\n",
    "La curva ROC se construye graficando la recuperación vs. 1-especificidad para varios umbrales de decisión. \n",
    "<center><img src=\"../tesis/figuras/ml-roc.png\" style=\"width:40%\"></center>\n",
    "<figcaption align = \"center\"> <small>Fig: La diagonal representa a un clasificador aleatorio o que no distingue entre clases. En este caso, el clasificador con la curva azul es mejor distinguiendo entre clases.</small></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "La curva ROC se construye graficando la recuperación vs. 1-especificidad para varios umbrales de decisión. En términos de HEP, la eficiencia de señal vs. la eficiencia de fondo. Como se observa en la figura 23, el clasificador es mejor a medida que la curva se acerca a la esquina superior izquierda, donde la tasa de verdaderos positivos o la eficiencia de señal es uno y los falsos positivos en cero. La diagonal representa un clasificador aleatorio. Una curva debajo de la diagonal significa que se está realizando la clasificación inversa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "El *área bajo de la curva* (AUC, por sus siglas en inglés) representa la habilidad del clasificador para distinguir entre clases. Un valor de AUC de 0.5 indica que la predicción no es mejor que una clasificación aleatoria. Menor a 0.5 indica que el clasificador está clasificando de manera inversa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Variaciones de la curva ROC\n",
    "En HEP se utilizan versiones de esta curva: la curva de *eficiencia de señal* vs. *rechazo de fondo* e *inverso de la eficiencia de fondo* vs. *eficiencia de señal*.\n",
    "\n",
    "<center><img src=\"../tesis/figuras/ml-otrasroc.PNG\" style=\"width:100%\"></center>\n",
    "<figcaption align = \"center\"> <small>Fig: Ejemplos de otras versiones de la curva ROC.</small></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "En HEP se utilizan variaciones de esta curva: eficiencia de señal vs. rechazo de fondo  y eficiencia de fondo vs. el inverso de la eficiencia de fondo. \n",
    "\n",
    "La curva ROC y el AUC tienen sus limitaciones:\n",
    "\n",
    "- La comparación de dos curvas ROC que se cruzan no es tan evidente, ya que el AUC se construye como una integral que otorga el mismo peso a todas las partes de la curva. Sin embargo, para la clasificación se escoge un punto específico. En este caso, otras métricas se deben utilizar para definir cuál clasificador proporciona mejor rendimiento en la región donde se elija el umbral de decisión.\n",
    "\n",
    "- El uso de las curvas ROC puede no ser apropiado para problemas que incluyan datos altamente desbalanceados, debido a que conduce a evaluaciones demasiado optimistas. La curva PR puede ser más informativa en este caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Curva Precisión-Recuperación (PR)\n",
    "Para datos altamente desbalanceados se suele sugerir el uso de la curva PR, donde se grafica la precisión vs. la recuperación.\n",
    "<center><img src=\"../tesis/figuras/ml-curvapr.png\" style=\"width:40%\"></center>\n",
    "<figcaption align = \"center\"> <small>Fig: Ejemplos de curvas precisión-recuperación.</small></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "---\n",
    "La curva PR se suele sugerir para datos altamente desbalanceados porque cambia al cambiar la distribución de clases, contrario a la curva ROC. La curva PR es mejor a medida que se acerca a la esquina superior derecha. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Análogo al AUC, se utiliza la *precisión promedio* (AP, por sus siglas en inglés). La precisión promedio resume la curva PR utilizando la media ponderada de las precisiones logradas en cada umbral, usando como peso el aumento en recuperación del umbral anterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Mejora de la significancia\n",
    "Está definida como:\n",
    "$$\n",
    "    \\text{Mejora de la significancia} = \\frac{\\epsilon_s}{\\sqrt{\\epsilon_b}}\n",
    "$$\n",
    "Una mejora de la significancia igual a 2 significa que la mejora de la significancia inicial es amplificada por un factor de 2 después de utilizar la estrategia de clasificación.\n",
    "\n",
    "<center><img src=\"../tesis/figuras/ml-significancia.PNG\" style=\"width:40%\"></center>\n",
    "<figcaption align = \"center\"> <small>Fig: Ejemplo de curva de mejora de la significancia.</small></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "La mejora de la significancia ayuda a visualizar la significancia pra distintos umbrales. La significancia estadística nos ayuda a cuantificar si un resultado probablemente se deba al azar o a algún factor de interés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Estructura de los datos\n",
    "Cada fila es un evento y la última columna hace referencia a si el evento es fondo (0) o señal (1).\n",
    "\n",
    "|Evento   |$pT$     | $\\eta$  | $\\phi$  | $pT$    | $\\eta$  | $\\phi$  | $\\cdots$| clase |\n",
    "|---------|---------|---------|---------|---------|---------|---------|---------|-------|\n",
    "| 1       | $\\cdots$| $\\cdots$| $\\cdots$| $\\cdots$| $\\cdots$| $\\cdots$| $\\cdots$| 1     |\n",
    "| 2       | $\\cdots$| $\\cdots$| $\\cdots$| $\\cdots$| $\\cdots$| $\\cdots$| $\\cdots$| 0     |\n",
    "| $\\vdots$| $\\cdots$| $\\cdots$| $\\cdots$| $\\cdots$| $\\cdots$| $\\cdots$| $\\cdots$| 0     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Comparación de las distribuciones entre R&D y BB1\n",
    "<table><tr>\n",
    "<td> <img src=\"../tesis/figuras/datospp-vardiff-RnD.png\" alt=\"Drawing\" style=\"width:100%;\"/> <figcaption align = \"center\"> <small>Fig: Distribución de $pT$, $E$, $m$ y $\\tau_{21}$ del jet principal en el conjunto R&D.</small></figcaption> </td>\n",
    "<td> <img src=\"../tesis/figuras/datospp-vardiff-BB1.png\" alt=\"Drawing\" style=\"width:100%;\"/> <figcaption align = \"center\"> <small>Fig: Distribución de $pT$, $E$, $m$ y $\\tau_{21}$ del jet principal en el conjunto BB1.</small></figcaption></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Jet principal del conjunto R&D: menor separación entre señal y fondo\n",
    "<center><img src=\"../tesis/figuras/datospp-vareq-RnD.png\" alt=\"Drawing\" style=\"width:100%\"/></center>\n",
    "<figcaption align = \"center\"> <small>Fig: Distribución de $\\eta$, nro. de hadrones y $\\phi$ del jet principal para el conjunto R&D.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Jet secundario del conjunto R&D\n",
    "<center><img src=\"../tesis/figuras/datospp-jet2-RnD.png\" alt=\"Drawing\" style=\"width:100%\"/></center>\n",
    "<figcaption align = \"center\"> <small>Fig: Distribución de las variables del jet secundario para el conjunto R&D.</small>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
