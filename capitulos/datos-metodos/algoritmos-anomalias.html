
<!DOCTYPE html>

<html lang="es">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>4.2. Algoritmos para detección de anomalías &#8212; Búsqueda de nueva física utilizando técnicas de aprendizaje automático en eventos de múltiples jets</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/translations.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Índice" href="../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../search.html" />
    <link rel="next" title="4.3. Algoritmos LHCO 2020" href="algoritmos-lhco.html" />
    <link rel="prev" title="4.1. Conjuntos de datos" href="datos-lhco.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="es">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo_ucv.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Búsqueda de nueva física utilizando técnicas de aprendizaje automático en eventos de múltiples jets</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Buscar este libro ..." aria-label="Buscar este libro ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduccion.html">
   1. Introducción
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../marco-teorico/resumen.html">
   2. Marco teórico
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../marco-teorico/modelo-estandar.html">
     2.1. El módelo estándar
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../marco-teorico/cromodinamica-cuantica.html">
     2.2. Cromodinámica cuántica
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../marco-teorico/mas-alla-del-ms.html">
     2.3. Más allá del modelo estándar
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../marco-teorico/detectores.html">
     2.4. Detectores de partículas
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../marco-teorico/reconstruccion-jets.html">
     2.5. Reconstrucción de jets
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ml-fisica/resumen.html">
   3. Aprendizaje automático para la búsqueda de nueva física
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-fisica/aprendizaje-automatico.html">
     3.1. Aprendizaje automático
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-fisica/olimpiadas-LHC.html">
     3.2. El uso del aprendizaje automático y las Olimpiadas LHC 2020
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-fisica/reproducibilidad.html">
     3.3. Olimpiadas LHC 2020 y la importancia de la reproducibilidad
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="resumen.html">
   4. Datos y métodos
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="datos-lhco.html">
     4.1. Conjuntos de datos
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     4.2. Algoritmos para detección de anomalías
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="algoritmos-lhco.html">
     4.3. Algoritmos LHCO 2020
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="metricas.html">
     4.4. Métricas de rendimiento
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="benchtools.html">
     4.5. Paquete benchtools
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../analisis-datos/resumen.html">
   5. Exploración de datos
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../analisis-datos/datos-sin-preprocesar.html">
     5.1. Datos sin preprocesar
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../analisis-datos/datos-preprocesados.html">
     5.2. Datos preprocesados
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../analisis-datos/datos-UCluster.html">
     5.3. Datos de UCluster
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../analisis-datos/datos-GAN-AE.html">
     5.4. Datos de GAN-AE
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../resultados/resumen.html">
   6. Resultados
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../resultados/clasificacion.html">
     6.1. Clasificación de eventos
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../resultados/comparacion-algoritmos.html">
     6.2. Comparación de algoritmos
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../discusion/discusion.html">
   7. Discusión de resultados
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../conclusion/conclusion.html">
   8. Conclusión
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../referencias/referencias.html">
   9. Referencias
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Navegación de palanca" aria-controls="site-navigation"
                title="Navegación de palanca" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Descarga esta pagina"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/capitulos/datos-metodos/algoritmos-anomalias.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Descargar archivo fuente" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Imprimir en PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/marianaiv/tesis_grado_UCV"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Repositorio de origen"><i
                    class="fab fa-github"></i>repositorio</button></a>
        <a class="issues-button"
            href="https://github.com/marianaiv/tesis_grado_UCV/issues/new?title=Issue%20on%20page%20%2Fcapitulos/datos-metodos/algoritmos-anomalias.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Abrir un problema"><i class="fas fa-lightbulb"></i>Tema abierto</button></a>
        <a class="edit-button" href="https://github.com/marianaiv/tesis_grado_UCV/edit/main/tesis/capitulos/datos-metodos/algoritmos-anomalias.md"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edita esta página"><i class="fas fa-pencil-alt"></i>sugerir editar</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Modo de pantalla completa"
        title="Modo de pantalla completa"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contenido
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bosque-aleatorio">
   4.2.1. Bosque aleatorio
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#potenciacion-del-gradiente">
   4.2.2. Potenciación del gradiente
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#analisis-de-discriminante-cuadratico">
   4.2.3. Análisis de discriminante cuadrático
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#redes-neuronales">
   4.2.4. Redes neuronales
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#k-means">
   4.2.5. K-means
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#codificador-automatico">
   4.2.6. Codificador automático
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#red-generativa-antagonica">
   4.2.7. Red generativa antagónica
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Algoritmos para detección de anomalías</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contenido </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bosque-aleatorio">
   4.2.1. Bosque aleatorio
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#potenciacion-del-gradiente">
   4.2.2. Potenciación del gradiente
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#analisis-de-discriminante-cuadratico">
   4.2.3. Análisis de discriminante cuadrático
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#redes-neuronales">
   4.2.4. Redes neuronales
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#k-means">
   4.2.5. K-means
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#codificador-automatico">
   4.2.6. Codificador automático
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#red-generativa-antagonica">
   4.2.7. Red generativa antagónica
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="algoritmos-para-deteccion-de-anomalias">
<span id="alg"></span><h1><span class="section-number">4.2. </span>Algoritmos para detección de anomalías<a class="headerlink" href="#algoritmos-para-deteccion-de-anomalias" title="Enlazar permanentemente con este título">¶</a></h1>
<p>En este proyecto se trata de resolver un problema de clasificación binaria con los datos de las LHCO 2020, que son altamente desbalanceados. Esto se conoce como una tarea de <em>detección de anomalías</em>, que en el caso de este trabajo hace referencia a la detección de los eventos de señal.</p>
<p>La implementación de aprendizaje automático en este trabajo está comprendida por los siguientes pasos:</p>
<ol class="simple">
<li><p>Preprocesamiento de los datos utilizando <code class="docutils literal notranslate"><span class="pre">benchtool</span></code>, descrito en la <a class="reference internal" href="benchtools.html#bench-pre"><span class="std std-numref">Sección 4.5.2</span></a>.</p></li>
<li><p>División de los datos en conjuntos mutuamente excluyentes, 70% en un conjunto de entrenamiento y 30% en uno de prueba.</p></li>
<li><p>Ajuste de los modelos minimizando una función de pérdida específica para cada uno (ver <a class="reference internal" href="../ml-fisica/aprendizaje-automatico.html#ml-conceptos"><span class="std std-numref">Sección 3.1.1</span></a>), utilizando los datos de entrenamiento.</p></li>
<li><p>Evaluación del rendimiento del modelo, calculando la función de pérdida con los datos de prueba.</p></li>
</ol>
<p>Los algoritmos utilizados en este trabajo se escogieron a partir de su rendimiento, estudiado durante el desarrollo de las herramientas de análisis de datos. Más información sobre cómo se escogieron estos algoritmos se encuentra en la sección <em><a class="reference external" href="https://github.com/marianaiv/benchtools/tree/main/notebooks">notebooks</a></em> del repositorio de <code class="docutils literal notranslate"><span class="pre">benchtools</span></code>.</p>
<p>A continuación, se explicarán los algoritmos utilizados, enfocándonos en su uso para la tarea de clasificación binaria. También se resumirán algunos métodos necesarios para explicar más adelante los algoritmos de las LHCO 2020 utilizados en este trabajo. La referencia principal de esta sección es <span id="id1">[<a class="reference internal" href="../referencias/referencias.html#id63" title="Pankaj Mehta, Marin Bukov, Ching-Hao Wang, Alexandre G.R. Day, Clint Richardson, Charles K. Fisher, and David J. Schwab. A high-bias, low-variance introduction to machine learning for physicists. Physics Reports, 810:1–124, may 2019. URL: https://doi.org/10.1016%2Fj.physrep.2019.03.001, doi:10.1016/j.physrep.2019.03.001.">68</a>]</span>.</p>
<div class="section" id="bosque-aleatorio">
<span id="alg-bosques"></span><h2><span class="section-number">4.2.1. </span>Bosque aleatorio<a class="headerlink" href="#bosque-aleatorio" title="Enlazar permanentemente con este título">¶</a></h2>
<p>Los bosques aleatorios son algoritmos supervisados ampliamente utilizados para tareas complejas de clasificación. Estos algoritmos son ensambles de árboles de decisión.</p>
<p>Un <strong>árbol de decisión</strong> utiliza una serie de preguntas para realizar la partición jerárquica de los datos con el objetivo de hallar un conjunto de reglas que separen el espacio de características<span id="id2">[<a class="reference internal" href="../referencias/referencias.html#id67" title="Anthony J. Myles, Robert N. Feudale, Yang Liu, Nathaniel A. Woody, and Steven D. Brown. An introduction to decision tree modeling. Journal of Chemometrics, 18(6):275-285, 2004. doi:10.1002/cem.873.">93</a>]</span>. El árbol de decisión utiliza las variables del conjunto de datos para crear preguntas con respuestas booleanas y dividir continuamente el conjunto de datos hasta aislar todos los puntos de datos que pertenecen a cada clase. Este proceso organiza los datos en una estructura de árbol. Cada vez que haces una pregunta estás agregando un nodo al árbol.</p>
<p>Los árboles de decisión utilizan funciones de pérdida que evalúan la partición de los datos en función de la pureza de los nodos resultantes, una función de pérdida que compara la distribución de clases antes y después de la división<span id="id3">[<a class="reference internal" href="../referencias/referencias.html#id135" title="Pang-Ning Tan, Michael Steinbach, and Vipin Kumar. Introduction to Data Mining. Addison Wesley, May 2005. ISBN 0321321367.">94</a>]</span>. Esto se conoce como <em>criterio de impureza</em>. Uno de los criterios más utilizado es el criterio <em>Gini</em>, que mide cuánto ruido tiene una categoría:</p>
<div class="math notranslate nohighlight" id="equation-gini">
<span class="eqno">(4.1)<a class="headerlink" href="#equation-gini" title="Enlace permanente a esta ecuación">¶</a></span>\[
    H(Q_m)=\sum_{k} p_{mk}(1-p_{mk})
\]</div>
<p>donde <span class="math notranslate nohighlight">\(Q_m\)</span> representa los datos en el nodo <span class="math notranslate nohighlight">\(m\)</span> y <span class="math notranslate nohighlight">\(p_{mk}\)</span> es la proporción de clase <span class="math notranslate nohighlight">\(k\)</span> observada en el nodo <span class="math notranslate nohighlight">\(m\)</span>, donde las clases para clasificación binaria son 0 y 1. Un diagrama de un árbol de decisión se observa en la <a class="reference internal" href="#ml-arboldecision"><span class="std std-numref">Figura 4.2</span></a>.</p>
<div class="figure align-default" id="ml-arboldecision">
<a class="reference internal image-reference" href="../../_images/ml-arboldecision.png"><img alt="../../_images/ml-arboldecision.png" src="../../_images/ml-arboldecision.png" style="width: 450px;" /></a>
<p class="caption"><span class="caption-number">Figura 4.2 </span><span class="caption-text">Ejemplo de un árbol de decisión. Para una conjunto de características <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, su etiqueta <span class="math notranslate nohighlight">\(y\)</span> es predicha, recorriéndolo desde su raíz, pasando por las hojas, siguiendo las ramas que satisface <span id="id4">[<a class="reference internal" href="../referencias/referencias.html#id63" title="Pankaj Mehta, Marin Bukov, Ching-Hao Wang, Alexandre G.R. Day, Clint Richardson, Charles K. Fisher, and David J. Schwab. A high-bias, low-variance introduction to machine learning for physicists. Physics Reports, 810:1–124, may 2019. URL: https://doi.org/10.1016%2Fj.physrep.2019.03.001, doi:10.1016/j.physrep.2019.03.001.">68</a>]</span>.</span><a class="headerlink" href="#ml-arboldecision" title="Enlace permanente a esta imagen">¶</a></p>
</div>
<p>Para crear un ensamble de árboles de decisión y obtener un bosque aleatorio, se deben utilizar procesos de aleatorización. Usualmente se utilizan en conjunto dos métodos. Los subconjuntos de datos utilizados para entrenar cada árbol se obtienen del conjunto de entrenamiento mediante bootstrapping, un método de muestreo con remplazo. Luego, en la construcción de los árboles de decisión, en cada nodo se utiliza un subconjunto aleatorio de las características de entrada.</p>
<p>Usualmente, cada árbol emite un voto unitario para la clase más popular dada una entrada y la clase con más votos es asignada a esta entrada<span id="id5">[<a class="reference internal" href="../referencias/referencias.html#id66" title="Leo Breiman. Random Forests. Machine Learning, 45(1):5–32, 2001. doi:10.1023/A:1010933404324.">95</a>]</span>, como se observa en la <a class="reference internal" href="#ml-bosquealeatorio"><span class="std std-numref">Figura 4.3</span></a>. Sin embargo, la implementación utilizada en este trabajo combina los árboles individuales promediando su predicción probabilística<span id="id6">[<a class="reference internal" href="../referencias/referencias.html#id73" title="scikit-learn documentation. Ensemble methods: random forests. URL: https://scikit-learn.org/stable/modules/ensemble.html#forests-of-randomized-trees.">96</a>]</span>.</p>
<div class="figure align-default" id="ml-bosquealeatorio">
<a class="reference internal image-reference" href="../../_images/ml-bosquealeatorio.png"><img alt="../../_images/ml-bosquealeatorio.png" src="../../_images/ml-bosquealeatorio.png" style="width: 550px;" /></a>
<p class="caption"><span class="caption-number">Figura 4.3 </span><span class="caption-text">Representación visual del funcionamiento de un bosque aleatorio <span id="id7">[<a class="reference internal" href="../referencias/referencias.html#id69" title="TIBCO. What is a random forest? URL: https://www.tibco.com/reference-center/what-is-a-random-forest.">97</a>]</span>.</span><a class="headerlink" href="#ml-bosquealeatorio" title="Enlace permanente a esta imagen">¶</a></p>
</div>
</div>
<div class="section" id="potenciacion-del-gradiente">
<span id="alg-gbc"></span><h2><span class="section-number">4.2.2. </span>Potenciación del gradiente<a class="headerlink" href="#potenciacion-del-gradiente" title="Enlazar permanentemente con este título">¶</a></h2>
<p>El clasificador de potenciación del gradiente (GBC, por sus siglas en inglés) es un modelo supervisado que utiliza árboles de regresión como aprendiz débil. Es un modelo supervisado y aditivo que avanza por etapas<span id="id8">[<a class="reference internal" href="../referencias/referencias.html#id71" title="scikit-learn documentation. Sklearn.ensemble.gradientboostingclassifier. URL: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html.">98</a>]</span>. En cada etapa, se ajusta el árbol al error residual, es decir, el error asociado al árbol anterior, con el objetivo de minimizar el error, como se muestra en la <a class="reference internal" href="#ml-gbc"><span class="std std-numref">Figura 4.4</span></a>.</p>
<div class="figure align-default" id="ml-gbc">
<a class="reference internal image-reference" href="../../_images/ml-gbc.png"><img alt="../../_images/ml-gbc.png" src="../../_images/ml-gbc.png" style="width: 40px;" /></a>
<p class="caption"><span class="caption-number">Figura 4.4 </span><span class="caption-text">Diagrama del ensamble de árboles de decisión para formar un GBC<span id="id9">[<a class="reference internal" href="../referencias/referencias.html#id134" title="Aratrika Pal. Gradient Boosting Trees for Classification: A Beginner’s Guide. 2020. URL: https://medium.com/swlh/gradient-boosting-trees-for-classification-a-beginners-guide-596b594a14ea.">99</a>]</span>.</span><a class="headerlink" href="#ml-gbc" title="Enlace permanente a esta imagen">¶</a></p>
</div>
<p>Su formulación matemática es la siguiente<span id="id10">[<a class="reference internal" href="../referencias/referencias.html#id74" title="scikit-learn documentation. Ensemble methods: gradient tree boosting. URL: https://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting.">100</a>]</span>: la predicción <span class="math notranslate nohighlight">\(y_i\)</span> del modelo para la entrada <span class="math notranslate nohighlight">\(x_i\)</span> está dada por:</p>
<div class="math notranslate nohighlight" id="equation-ml-gbcpred">
<span class="eqno">(4.2)<a class="headerlink" href="#equation-ml-gbcpred" title="Enlace permanente a esta ecuación">¶</a></span>\[
    \hat{y}_i=F_M(x_i)=\sum_{m=1}^{M}h_m(x_i)
\]</div>
<p><span class="math notranslate nohighlight">\(h_m\)</span> son los aprendices débiles. En el caso de clasificación, el mapeo del valor de <span class="math notranslate nohighlight">\(F_M(x_i)\)</span> a una clase o probabilidad es dependiente de la pérdida. La probabilidad de que <span class="math notranslate nohighlight">\(x_i\)</span> pertenezca a la clase positiva se modela usando la función sigmoid,</p>
<div class="math notranslate nohighlight" id="equation-ml-gbcsigmoid">
<span class="eqno">(4.3)<a class="headerlink" href="#equation-ml-gbcsigmoid" title="Enlace permanente a esta ecuación">¶</a></span>\[
    p(y_i=|x_i)=\sigma(F_M(x_i))
\]</div>
<p>El GBC se construye de la siguiente manera:</p>
<div class="math notranslate nohighlight" id="equation-ml-gbc">
<span class="eqno">(4.4)<a class="headerlink" href="#equation-ml-gbc" title="Enlace permanente a esta ecuación">¶</a></span>\[
    F_m(x)=F_{m-1}(x)+h_m(x)
\]</div>
<p>y <span class="math notranslate nohighlight">\(h_m\)</span> se ajusta para minimizar la suma de las pérdidas dado el ensamble anterior <span class="math notranslate nohighlight">\(F_{m-1}\)</span>,</p>
<div class="math notranslate nohighlight" id="equation-ml-gbcaprendizdebil">
<span class="eqno">(4.5)<a class="headerlink" href="#equation-ml-gbcaprendizdebil" title="Enlace permanente a esta ecuación">¶</a></span>\[
    h_m\approx\text{arg min}_h\sum_{i=1}^{n}h(x_i)g_i
\]</div>
<p>donde <span class="math notranslate nohighlight">\(g_i\)</span> es la derivada de la función de pérdida con respecto a su segundo parámetro, evaluada en <span class="math notranslate nohighlight">\(F_{m-1}(x)\)</span>. La suma en la ec.<a class="reference internal" href="#equation-ml-gbcaprendizdebil">(4.5)</a> se minimiza si <span class="math notranslate nohighlight">\(h(x_i)\)</span> se ajusta para predecir un valor proporcional al gradiente negativo <span class="math notranslate nohighlight">\(−g_i\)</span>. Por lo tanto, en cada iteración, el estimador <span class="math notranslate nohighlight">\(h_m\)</span> está ajustado para predecir los gradientes negativos de las muestras. Estos gradientes se actualizan en cada iteración. El proceso puede considerarse como una especie de descenso de gradiente en un espacio funcional.</p>
</div>
<div class="section" id="analisis-de-discriminante-cuadratico">
<span id="alg-qda"></span><h2><span class="section-number">4.2.3. </span>Análisis de discriminante cuadrático<a class="headerlink" href="#analisis-de-discriminante-cuadratico" title="Enlazar permanentemente con este título">¶</a></h2>
<p>El análisis de discriminante cuadrático (QDA, por sus siglas en inglés)<span id="id11">[<a class="reference internal" href="../referencias/referencias.html#id75" title="1.2. linear and quadratic discriminant analysis. Accessed: 08-04-2022. URL: scikit-learn.org/stable/modules/lda_qda.html.">101</a>]</span> es un clasificador supervisado con un límite de decisión cuadrático. El modelo asume que las densidades condicionales de clase <span class="math notranslate nohighlight">\(P(\mathbf{X}|y=k)\)</span>, para cada clase <span class="math notranslate nohighlight">\(k\)</span>, están distribuidas normalmente. Las predicciones para cada muestra de entrenamiento <span class="math notranslate nohighlight">\(x\)</span> se obtienen utilizando el teorema de Bayes:</p>
<div class="math notranslate nohighlight" id="equation-ml-qdabayes">
<span class="eqno">(4.6)<a class="headerlink" href="#equation-ml-qdabayes" title="Enlace permanente a esta ecuación">¶</a></span>\[
    P(y=k|x) = \frac{P(x|y=k)P(y=k)}{P(x)}
\]</div>
<p>donde se selecciona la clase <span class="math notranslate nohighlight">\(k\)</span> que maximice esta probabilidad. Un ejemplo de clasificación utilizando este método se observa en la <a class="reference internal" href="#ml-qda"><span class="std std-numref">Figura 4.5</span></a>.</p>
<div class="figure align-default" id="ml-qda">
<a class="reference internal image-reference" href="../../_images/ml-qda.png"><img alt="../../_images/ml-qda.png" src="../../_images/ml-qda.png" style="width: 500px;" /></a>
<p class="caption"><span class="caption-number">Figura 4.5 </span><span class="caption-text">Clasificación con QDA. a) Los puntos a ser clasificados, b) los límites o fronteras de decisión. La barra de color indica la probabilidad de pertenecer a la clase 1<span id="id12">[<a class="reference internal" href="../referencias/referencias.html#id76" title="Quadratic discriminant analysis. 2021. Accessed: 08-04-2022. URL: https://towardsdatascience.com/quadratic-discriminant-analysis-ae55d8a8148a.">102</a>]</span></span><a class="headerlink" href="#ml-qda" title="Enlace permanente a esta imagen">¶</a></p>
</div>
</div>
<div class="section" id="redes-neuronales">
<span id="alg-neural"></span><h2><span class="section-number">4.2.4. </span>Redes neuronales<a class="headerlink" href="#redes-neuronales" title="Enlazar permanentemente con este título">¶</a></h2>
<p>Las redes neuronales (NN, por sus siglas en inglés) son modelos supervisados y no lineales inspirados en las neuronas. Se definen mediante una serie de transformaciones que mapean la entrada <span class="math notranslate nohighlight">\(x\)</span> a estados «ocultos» <span class="math notranslate nohighlight">\(\mathbf{h}_i\)</span>. Finalmente, una última transformación mapea estos estados a una función de salida <span class="math notranslate nohighlight">\(\mathbf{y}\)</span><span id="id13">[<a class="reference internal" href="../referencias/referencias.html#id77" title="Dan Guest, Kyle Cranmer, and Daniel Whiteson. Deep learning and its application to LHC physics. Annual Review of Nuclear and Particle Science, 68(1):161–181, oct 2018. URL: https://doi.org/10.1146%2Fannurev-nucl-101917-021019, doi:10.1146/annurev-nucl-101917-021019.">64</a>]</span>. Esto también se conoce como perceptrón multicapas. Las transformaciones se pueden escribir matemáticamente como:</p>
<div class="math notranslate nohighlight" id="equation-ml-nnneurona">
<span class="eqno">(4.7)<a class="headerlink" href="#equation-ml-nnneurona" title="Enlace permanente a esta ecuación">¶</a></span>\[
    \mathbf{h}_i = g_i(W_i\mathbf{h}_i+\mathbf{b}_i)
\]</div>
<p>donde <span class="math notranslate nohighlight">\(g_i\)</span> es una función conocida como <em>función de activación</em> y <span class="math notranslate nohighlight">\(\mathbf{h}_i\)</span> representa la transformación iésima de <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, llamada <em>encaje</em>. <span class="math notranslate nohighlight">\(W\)</span> es la matriz de los <em>pesos</em> y <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> el vector de los <em>sesgos</em>.</p>
<p>El objetivo es hallar los pesos y sesgos que optimizan la función de pérdida. Esto se logra utilizando las etiquetas de los datos y calculando el gradiente de la función de pérdida con respecto a los parámetros del modelo. Este proceso se conoce como <em>retropropagación</em> y requiere que las funciones sean diferenciables.</p>
<p>Las transformaciones se ordenan en capas, como se observa en la <a class="reference internal" href="#ml-nn"><span class="std std-numref">Figura 4.6</span></a>, donde la salida de una capa es la entrada de la siguiente.</p>
<div class="figure align-default" id="ml-nn">
<a class="reference internal image-reference" href="../../_images/ml-nn.png"><img alt="../../_images/ml-nn.png" src="../../_images/ml-nn.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-number">Figura 4.6 </span><span class="caption-text">Diagrama de una red neuronal. Las transformaciones se ordenan por capas, donde la salida de una capa es la entrada de la siguiente<span id="id14">[<a class="reference internal" href="../referencias/referencias.html#id63" title="Pankaj Mehta, Marin Bukov, Ching-Hao Wang, Alexandre G.R. Day, Clint Richardson, Charles K. Fisher, and David J. Schwab. A high-bias, low-variance introduction to machine learning for physicists. Physics Reports, 810:1–124, may 2019. URL: https://doi.org/10.1016%2Fj.physrep.2019.03.001, doi:10.1016/j.physrep.2019.03.001.">68</a>]</span></span><a class="headerlink" href="#ml-nn" title="Enlace permanente a esta imagen">¶</a></p>
</div>
<p>La tarea de la red depende de su arquitectura. Aunque su uso es extenso, nos enfocaremos en su aplicación para clasificación binaria.. Para utilizar una red neuronal como clasificador binario, se utiliza la función sigmoid como función de activación de la última transformación. Se suele utilizar la <em>entropía cruzada binaria</em> como función de pérdida, que calcula la entropía cruzada entre las clases predichas y las clases reales.</p>
<div class="math notranslate nohighlight" id="equation-binary-crossentropy">
<span class="eqno">(4.8)<a class="headerlink" href="#equation-binary-crossentropy" title="Enlace permanente a esta ecuación">¶</a></span>\[
    \mathcal{L}_{BC} = -\frac{1}{N}\sum_{i=1}^N y_i\log(p(y_i))+(1-y_i)\log(1-p(y_1))
\]</div>
<p>donde <span class="math notranslate nohighlight">\(N\)</span> es el número de muestras a clasificar, <span class="math notranslate nohighlight">\(y_i\)</span> es la etiqueta de la muestra iésima y <span class="math notranslate nohighlight">\(p(y_i)\)</span> es la probabilidad de que la muestra sea de clase 1.</p>
</div>
<div class="section" id="k-means">
<span id="alg-kmeans"></span><h2><span class="section-number">4.2.5. </span>K-means<a class="headerlink" href="#k-means" title="Enlazar permanentemente con este título">¶</a></h2>
<p><em>K-means</em> es un algoritmo no supervisado que separa los datos en <span class="math notranslate nohighlight">\(K\)</span> grupos con igual varianza. Los grupos están caracterizados por la media de los datos pertenecientes al grupo. Estos se conocen como «centroides» y se representan con <span class="math notranslate nohighlight">\(\mu_k\)</span><span id="id15">[<a class="reference internal" href="../referencias/referencias.html#id78" title="scikit-learn documentation. Clustering: k-means. 2021. Accessed: 09-04-2022. URL: https://scikit-learn.org/stable/modules/clustering.html#k-means.">103</a>]</span>.</p>
<p>El objetivo del algoritmo es minimizar la <em>inercia</em> o <em>criterio de suma de cuadrados</em> dentro del grupo, definida como:</p>
<div class="math notranslate nohighlight" id="equation-ml-kmeansinertia">
<span class="eqno">(4.9)<a class="headerlink" href="#equation-ml-kmeansinertia" title="Enlace permanente a esta ecuación">¶</a></span>\[
    \mathcal{C}(\{x,\mu\})=\sum_{k=1}^{K}\sum_{n=1}^{N}r_{nk}(\mathbf{x}_n-\mu_k)^2
\]</div>
<p>donde <span class="math notranslate nohighlight">\(\mathbf{x}_n\)</span> es la observación enésima y <span class="math notranslate nohighlight">\(r_{nk}\)</span> es la asignación. <span class="math notranslate nohighlight">\(r_{nk}\)</span> es 1 si <span class="math notranslate nohighlight">\(x_n\)</span> pertenece al grupo y 0 de otra forma.</p>
<p>El algoritmo funciona mediante los siguientes pasos:</p>
<div class="proof algorithm admonition" id="alg-kmeans">
<p class="admonition-title"><span class="caption-number">Algorithm 4.1 </span> (K-means)</p>
<div class="algorithm-content section" id="proof-content">
<ol class="simple">
<li><p>Escoger los centroides. En la primera inicialización se escogen puntos aleatorios de los datos.</p></li>
<li><p>Asignar cada muestra al centroide más cercano, minimizando <span class="math notranslate nohighlight">\(\mathcal{C}\)</span>.</p></li>
<li><p>Crear nuevos centroides tomando el valor medio de todas las muestras asignadas a cada centroide anterior.</p></li>
<li><p>Calcular la diferencia entre los centroides anteriores y los nuevos.</p></li>
</ol>
</div>
</div><p>Los últimos tres pasos se repiten hasta que la diferencia entre los centroides esté debajo de un umbral, es decir, hasta que los centroides no se muevan significativamente. Como la inicialización de los centroides es aleatoria, usualmente se realizan múltiples inicializaciones y se escoge la que resulte en un menor valor de la inercia.</p>
</div>
<div class="section" id="codificador-automatico">
<span id="alg-ae"></span><h2><span class="section-number">4.2.6. </span>Codificador automático<a class="headerlink" href="#codificador-automatico" title="Enlazar permanentemente con este título">¶</a></h2>
<p>Los codificadores automáticos (AE, por sus siglas en inglés) son redes neuronales de aprendizaje no supervisado que mapean una entrada a una representación comprimida en un encaje, o espacio latente, y luego vuelve a sí misma, como se representa en la <a class="reference internal" href="#alg-aefig"><span class="std std-numref">Figura 4.7</span></a>. Al aprender como reproducir la salida original, el modelo extrae características de los datos de entrada<span id="id16">[<a class="reference internal" href="../referencias/referencias.html#id103" title="Yuichiro Nakai. Searching for New Physics with Deep Autoencoders. 2019. Accessed: 28-04-2022. URL: https://indico.cern.ch/event/746178/contributions/3389042/attachments/1848494/3033674/AutoencoderV5.pdf.">104</a>]</span>.</p>
<p>Estas redes se pueden dividir en dos partes. El codificador, que comprime los datos a un espacio latente, y el decodificador que produce la reconstrucción<span id="id17">[<a class="reference internal" href="../referencias/referencias.html#id102" title="Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. \url http://www.deeplearningbook.org.">105</a>]</span>. Una medida de qué tan bien funciona el codificador es la diferencia entre la entrada y la salida de acuerdo a alguna métrica de distancia conocida como «error de reconstrucción».</p>
<div class="figure align-default" id="alg-aefig">
<a class="reference internal image-reference" href="../../_images/alg-ae.png"><img alt="../../_images/alg-ae.png" src="../../_images/alg-ae.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Figura 4.7 </span><span class="caption-text">Diagrama del funcionamiento de un codificador automático. La entrada se mapea a una representación de dimensionalidad reducida y luego es reconstruida <span id="id18">[<a class="reference internal" href="../referencias/referencias.html#id104" title="Marco Farina, Yuichiro Nakai, and David Shih. Searching for new physics with deep autoencoders. Physical Review D, 101:075021, Apr 2020. URL: https://link.aps.org/doi/10.1103/PhysRevD.101.075021, doi:10.1103/PhysRevD.101.075021.">106</a>]</span>.</span><a class="headerlink" href="#alg-aefig" title="Enlace permanente a esta imagen">¶</a></p>
</div>
<p>Este algoritmo se ha empezado a utilizar en HEP como detector de anomalías puesto que, al entrenar el codificador automático en una muestra de eventos de fondo, va a aprender las características de fondo, y se espera que un evento de señal no sea reconstruido correctamente. Así, se puede utilizar un corte en el error de reconstrucción como un umbral de anomalía<span id="id19">[<a class="reference internal" href="../referencias/referencias.html#id104" title="Marco Farina, Yuichiro Nakai, and David Shih. Searching for new physics with deep autoencoders. Physical Review D, 101:075021, Apr 2020. URL: https://link.aps.org/doi/10.1103/PhysRevD.101.075021, doi:10.1103/PhysRevD.101.075021.">106</a>]</span>.</p>
</div>
<div class="section" id="red-generativa-antagonica">
<span id="alg-gan"></span><h2><span class="section-number">4.2.7. </span>Red generativa antagónica<a class="headerlink" href="#red-generativa-antagonica" title="Enlazar permanentemente con este título">¶</a></h2>
<p>Una <em>red generativa antagónica</em> (GAN, por sus siglas en inglés) está basada en modelado generativo y en conceptos de teoría de juegos.</p>
<p>El modelado generativo es una tarea de aprendizaje no supervisado en la que los modelos aprenden automáticamente regularidades o patrones en los datos de entrada, con el objetivo de generar nuevos ejemplos que podrían haberse extraído del conjunto de datos original.</p>
<p>Una GAN se construye a partir de dos redes neuronales conocidas como <em>generador</em> y <em>discriminador</em>. El generador aproxima una función generadora <span class="math notranslate nohighlight">\(G(z)\)</span>,</p>
<div class="math notranslate nohighlight" id="equation-alg-generativa">
<span class="eqno">(4.10)<a class="headerlink" href="#equation-alg-generativa" title="Enlace permanente a esta ecuación">¶</a></span>\[
    G(\mathbf{z})=\mathbf{\hat{x}}
\]</div>
<p>donde <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> es muestreado a partir de una distribución de probabilidad a priori en un espacio latente y <span class="math notranslate nohighlight">\(\mathbf{\hat{x}}\)</span> son las muestras generadas. El discriminador aproxima una función discriminadora que distingue entre muestras <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> de los datos originales y muestras <span class="math notranslate nohighlight">\(\mathbf{\hat{x}}\)</span> sintéticas.</p>
<p>El discriminador se entrena para diferenciar entre las muestras sintéticas y los datos reales y el generador para engañar al discriminador. La función de pérdida del discriminador depende de los parámetros del generador y viceversa. Los modelos se entrenan juntos hasta que el discriminador es engañado una cantidad de veces sobre algún umbral, lo que significa que el generador está generando ejemplos plausibles.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./capitulos\datos-metodos"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="datos-lhco.html" title="anterior página">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">anterior</p>
            <p class="prev-next-title"><span class="section-number">4.1. </span>Conjuntos de datos</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="algoritmos-lhco.html" title="siguiente página">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">siguiente</p>
        <p class="prev-next-title"><span class="section-number">4.3. </span>Algoritmos LHCO 2020</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      Por Mariana Vivas, Universidad Central de Venezuela<br/>
    
        &copy; Derechos de autor 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>