(discusion)=
# Discusión de resultados

En el capítulo anterior, se presentó la clasificación realizada por distintos modelos de aprendizaje automático, y la comparación con los algoritmos participantes de las LHCO 2020, UCluster y GAN-AE. A continuación, se analizarán los resultados obtenidos.

## Clasificación
Como se observa en la {numref}`clas-feature-imp`, las variables más importantates para la clasificación de eventos, de acuerdo a los algoritmos RFC y GBC, son la variable de subestructura $\tau_{21}$ y el $p_T$, de los dos jets principales, $\Delta R$ y el número de hadrones del evento. Que las variables $\tau_{21}$, $\Delta R$ y el número de hadrones del evento sean relevante es positivo porque nos interesa que los modelos puedan diferenciar la señal del fondo por características que no sean inherentes de una señal en específico. Sin embargo, también son relevantes las variables de $p_T$ de ambos jets, variables que dependen de las masas de las partículas de nueva física de un evento en específico, lo que no es deseable si se quiere un modelo que logre generalizar para eventos de física BSM independientes de las masas de las partículas.

 Una distancia angular $\Delta R$ más angosta para la señal, como se observa en {numref}`datospp-vardiff-RnD`, es característica natural de una partícula de nueva física masiva, debido a que su creación requiere de una mayor transferencia de momento, los que resulta en jets que se producen en direcciones opuestas y son más centrales en los detectores. Sin embargo, la clasificación se ve limitada a partículas nuevas con estado final de quarks, de manera que los eventos de señal posean menos hadrones que los de fondo, como se observa en la {numref}`rawdata-nhadrones`, y señales cuyos jets posean una estructura de dos o más subjets, que son las que coinciden con un menor valor de $\tau_{21}$. Aunque un mayor $p_T$ de los jets de señal es característica natural de una partícula de nueva física masiva, puesto que requiere mayor transferencia de momento, el valor exacto es dependiente de la masa de la partícula, a mayores masas, mayor será el valor de $p_T$. Esto limita a los modelos al momento de clasificar conjuntos de datos con distintas distribuciones de masa, como lo es el conjunto BB1.

Las variables menos discriminantes son $\phi$ y $\eta$ de ambos jets, que son las que presentan menos separación entre señal y fondo de acuerdo a la {numref}`datospp-vareq-RnD`. La variable $E$ también se encuentra entre las menos relevantes. Esto se puede deber a que esta variable está correlacionada con $p_T$, pero las distribuciones de $p_T$ de señal y fondo tiene un menor grado de superposisión que las distribuciones de $E$, por lo que son más útiles para distinguir clases.

Las variables más relevantes coinciden para ambos modelos, y da un indicio de los criterios de clasificación de otros modelos. Sin embargo, no significa que estas sean las variables más relevantes para todos los modelos, porque los puntajes para cada variable se obtienen de acuerdo al algoritmo utilizado. De hecho, aunque las variables más relevantes son similares para RFC y GBC, RFC da una mayor valoración a la variable de subestructura $\tau_{21}$ de ambos jets, mientras que la variable más relevante para GBC es el $p_T$ del jet secundario. Además, modelos no-supervisados como KMeans no funcionan de acuerdo a la importancia de las variables, si no que clasifican de acuerdo a la estructura interna de las mismas, y utilizan todas las variables para ello.

Como se mencionó anteriormente, los modelos fueron entrenados utilizando un subconjunto aleatorio del conjunto R&D. Por lo tanto, los resultados obtenidos para el **conjunto R&D** consisten en el subconjunto restante. En general, los resultados obtenidos para este conjunto son mejores para los modelos supervisados que para los modelos no supervisados, incluyendo UCluster y GAN-AE. Los modelos supervisados obtienen mejores resultados porque poseen mayor información en el entrenamiento, puesto que tienen las etiquetas para poder aprender de los datos. Los modelos no supervisados requieren de datos con alguna estructura interna que, como se mencionó anteriormente, si los datos no la poseen naturalmente, debe lograrse con el pre-procesamiento. Los datos pre-procesados utilizados por los modelos no-supervisados en este trabajo no logran una estructura suficientemente clara para que los modelos distingan mejor entre clases que los modelos supervisados.

Al considerar las métricas númericas, observamos que el rendimiento de los modelos varía de acuerdo a la métrica, es decir, no existe un único modelo que posea mayores o menores puntajes de acuerdo a todas las métricas. De acuerdo a la {numref}`comp-metricas-num-RnD`, para el conjunto R&D, la exactitud balanceada es mayor para el clasificador de TensorFlow, seguido de MLP, QDA, RFC, UCluster, GAN-AE, KMeans y GBC. Sin embargo, para el puntaje f1, que contiene la precisión y recuperación, el mejor rendimiento resulta del modelo MLP, seguido de RFC, GBC, QDA, el clasificador de TensorFlow, GAN-AE, KMeans y UCluster.

Las métricas bidimensionales parecen coincidir. De acuerdo al AUC y la precisión media, el modelo con mejor rendimiento es MLP, seguido del clasificador de TensorFlow, RFC, GBC, QDA y GAN-AE. Los modelos con menores puntajes son UCluster y KMeans. Según el AUC UCluster tiene un mejor rendimiento que KMeans, pero según la precisión promedio KMeans tiene un mejor rendimiento que UCluster.

Como no todas las métricas coinciden, es necesario definir cuáles son de mayor importancia en la tarea específica de clasificación de eventos de señal en HEP. Como se discute en {cite}`valassi_2019`, en el problema de clasificación de eventos en HEP los datos son altamente desbalanceados, y a nivel cualitativo también existe un desbalance; la correcta clasificación de eventos de fondo (TN) es irrelevante. El considerar esto, implica que cualquier métrica que dependa de TN es de limitada relevancia para comparar los modelos. Si se observa la {numref}`met-metricas` y la eq. {eq}`ml-exactitudbalanceada`, vemos que la exactitud balanceada considera la efectividad del clasificador para identificar eventos de fondo, ya que incluye la eficiencia de fondo o especificidad. De las métricas bidimensionales, la curva de eficiencia de señal vs. rechazo de fondo y la curva ROC inversa consideran la eficiencia de señal en alguno de sus ejes y, por ende, el AUC es una métrica basada en la eficiencia de fondo. La curva ROC, además de que incluye los TN, es una métrica no recomendada para tareas de clasificación con datos altamente desbalanceados, debido a que no utiliza ambas columnas de la matriz de confusión{cite}`Fawcett_2004`, es decir, la distribución de clases no afecta la métrica. Esto puede conducir a evaluaciones sobre optimistas.

Teniendo en cuenta lo anterior, entre las métricas númericas, el puntaje f1 es mejor métrica para este problema de clasificación, y entre las métricas bidimensionales, el gráfico de precisión-recuperación da una evaluación más realista de los modelos. A su vez, son más relevantes las métricas bidimensionales, debido a que poseen un mayor rango de clasificaciones posibles, donde, dependiendo del umbral de decisión, se pueden obtener mejores valores para las métricas numéricas.

En el caso de la clasificación del conjunto R&D, las métricas bidimensionales están de acuerdo sobre el rendimiento de los modelos, salvo entre el rendimiento de UCluster y KMeans. Sin embargo, de acuerdo al puntaje f1 y el gráfico de precisión-recuperación, que son las métricas más relevantes para esta tarea, KMeans tiene un mejor rendimiento que UCluster para este conjunto. El bajo rendimiento de UCluster puede ser resultado de la configuración utilizada al entrenar y clasificar los eventos, procesos en los que se usó la configuración por defecto, puesto que la configuración utilizada para obtener los resultados en en {cite}`Kasieczka_2021` no se encuentra especificada.

Los resultado obtenidos por las métricas concuerdan con lo observado en las distribuciones de las clasificaciones de este conjunto de datos. De acuerdo a las {numref}`clas-variables-dist`, {numref}`clas-variables-noimp-dist` y {numref}`clas-masa-dist`, y a lo discutido en la {numref}`clas`, el modelo MLP es el que obtiene una una clasificación con distribuciones más cercanas a las de los datos reales y KMeans obtiene las distribuciones menos precisas. 

Estas distribuciones nos permiten tener una idea de cómo están clasificando los modelos de las LHCO 2020. De acuerdo con el puntaje f1, GAN-AE tiene un menor rendimiento que el clasificador de TensorFlow, pero mejor rendimiento que KMeans, por lo que se espera que la clasificación con GAN-AE obtenga distribuciones de las variables más cercanas a las de los datos reales que las obtenidas con KMeans, pero menos precisas que las obtenidas con el clasificador de TensorFlow. Igualmente, UCluster tiene un menor puntaje f1 que KMeans, por lo que se espera que las distribuciones obtenidas por la clasificación de UCluster no separen correctamente entre clases.

La clasificación del **conjunto BB1** fue, en general, menos exitosa que para el conjunto R&D. Principalmente, porque se utilizaron variables dependientes de la masa, $p_T$ y $E$ de los jets principales, en el entrenamiento de los modelos implementados para este trabajo. GAN-AE también hace uso del $p_T$ de los jets, $E$ y las variables de masa, que son parte de las variables más relevantes para la clasificación según {cite}`Vaslin_2020`. Además, el conjunto tiene menor proporción de eventos de señal, lo que aumenta la probabilidad de realizar clasificaciones erroneas.

Según la exactitud balanceada ({numref}`comp-metricasnumericas-BB1`), el modelo con mejor resultado es GAN-AE, seguido de el clasificador de TensorFlow, MLP, KMeans, UCluster, GBC, RDC y QDA. Según el puntaje f1, el modelo que logró el mejor resultado es GBC, seguido de RFC, QDA, GAN-AE, el clasificador de TensorFlow, KMeans, MLP y UCluster. Al igual que en la clasificación del conjunto R&D, un modelo supervisado logró una mejor clasificación que los modelos no-supervisados.

De acuerdo a las métricas bidimensionales ({numref}`comp-metricas-graf-BB1`), GAN-AE tiene un mejor rendimiento que todos los modelos y UCluster es el que posee puntajes más bajos. Según el AUC, después de GAN-AE se encuentra MLP, GBC, RFC, el clasificador de TensorFlow, QDA, KMeans y UCluster. Sin embargo, según la precisión promedio, después de GAN-AE se encuentra GBC, el clasificador de TensorFlow, QDA, MLP, RFC, KMeans y UCluster.

De este resultado, notamos que un modelo no-supervisado puede ser mejor distinguiendo entre clases debido a una estructura interna en los datos, lograda en el pre-procesamiento, incluso a pesar de utilizar variables dependientes de la masa para el entrenamiento y la clasificación. Esto da a entender que, aunque los modelos supervisados son mejores clasificando eventos iguales a los datos con los que fueron entrenados, en el momento de generalizar, y realizar una búsqueda más libre de modelo, donde no se conocen las masas de las partículas, los modelos no-supervisados pueden tener mejor rendimiento.

De las distribuciones obtenidas de la clasificación del conjunto BB1, no se evidenció cuál algoritmo realiza una mejor clasificación. Al comparar con los modelos de las LHCO 2020, según el puntaje f1, GAN-AE obtiene una distribución de señal con mejor separación de clases que el modelo de TensorFlow, pero menor que QDA, para el conjunto BB1. UCluster obtiene distribuciones con una separación de clases menor que las obtenidas con el modelo MLP.

## Reproducibilidad
A nivel de reproducibilidad, el material publicado de ambos modelos fue sencillo de utilizar. 

El pre-procesamiento de los datos para UCluster se logra directamente siguiendo las instrucciones dadas por los autores. El problema principal para reproducir los resultados de este modelo es que no proporcionaron información del ambiente computacional, específicamente, de las versiones de las librerías utilizadas. Además, para el entrenamiento y clasificación hay varias configuraciones posibles. Las configuraciones utilizadas para obtener los resultados en {cite}`Kasieczka_2021` no se especifican, por lo que se utilizó la configuración por defecto.

Para GAN-AE, el pre-procesamiento implicó algunas modificaciones al código para que funcionara correctamente. Entre la información proporcionada para reproducir los resultados, tampoco se incluye información sobre las versiones de las librerías utilizadas, lo que dificulta la utilización del código. También se tuvo que escribir un código aparte para realizar la clasificación del conjunto BB1 con etiquetas, ya que el codigo publicado no era adaptable para esto.

El análisis de la reproducibilidad de los modelos de las LHCO 2020 inspiró gran parte de las consideraciones de reproducibilidad tomadas en este trabajo. Para reproducir los resultados de los modelos surgió la necesidad de que estos cumplieran con las características mencionadas en la {numref}`alglhco-rep`. Estas características se aplicaron al paquete de software `benchtools` En particular, se consideró que debía ser configurable, de manera que pueda ser utilizado para comparar los resultados de cualquier modelo con los modelos base planteados en este trabajo. Para que sea configurable y reusable por otros, se agregó información sobre los pasos a seguir para obtener el resultado, considerando que debe haber información sobre el ambiente computacional, el código debe ser libre y estar comentado para que otras personas puedan entenderlo, y debe incluir una licencia con información sobre bajo qué condiciones se puede reusar y modificar la información publicada. Esto se encuentra explicado con mayor detalle en el [repositorio de benchtools](https://github.com/marianaiv/benchtools).