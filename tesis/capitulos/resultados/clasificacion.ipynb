{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(clas)=\n",
    "# Clasificación de eventos\n",
    "Para comparar los algoritmos de las LHCO 2020 utilizamos como base algunos algoritmos sencillos ya implementados en librerías como `scikit-learn`{cite}`scikit-learn` y uno programado usando `tensorflow`{cite}`tensorflow2015-whitepaper`. \n",
    "\n",
    "En esta sección se observarán algunas de las característica de la clasificación con estos algoritmos. Los modelos utilizados son:\n",
    "\n",
    "```{table} Algoritmos utilizados para comparación\n",
    ":name: clas-alg\n",
    "\n",
    "|      Nombre                      | Implementación | Tipo                                             |\n",
    "|:--------------------------------:|:--------------:|:------------------------------------------------:|\n",
    "|Random Forest Classifier (RFC)    | `scikit-learn` | [Bosque aleatorio](alg-bosques)                  |\n",
    "|Gradient Boosting Classifier (GBC)| `scikit-learn` | [Clasificador del gradiente del impulso](alg-gbc)|\n",
    "|Quadratic Discriminant Analysis (QDA) | `scikit-learn` | [Análisis de discriminante cuadrático](alg-qda)  | \n",
    "|MLP Classifier                    | `scikit-learn` | [Red neuronal](alg-neural)                       |\n",
    "|Tensorflow Classifier             | `tensorflow`   | [Red neuronal](alg-neural)                       |\n",
    "| KMeans                           | `scikit-learn` | [K-means](alg-kmeans)                            |\n",
    "```\n",
    "explicados en la {numref}`alg`.\n",
    "\n",
    "Para este análisis, se utilizó el conjunto de datos R&D.\n",
    "(clas-importancia)=\n",
    "## Importancia de las características\n",
    "Por la forma en la que clasifican, algunos modelos permiten conocer cuáles de las características de los eventos fueron las variables más relevantes para la clasificación. De los modelos en la {numref}`clas-alg`, RFC y GBC asignan puntajes a las variables de acuerdo a su importancia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "A continuación, realizaremos el entrenamiento de los modelos y la clasificación de los datos de prueba. Las celdas siguientes preparan los datos, entrenan los modelos y realizan la clasificación utilizando funciones de `benchtools`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Importamos las librerías principales\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import os.path\n",
    "\n",
    "# De scikit-learn importamos herramientas\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Y los clasificadores\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Lo necesario para construir el modelo de tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Funciones de benchtools\n",
    "from benchtools.src.plotools import pred_test_hist, image_grid\n",
    "from benchtools.src.clustering import build_features\n",
    "from benchtools.src.datatools import separate_data\n",
    "from benchtools.scripts.run import TensorflowClassifier, training, evaluate\n",
    "from benchtools.src.metrictools import rejection_plot, precision_recall_plot\n",
    "\n",
    "# Definimos semillas para la reproducibilidad\n",
    "tf.random.set_seed(125)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Datos a utilizar\n",
    "path_data = \"../../../datos/events_anomalydetection.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A file with that name already exists\n"
     ]
    }
   ],
   "source": [
    "# Esta celda se corre una vez para pre-procesar los datos\n",
    "# Una vez que el archivo existe no vuelve a correr\n",
    "build_features(path_data=path_data, nbatch=11, outname='RnD-1100000', outdir='../../../datos/', chunksize=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# En esta celda preparamos los datos para utilizar los algoritmos\n",
    "# Separamos los datos en conjuntos de entrenamiento y prueba\n",
    "df_RnD = pd.read_csv('../../../datos/RnD-1100000.csv')\n",
    "X, y = separate_data(df_RnD, standarize=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y) # 70% training y 30% test\n",
    "\n",
    "# Guardamos las columnas de masa\n",
    "X_train_m = X_train.loc[:,['m_j1', 'm_j2', 'm_jj']].copy()\n",
    "X_test_m = X_test.loc[:,['m_j1', 'm_j2', 'm_jj']].copy()\n",
    "\n",
    "# Eliminamos las masas de los conjuntos de entrenamiento y prueba\n",
    "X_train.drop(['m_j1', 'm_j2', 'm_jj'], axis=1, inplace=True)\n",
    "X_test.drop(['m_j1', 'm_j2', 'm_jj'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Listamos los clasificadores a utilizar\n",
    "# En conjunto con el scaler\n",
    "classifiers = [(MinMaxScaler(feature_range=(-1,1)), TensorflowClassifier(input_shape = [X_train.shape[1]])),\n",
    "                (StandardScaler(), RandomForestClassifier(random_state=1)),\n",
    "                (RobustScaler(), GradientBoostingClassifier(random_state=4)),\n",
    "                (RobustScaler(), QuadraticDiscriminantAnalysis()), \n",
    "                (StandardScaler(), MLPClassifier(random_state=7)),\n",
    "                (StandardScaler(), KMeans(n_clusters=2, random_state=15))\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 14)               56        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               7680      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,069,113\n",
      "Trainable params: 1,063,965\n",
      "Non-trainable params: 5,148\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "1504/1504 [==============================] - 135s 88ms/step - loss: 0.1568 - binary_accuracy: 0.9430 - val_loss: 0.1229 - val_binary_accuracy: 0.9557\n",
      "Epoch 2/200\n",
      "1504/1504 [==============================] - 134s 89ms/step - loss: 0.1214 - binary_accuracy: 0.9552 - val_loss: 0.1168 - val_binary_accuracy: 0.9572\n",
      "Epoch 3/200\n",
      "1504/1504 [==============================] - 123s 82ms/step - loss: 0.1170 - binary_accuracy: 0.9567 - val_loss: 0.1139 - val_binary_accuracy: 0.9579\n",
      "Epoch 4/200\n",
      "1504/1504 [==============================] - 120s 80ms/step - loss: 0.1150 - binary_accuracy: 0.9574 - val_loss: 0.1116 - val_binary_accuracy: 0.9583\n",
      "Epoch 5/200\n",
      "1504/1504 [==============================] - 120s 80ms/step - loss: 0.1134 - binary_accuracy: 0.9581 - val_loss: 0.1126 - val_binary_accuracy: 0.9587\n",
      "Epoch 6/200\n",
      "1504/1504 [==============================] - 121s 80ms/step - loss: 0.1120 - binary_accuracy: 0.9586 - val_loss: 0.1145 - val_binary_accuracy: 0.9583\n",
      "Epoch 7/200\n",
      "1504/1504 [==============================] - 121s 81ms/step - loss: 0.1113 - binary_accuracy: 0.9590 - val_loss: 0.1108 - val_binary_accuracy: 0.9583\n",
      "Epoch 8/200\n",
      "1504/1504 [==============================] - 122s 81ms/step - loss: 0.1101 - binary_accuracy: 0.9593 - val_loss: 0.1080 - val_binary_accuracy: 0.9600\n",
      "Epoch 9/200\n",
      "1504/1504 [==============================] - 120s 80ms/step - loss: 0.1093 - binary_accuracy: 0.9596 - val_loss: 0.1116 - val_binary_accuracy: 0.9589\n",
      "Epoch 10/200\n",
      "1504/1504 [==============================] - 138s 92ms/step - loss: 0.1085 - binary_accuracy: 0.9599 - val_loss: 0.1146 - val_binary_accuracy: 0.9576\n",
      "Epoch 11/200\n",
      "1504/1504 [==============================] - 190s 126ms/step - loss: 0.1075 - binary_accuracy: 0.9604 - val_loss: 0.1150 - val_binary_accuracy: 0.9572\n",
      "Epoch 12/200\n",
      "1504/1504 [==============================] - 201s 134ms/step - loss: 0.1065 - binary_accuracy: 0.9605 - val_loss: 0.1123 - val_binary_accuracy: 0.9590\n",
      "Epoch 13/200\n",
      "1504/1504 [==============================] - 193s 128ms/step - loss: 0.1059 - binary_accuracy: 0.9609 - val_loss: 0.1136 - val_binary_accuracy: 0.9579\n",
      "Epoch 14/200\n",
      "1504/1504 [==============================] - 201s 134ms/step - loss: 0.1052 - binary_accuracy: 0.9611 - val_loss: 0.1143 - val_binary_accuracy: 0.9581\n",
      "Epoch 15/200\n",
      "1504/1504 [==============================] - 213s 141ms/step - loss: 0.1045 - binary_accuracy: 0.9612 - val_loss: 0.1173 - val_binary_accuracy: 0.9562\n",
      "Epoch 16/200\n",
      "1504/1504 [==============================] - 199s 132ms/step - loss: 0.1039 - binary_accuracy: 0.9614 - val_loss: 0.1211 - val_binary_accuracy: 0.9550\n",
      "Epoch 17/200\n",
      "1504/1504 [==============================] - 143s 95ms/step - loss: 0.1035 - binary_accuracy: 0.9617 - val_loss: 0.1240 - val_binary_accuracy: 0.9543\n",
      "Epoch 18/200\n",
      "1504/1504 [==============================] - 137s 91ms/step - loss: 0.1027 - binary_accuracy: 0.9621 - val_loss: 0.1208 - val_binary_accuracy: 0.9546\n",
      "Epoch 19/200\n",
      "1504/1504 [==============================] - 138s 92ms/step - loss: 0.1024 - binary_accuracy: 0.9622 - val_loss: 0.1211 - val_binary_accuracy: 0.9552\n",
      "Epoch 20/200\n",
      "1504/1504 [==============================] - 160s 107ms/step - loss: 0.1017 - binary_accuracy: 0.9626 - val_loss: 0.1253 - val_binary_accuracy: 0.9541\n",
      "Epoch 21/200\n",
      "1504/1504 [==============================] - 182s 121ms/step - loss: 0.1011 - binary_accuracy: 0.9627 - val_loss: 0.1207 - val_binary_accuracy: 0.9546\n",
      "Epoch 22/200\n",
      "1504/1504 [==============================] - 203s 135ms/step - loss: 0.1009 - binary_accuracy: 0.9630 - val_loss: 0.1300 - val_binary_accuracy: 0.9505\n",
      "Epoch 23/200\n",
      "1504/1504 [==============================] - 192s 128ms/step - loss: 0.1003 - binary_accuracy: 0.9631 - val_loss: 0.1321 - val_binary_accuracy: 0.9506\n",
      "Epoch 24/200\n",
      "1504/1504 [==============================] - 188s 125ms/step - loss: 0.0997 - binary_accuracy: 0.9632 - val_loss: 0.1298 - val_binary_accuracy: 0.9513\n",
      "Epoch 25/200\n",
      "1504/1504 [==============================] - 179s 119ms/step - loss: 0.0995 - binary_accuracy: 0.9634 - val_loss: 0.1375 - val_binary_accuracy: 0.9489\n",
      "Epoch 26/200\n",
      "1504/1504 [==============================] - 156s 104ms/step - loss: 0.0990 - binary_accuracy: 0.9635 - val_loss: 0.1333 - val_binary_accuracy: 0.9510\n",
      "Epoch 27/200\n",
      "1504/1504 [==============================] - 125s 83ms/step - loss: 0.0988 - binary_accuracy: 0.9637 - val_loss: 0.1327 - val_binary_accuracy: 0.9507\n",
      "Epoch 28/200\n",
      "1504/1504 [==============================] - 140s 93ms/step - loss: 0.0984 - binary_accuracy: 0.9640 - val_loss: 0.1324 - val_binary_accuracy: 0.9509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [1:38:58<00:00, 989.73s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved\n"
     ]
    }
   ],
   "source": [
    "# Con esta función entrenamos los modelos\n",
    "# Solo hace falta correrla una vez\n",
    "training(X_train, X_test, y_train, y_test, classifiers, '../../../datos', 'log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:41<00:00, 16.91s/it]\n"
     ]
    }
   ],
   "source": [
    "# En esta celda cargamos los modelos entrenados\n",
    "models = []\n",
    "\n",
    "# Cargamos el algoritmo entrenado de tensorflow\n",
    "tf_model = load_model(os.path.join('../../../datos','tf_model_{}.h5'.format('log')))\n",
    "models.append(('TensorflowClassifier', tf_model))\n",
    "\n",
    "# Cargamos los algoritmos entrenados de scikit-learn\n",
    "with open(os.path.join('../../../datos',\"sklearn_models_{}.pckl\".format('log')), \"rb\") as f:\n",
    "    while True:\n",
    "        try:\n",
    "            models.append(pickle.load(f))\n",
    "        except EOFError:\n",
    "            break\n",
    "\n",
    "# Evaluamos\n",
    "clfs = evaluate(X_test, y_test, models ,train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De los modelos entrenados, se puede obtener la importancia de cada variable. Un gráfico de estos puntajes se observa en la {numref}`clas-feature-imp` para RFC y GBC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# En esta celda graficamos las características más importantes para los modelos\n",
    "\n",
    "# Obtenemos las características más importantes\n",
    "# models[x][y] : x = 1(random forest), 2(gradient boosting); y = 0(nombre del modelo), 1(modelo entrenado) \n",
    "fi_rf = models[1][1].steps[1][1].feature_importances_.tolist()\n",
    "fi_gb = models[2][1].steps[1][1].feature_importances_.tolist()\n",
    "\n",
    "# Redondeamos los puntajes\n",
    "weights = [fi_rf, fi_gb]\n",
    "weights = [[ round(elem, 3) for elem in weight ] for weight in weights]\n",
    "# Obtenemos los nombres de los modelos\n",
    "names_fi = [models[1][0], models[2][0]]\n",
    "# Obtenemos los nombres de las características\n",
    "features = X_train.columns.tolist()\n",
    "\n",
    "# Juntamos los puntajes con las características en tuplas\n",
    "importance = {}\n",
    "ii = 0\n",
    "for weight in weights:\n",
    "    f_i = list(zip(features, weight))\n",
    "    importance[names_fi[ii]] = f_i\n",
    "    ii +=1\n",
    "\n",
    "# Graficamos en un bucle \n",
    "colors=['darkorange', 'green']\n",
    "lista_images = []\n",
    "ii = 0\n",
    "for name, scores in importance.items():\n",
    "\n",
    "    # Ordenamos de menor a mayor\n",
    "    scores.sort(key=lambda x: x[1], reverse=False) \n",
    "\n",
    "    # Salvamos los nombres y su puntaje separados\n",
    "    # y revertimos las tuplas para tener de mayor a menor puntaje \n",
    "    features = list(zip(*scores))[0]\n",
    "    score = list(zip(*scores))[1]\n",
    "    x_pos = np.arange(len(features)) \n",
    "\n",
    "    # Graficamos\n",
    "    fig = plt.figure(facecolor='white')\n",
    "    plt.barh(x_pos, score,align='center', color = colors[ii])\n",
    "    plt.yticks(x_pos, features) \n",
    "    plt.xlabel('Feature importance')\n",
    "    plt.title('Feature importance: {}'.format(name))\n",
    "    filename = '../../figuras/{}-feature-importance.png'.format(name)\n",
    "    lista_images.append(filename)\n",
    "    plt.savefig(filename, bbox_inches='tight')\n",
    "    plt.close('all')\n",
    "    ii += 1\n",
    "    \n",
    "image_grid(rows=1, columns=2, images=lista_images, name='clas-feature-imp', path='../../figuras/', remove=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ./../../figuras/clas-feature-imp.png\n",
    "---\n",
    "name: clas-feature-imp\n",
    "width: 80%\n",
    "---\n",
    "Importancia de las variables utilizadas en el entrenamiento según RFC (izquierda) y GBC (derecha).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que las características más relevantes para ambos clasificadores son la variable de subestructura $\\tau_{21}$ y el $p_T$ de los jets. En general, son relevantes las características que presentan distribuciones diferentes para señal y fondo.\n",
    "(clas-reconstruccion)=\n",
    "## Reconstrucción de distribuciones\n",
    "De la clasificación con los distintos modelos, se pueden reconstruir las variables para observar cómo están clasificando los algoritmos. En la {numref}`clas-variables-dist` observamos las distribuciones reales y la clasificación de las variables más importante según {numref}`clas-feature-imp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false,
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# En esta celda graficamos algunas variables importantes\n",
    "list_images =[]\n",
    "variables=['pT_j2', 'tau_21_j1', 'n_hadrons']\n",
    "for model in clfs:\n",
    "    for variable in variables:\n",
    "        # Obtenemos predicciones como pandas Series\n",
    "        pred = pd.Series(model.pred.flatten(), name='ypred')\n",
    "        # Obtenemos las etiquetas como pandas Serie\n",
    "        label = pd.Series(model.label, name='ytest').reset_index(drop=True)\n",
    "        # Juntamos las masas, las predicciones y las etiquetas\n",
    "        X_plot = pd.concat([X_test.reset_index(drop=True), X_test_m.reset_index(drop=True), pred, label], axis=1)\n",
    "        # Graficamos\n",
    "        fig = plt.figure(facecolor='white')\n",
    "        pred_test_hist(X_plot, variable, ypred='ypred', ytest='ytest', n_bins=50, log=False)\n",
    "        plt.title('{}: distribución de {}'.format(model.name, variable))\n",
    "        # Guardando el path de cada imagen\n",
    "        filename = os.path.join('../../figuras/','{}-{}.png'.format(model.name,variable))\n",
    "        list_images.append(filename)\n",
    "        # Salvamos la imagen como png\n",
    "        plt.savefig(filename, bbox_inches='tight', facecolor=fig.get_facecolor(),edgecolor='none')\n",
    "        plt.close('all')\n",
    "    del X_plot, pred, label\n",
    "\n",
    "image_grid(rows=6, columns=3, images=list_images, name='clas-variables-dist', path='../../figuras/', remove=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ./../../figuras/clas-variables-dist.png\n",
    "---\n",
    "name: clas-variables-dist\n",
    "width: 100%\n",
    "---\n",
    "Distribución de algunas de las variables más importantes para la clasificación. Cada fila de imágenes representa un clasificador. De arriba a abajo: Tensorflow Classifier, RFC, GBC, QDA, MLP y KMeans. Por columna, de izquierda a derecha, se encuentras las distribuciones de $p_T$ del jet secundario, $\\tau_{21}$ del jet principal y el número de hadrones de los eventos.\n",
    "```\n",
    "En {numref}`clas-variables-dist` vemos que los modelos, en general, predicen el fondo con bastante precisión. La distinción entre señal y fondo, es lograda en gran medida por los modelos supervisados. Particularmente, vemos que en todas las variables el clasificador de Tensorflow está sobreestimando el fondo y subestimando la señal. Al contrario, RFC, GBC y QDA sobreestiman la señal y subestiman el fondo. El clasificador MLP parece estimar las clases con mayor precisión. \n",
    "\n",
    "El único modelo no-supervisado, Kmeans, según los gráficos de $p_T$ y de $\\tau_{12}$, logra separar algunos eventos de señal, pero es evidente que en la clasificación de señal hay gran cantidad de fondo. En el plot de número de hadrones vemos que no logra diferencia entre señal y fondo.\n",
    "\n",
    "Al graficar algunas de las variables con menor importancia según la {numref}`clas-feature-imp`, vemos en la {numref}`clas-variables-noimp-dist`, que los modelos supervisados logran separar señal y fondo. El modelo de TensorFlow parece ser el más preciso estimando $\\eta$ del jet principal, variable para la cual el clasificador MLP logra separar señal y fondo pero la distribución de la señal predicha está corrida a la izquierda. \n",
    "\n",
    "Nuevamente, Kmeans parece separar siferenciar ligeramente entre señal y fondo en algunas variables. En la variable $\\eta$ vemos que no logra separar señal y fondo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# En esta celda graficamos las variables menos relevantes para la clasificación\n",
    "list_images =[]\n",
    "variables=['deltaR_j12', 'E_j1', 'eta_j1']\n",
    "for model in clfs:\n",
    "    for variable in variables:\n",
    "        # Obtenemos predicciones como pandas Series\n",
    "        pred = pd.Series(model.pred.flatten(), name='ypred')\n",
    "        # Obtenemos las etiquetas como pandas Serie\n",
    "        label = pd.Series(model.label, name='ytest').reset_index(drop=True)\n",
    "        # Juntamos las masas, las predicciones y las etiquetas\n",
    "        X_plot = pd.concat([X_test.reset_index(drop=True), X_test_m.reset_index(drop=True), pred, label], axis=1)\n",
    "        # Graficamos\n",
    "        fig = plt.figure(facecolor='white')\n",
    "        pred_test_hist(X_plot, variable, ypred='ypred', ytest='ytest', n_bins=50, log=False)\n",
    "        plt.title('{}: distribución de {}'.format(model.name, variable))\n",
    "        # Guardando el path de cada imagen\n",
    "        filename = os.path.join('../../figuras/','{}-{}.png'.format(model.name,variable))\n",
    "        list_images.append(filename)\n",
    "        # Salvamos la imagen como png\n",
    "        plt.savefig(filename, bbox_inches='tight', facecolor=fig.get_facecolor(),edgecolor='none')\n",
    "        plt.close('all')\n",
    "    del X_plot, pred, label\n",
    "\n",
    "image_grid(rows=6, columns=3, images=list_images, name='clas-variables-noimp-dist', path='../../figuras/', remove=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ./../../figuras/clas-variables-noimp-dist.png\n",
    "---\n",
    "name: clas-variables-noimp-dist\n",
    "width: 100%\n",
    "---\n",
    "Distribución de algunas de las variables menos relevantes para la clasificación. Cada fila de imágenes representa un clasificador. De arriba a abajo: Tensorflow Classifier, RFC, GBC, QDA, MLP y KMeans. Por columna, de izquierda a derecha, se encuentras las distribuciones de $\\Delta R$, $E$ del jet principal y $\\eta$ del jet principal.\n",
    "```\n",
    "En la {numref}`clas-masa-dist` podemos ver la predicción de la variable de masa invariante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false,
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# En esta celda graficamos la masa invariante predicha\n",
    "list_mass_images=[]\n",
    "variables=['m_jj']\n",
    "for model in clfs:\n",
    "    for variable in variables:\n",
    "        # Obtenemos predicciones como pandas Series\n",
    "        pred = pd.Series(model.pred.flatten(), name='ypred')\n",
    "        # Obtenemos las etiquetas como pandas Serie\n",
    "        label = pd.Series(model.label, name='ytest').reset_index(drop=True)\n",
    "        # Juntamos las masas, las predicciones y las etiquetas\n",
    "        X_plot = pd.concat([X_test.reset_index(drop=True), X_test_m.reset_index(drop=True), pred, label], axis=1)\n",
    "        # Graficamos\n",
    "        pred_test_hist(X_plot, variable, ypred='ypred', ytest='ytest', n_bins=50, log=False)\n",
    "        plt.title('{}: distribución de {}'.format(model.name, variable))\n",
    "        # Guardando el path de cada imagen\n",
    "        filename = os.path.join('../../figuras/','{}-{}.png'.format(model.name,variable))\n",
    "        list_mass_images.append(filename)\n",
    "        # Salvamos la imagen como png\n",
    "        plt.savefig(filename, bbox_inches='tight', facecolor=fig.get_facecolor(),edgecolor='none')\n",
    "        plt.close('all')\n",
    "    del X_plot, pred, label\n",
    "    \n",
    "image_grid(rows=2, columns=3, images=list_mass_images, name='clas-masa-dist', path='../../figuras/', remove=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ./../../figuras/clas-masa-dist.png\n",
    "---\n",
    "name: clas-masa-dist\n",
    "width: 100%\n",
    "---\n",
    "Distribución la masa invariante. En la fila superior, de izquierda a derecha, las predicciones de: Tensorflow Classifier, RFC, GBC y en la fila inferior: QDA, MLP y KMeans.\n",
    "```\n",
    "Las variables de masa no fueron utilizadas para entrenamiento, pero los modelos supervisados logran obtener distribuciones cercanas, lo que indica que están realizando correctamente la clasificación. El modelo de TensorFlow parece estar subestimando la señal. Los demás modelos supervisados parecen sobreestimar la señal de acuerdo con los gráficos de predicción de $m_{jj}$. Kmeans obtiene un pico para la señal correctamente clasificado, pero es evidente que está clasificando algunos eventos de fondo como señal.\n",
    "\n",
    "Aunque esta comparación de distribuciones proporciona un punto de partida para la comparación de los algoritmos, es necesario el uso de métricas, porque no es evidente qué algoritmo está clasificando mejor el conjunto de datos."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "be244558907c567e73a32fad5ffef5514602d6da01bb2b6b51508d7e46fcc84d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
