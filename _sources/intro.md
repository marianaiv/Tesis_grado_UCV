# Búsqueda de nueva física utilizando técnicas de aprendizaje automático en eventos de múltiples jets: análisis comparativo de algoritmos de clasificación en términos de reproducibilidad y rendimiento 

> Trabajo especial de grado realizado por Mariana Vivas para la Universidad Central de Venezuela como requisito parcial para optar por el título de licenciada en física.
### Resumen
El modelo estándar de la física de partículas es la teoría que clasifica los componentes de la materia y explica sus interacciones. Se considera uno de los mayores éxitos científicos de la física moderna, debido a que múltiples predicciones realizadas con este modelo se han comprobado experimentalmente. Sin embargo, se sabe que no es una teoría completa porque no incluye todas las fuerzas fundamentales; la fuerza gravitatoria no está incluida en la teoría. Además, varias observaciones experimentales no se pueden explicar con el modelo. Debido a esto, la búsqueda de física más allá del modelo estándar ha sido uno de los ejes principales de investigación en distintos centros, como el Gran Colisionador de Hadrones (LHC, por sus siglas en inglés). 

Hasta ahora, la búsqueda no ha sido éxitos. La gran cantidad y complejidad de los datos generados en los experimentos dificultan su análisis, por lo que en los últimos años se ha estudiado el uso de aprendizaje automático en física de altas energías, con el objetivo de aprovechar la gran cantidad de datos y buscar nuevas partículas con cierta independencia de modelo. Estas herramientas fueron estudiadas particularmente en las [olimpiadas LHC 2020](https://lhco2020.github.io/homepage/) (LHCO 2020), llevadas a cabo en el verano de 2020. Para esta competencia, se simularon conjuntos de datos con eventos de múltiples jets que incluyen nueva física, con el objetivo de que los participantes desarrollaran algoritmos de aprendizaje automático capaces de hallar las señales de nuevas partículas que son anómalas en los datos. Sin embargo, los resultados de los algoritmos no son directamente comparables; no es evidente cuál aproximación obtuvo un mejor resultado.

En este trabajo, se compararon directamente dos algoritmos participantes de las LCHO 2020, UCluster y GAN-AE, escogidos de acuerdo a la reproducibilidad de los modelos. Para evaluar la reproducibilidad, y hacer este trabajo de forma reproducible, se siguieron los lineamientos planteados en [The Turing Way](https://the-turing-way.netlify.app/welcome.html), un manual para hacer ciencia de datos de forma reproducible, ética y colaborativa. Los algoritmos de las LHCO 2020 se compararon con algoritmos de sencilla implementación: bosque aleatorio, análisis de discriminante cuadrático, potenciación del gradiente, redes neuronales y K-means. Para el análisis, se desarrolló `benchtools`, un paquete de software basado en Python que contiene herramientas para el manejo de los datos de las LHCO 2020 y un pipeline que compara modelos de clasificación binaria utilizando métricas naturales de esta tarea. El enfoque de la búsqueda se fijó en la topología de dos jets, o dijet, utilizando dos conjuntos de datos de las olimpiadas.

De las métricas utilizadas para comparar los modelos, se concluyó que el *puntaje f1* y la *curva de precisión-recuperación* (PR) son las métricas más adecuadas para evaluar el rendimiento de los algoritmos en esta tarea de clasificación, siendo esta última métrica más relevante porque proporciona más información sobre la clasificación realizada por los modelos. Del análisis, se halló que la clasificación del conjunto de datos utilizado para entrenar los modelos es, en general, mejor para los modelos supervisados. En particular, el modelos con mejor rendimiento para este conjunto de datos, de acuerdo al puntaje f1 y a la curva PR, es el clasificador de potenciación del gradiente. Al clasificar otro conjunto de datos ligeramente distinto, el modelo con mejor rendimiento, de acuerdo a la curva PR, fue GAN-AE, y de acuerdo al puntaje f1, el clasificador de bosque aleatorio. En ambos conjuntos, UCluster y KMeans tuvieron el menor rendimiento. El bajo rendimiento de UCluster se podría explicar debido a que la información sobre la configuración utilizada para obtener los resultados de las olimpiadas no se encuentra disponble.

A nivel general, se evidenció la necesidad de estudiar nuevas métricas para comparar modelos de aprendizaje automático utilizados para la búsqueda de nueva física y la importancia de tomar consideraciones acerca de la reproducibilidad a la hora de estudiar estas herramientas.

**Palabras clave**: modelo estándar, nueva física, jets, dijets, LHC, aprendizaje automático, redes neuronales, algoritmo de clasificación, métricas, análisis comparativo, reproducibilidad 
### Tutores
| Nombre | Role | email | Github | 
| --- | --- | --- | --- |
| Reina Camacho Toro | Investigadora en LPNHE/CNRS  | [reina.camacho@cern.ch](mailto:reina.camacho@cern.ch) | [@camachoreina](https://camachoreina.github.io) |
| Camila Rangel Smith | Investigadora científica de datos en El Instituto Alan Turing | [crangelsmith@turing.ac.uk](mailto:crangelsmith@turing.ac.uk) |[@crangelsmith](https://github.com/crangelsmith) |
| José Antonio Lopez | Universidad Central de Venezuela | [jal.ccs@gmail.com](mailto:jal.ccs@gmail.com) | - |